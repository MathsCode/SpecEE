# 在 `llama.cpp` 中添加新模型架构

添加新模型需要以下几个步骤：

1. 将模型转换为 GGUF 格式
2. 在 `llama.cpp` 中定义模型架构
3. 构建 GGML 图实现

完成这些步骤后，您可以提交 PR。

此外，确保新架构与主要的 ggml 后端（CUDA、METAL、CPU）以及示例正常工作，尤其是：
- [main](/examples/main/)
- [imatrix](/examples/imatrix/)
- [quantize](/examples/quantize/)
- [server](/examples/server/)

### 1. 将模型转换为 GGUF 格式

此步骤在 Python 中使用 [gguf](https://pypi.org/project/gguf/) 库和 `convert` 脚本完成。根据模型架构，您可以使用 [convert_hf_to_gguf.py](/convert_hf_to_gguf.py) 或 [convert_legacy_llama.py](/examples/convert_legacy_llama.py)（针对 `.pth` 格式的 `llama/llama2` 模型）。

`convert` 脚本读取模型配置、分词器、张量名称和数据，并将它们转换为 GGUF 元数据和张量。

对于 HF 模型，实施步骤如下：

1. 在新的 `Model` 子类中定义 `Model.register` 注解，示例：

```python
@Model.register("MyModelForCausalLM")
class MyModel(Model):
    model_arch = gguf.MODEL_ARCH.GROK
```

2. 在 [constants.py](/gguf-py/gguf/constants.py) 中定义 GGUF 张量的布局。

在 `MODEL_ARCH` 中添加枚举项，在 `MODEL_ARCH_NAMES` 中添加模型的友好名称，并在 `MODEL_TENSORS` 中添加 GGUF 张量名称。

以 `falcon` 模型为例：
```python
    MODEL_ARCH.FALCON: [
        MODEL_TENSOR.TOKEN_EMBD,
        MODEL_TENSOR.OUTPUT_NORM,
        MODEL_TENSOR.OUTPUT,
        MODEL_TENSOR.ATTN_NORM,
        MODEL_TENSOR.ATTN_NORM_2,
        MODEL_TENSOR.ATTN_QKV,
        MODEL_TENSOR.ATTN_OUT,
        MODEL_TENSOR.FFN_DOWN,
        MODEL_TENSOR.FFN_UP,
    ]
```

3. 将原始张量名称映射到 GGUF 中的标准名称。

一般规则是，在向 GGUF 添加新张量名称之前，确保该名称没有现有等价项。

找到 GGUF 张量名称的等价项后，将其添加到 [tensor_mapping.py](/gguf-py/gguf/tensor_mapping.py) 文件中。

如果张量名称是重复层/块的一部分，可以使用关键字 `bid` 进行替代。

例如，注意力层中的归一化张量：

```python
block_mappings_cfg: dict[MODEL_TENSOR, tuple[str, ...]] = {
        # Attention norm
        MODEL_TENSOR.ATTN_NORM: (
            "gpt_neox.layers.{bid}.input_layernorm",                # gptneox
            "transformer.h.{bid}.ln_1",                             # gpt2 gpt-j refact qwen
            "transformer.blocks.{bid}.norm_1",                      # mpt
            ...
        )
}
```

`transformer.blocks.{bid}.norm_1` 将映射为 GGUF 中的 `blk.{bid}.attn_norm`。

根据模型配置、分词器、代码和张量布局，您需要重写以下方法：
- `Model#set_gguf_parameters`
- `Model#set_vocab`
- `Model#write_tensors`

注意：张量名称必须以 `.weight` 后缀结尾，这是惯例，并且多个工具（如 `quantize`）都依赖此惯例处理权重。

### 2. 在 `llama.cpp` 中定义模型架构

必须在 `llama.cpp` 中定义模型参数和张量布局：
1. 定义新的 `llm_arch`
2. 在 `LLM_TENSOR_NAMES` 中定义张量布局
3. 在 `llm_load_hparams` 中添加任何非标准的元数据
4. 在 `llm_load_tensors` 中为推理创建张量
5. 如果模型有 RoPE 操作，请在 `llama_rope_type` 中添加 RoPE 类型

注意：`ggml` 中的维度通常与 `pytorch` 的维度顺序相反。

### 3. 构建 GGML 图实现

这是最有趣的部分，您需要在 `llama_build_graph` 中提供新模型架构的推理图实现。

可以参考现有的实现，如 `build_llama`、`build_dbrx` 或 `build_bert`。

在实现新图时，请注意底层的 `ggml` 后端可能不支持所有操作，您可以在另一个 PR 中添加对缺失后端操作的支持。

注意：要调试推理图，您可以使用 [llama-eval-callback](/examples/eval-callback/)。

## GGUF 规范

https://github.com/ggerganov/ggml/blob/master/docs/gguf.md

## 资源

- YaRN RoPE scaling https://github.com/ggerganov/llama.cpp/pull/2268
- 支持 Baichuan 模型系列 https://github.com/ggerganov/llama.cpp/pull/3009
- 支持注意力偏置 https://github.com/ggerganov/llama.cpp/pull/4283
- 支持 Mixtral https://github.com/ggerganov/llama.cpp/pull/4406
- BERT 嵌入 https://github.com/ggerganov/llama.cpp/pull/5423
- 支持 Grok-1 https://github.com/ggerganov/llama.cpp/pull/6204
- 支持 Command R Plus https://github.com/ggerganov/llama.cpp/pull/6491
- 支持 DBRX 架构 https://github.com/ggerganov/llama.cpp/pull/6515
- 如何将 HuggingFace 模型转换为 GGUF 格式 https://github.com/ggerganov/llama.cpp/discussions/2948