[
    {
        "Model": "specee",
        "Inference time": "2667420.54 ms",
        "Tokens": "36542",
        "Tokens per second": "13.70"
    },
    {
        "inference_time": "39756.31",
        "tokens": "535",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Hillary Clinton\u2019s security detail arrived at a suburban Des Moines, Iowa fruit processing company on Tuesday with an added vehicle \u2013 a second Scooby. After her signature oversize black Chevy conversion van dropped her off at Capitol Fruit Company in Norwalk, Iowa, a visually identical GMC van drove up to the building with a nearly identical Secret Service escort vehicle. Both armored vehicles have raised roofs, deep-tinted windows and New York license plates. But while the original van \u2013 the one nicknamed 'Scooby' after the Scooby-Doo cartoon show \u2013 sports a mustard-yellow New York tag, the second has blue and white plates of a different design. Scroll down for video. WHY BUY ONE WHEN YOU CAN HAVE TWO AT TWICE THE PRICE? The first picture of both of Hillary Clinton's Scooby mobiles. One is a GMC and the other is a Chevrolet, but they are mechanically identical. CONVOY: Scooby-one and Scooby-two took up positions in Hillary's motorcade on a freeway near Des Moines. BACK SEAT DRIVER? Hillary Clinton  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Hillary Clinton\u2019s security detail arrived at a suburban Des Moines, Iowa fruit processing company on Tuesday with an added vehicle \u2013 a second Scooby. After her signature oversize black Chevy conversion van dropped her off at Capitol Fruit Company in Norwalk, Iowa, a visually identical GMC van drove up to the building with a nearly identical Secret Service escort vehicle. Both armored vehicles have raised roofs, deep-tinted windows and New York license plates. But while the original van \u2013 the one nicknamed 'Scooby' after the Scooby-Doo cartoon show \u2013 sports a mustard-yellow New York tag, the second has blue and white plates of a different design. Scroll down for video. WHY BUY ONE WHEN YOU CAN HAVE TWO AT TWICE THE PRICE? The first picture of both of Hillary Clinton's Scooby mobiles. One is a GMC and the other is a Chevrolet, but they are mechanically identical. CONVOY: Scooby-one and Scooby-two took up positions in Hillary's motorcade on a freeway near Des Moines. BACK SEAT DRIVER? Hillary Clinton  was seen in the back of Scooby-one, the original van, while her husband Bill Clinton was in the back of Scooby- second, the new GMC van. The two vans are mechanically identical but have different license plate numbers. The second van, which is a GMC van, has a new set of blue and white plates. The two vans are mechanically identical but have different license plate numbers. The second van, which is a GMC van, has a new set of blue and white plates.\n\nThe two van are identical in every other way except for their license plate number. The first van is a Chevy van with a mustard- yellow New York license plate number, while the second van is a GMCvan with a new set of blue and white pl plate numbers. The two van are mechanically identical but have different license plate numbers. The second van is a GMCvan with a new set of blue and white pl plate numbers. The difference in the number pl plate numbers is the only difference between the two vans.\n\nThe two van are identical in other ways except for their license plate number. The first van is a Chevy van with a mustard- yellow New\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.51 ms /   536 runs   (    0.01 ms per token, 97189.48 tokens per second)\nllama_perf_context_print:        load time =    5145.87 ms\nllama_perf_context_print: prompt eval time =    1064.62 ms /   280 tokens (    3.80 ms per token,   263.00 tokens per second)\nllama_perf_context_print:        eval time =   38642.81 ms /   255 runs   (  151.54 ms per token,     6.60 tokens per second)\nllama_perf_context_print:       total time =   39756.31 ms /   535 tokens\nllama_perf_context_print:  mean exit layer =      25.65\nllama_perf_context_print:       exit ratio =       0.61  = 158 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 29, 32, 20, 27, 25, 22, 32, 32, 32, 32, 32, 27, 32, 20, 32, 17, 32, 32, 17, 32, 30, 20, 32, 15, 25, 22, 22, 18, 18, 32, 17, 16, 32, 25, 22, 30, 32, 32, 32, 32, 28, 32, 16, 32, 28, 32, 17, 32, 27, 30, 29, 31, 30, 25, 31, 32, 32, 30, 23, 32, 21, 29, 32, 16, 16, 32, 25, 20, 17, 32, 32, 27, 32, 32, 20, 32, 27, 22, 32, 17, 32, 17, 32, 16, 32, 20, 18, 20, 32, 29, 22, 30, 18, 32, 16, 32, 25, 23, 16, 32, 16, 32, 20, 16, 22, 20, 32, 25, 27, 17, 32, 32, 32, 30, 25, 32, 23, 32, 32, 17, 32, 17, 32, 18, 32, 20, 25, 30, 32, 32, 25, 25, 29, 32, 27, 20, 27, 18, 32, 18, 32, 20, 32, 17, 32, 20, 27, 32, 20, 22, 20, 24, 18, 32, 20, 32, 18, 32, 16, 16, 32, 16, 20, 20, 20, 27, 22, 20, 32, 32, 20, 24, 29, 32, 17, 32, 18, 32, 17, 32, 20, 27, 18, 22, 27, 18, 29, 18, 17, 32, 17, 32, 16, 32, 20, 14, 32, 17, 17, 32, 22, 27, 16, 32, 17, 32, 32, 18, 32, 22, 29, 32, 32, 20, 17, 15, 32, 15, 32, 27, 32, 32, 29, 31, 31, 22, 32, 32, 27, 20, 32, 20, 32, 18, 32, 20, 18, 17, 29, 32, 22, 15, 32, 32, 25, 20, 18, 16, 32, 17, 32, 25, 32, }\n"
    },
    {
        "inference_time": "36679.08",
        "tokens": "469",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: After a fire took a family's home of four years leaving them without anything, a Georgia mother of six is making a desperate plea for help. Kalimah Dixon said she was awakened by a neighbor yelling there was a fire early on Monday morning, and was lucky to get her children, aged one to 14, out of the Decatur condo safely. However, all that was left of her rented condo was a pile of rubble, and in her efforts to get her family back on their feet, she is asking for assistance. 'My children are homeless. They have nothing. They lost everything,' Dixon told Channel 2. Scroll down for video. A mother of six said her family has nothing after fire reduced their Georgia condo to a pile of rubble (above Kalimah Dixon with her five of her six children) The Dekalb County mother said the fire happened early on Monday and that she managed to get all of her children safely out (remains of the condo following the fire above) While Dixon was renting the Condo in the last four years, she had quit working to enroll  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: After a fire took a family's home of four years leaving them without anything, a Georgia mother of six is making a desperate plea for help. Kalimah Dixon said she was awakened by a neighbor yelling there was a fire early on Monday morning, and was lucky to get her children, aged one to 14, out of the Decatur condo safely. However, all that was left of her rented condo was a pile of rubble, and in her efforts to get her family back on their feet, she is asking for assistance. 'My children are homeless. They have nothing. They lost everything,' Dixon told Channel 2. Scroll down for video. A mother of six said her family has nothing after fire reduced their Georgia condo to a pile of rubble (above Kalimah Dixon with her five of her six children) The Dekalb County mother said the fire happened early on Monday and that she managed to get all of her children safely out (remains of the condo following the fire above) While Dixon was renting the Condo in the last four years, she had quit working to enroll 13-year- old daughter in a special needs school. She now has to find a new place for her family to live, and is asking for help with rent, clothing etc. Dixon said she has received some donations but more than ever is needed. 'I'm just trying to survive. I'm just trying to surv survive,' she said. 'I just want to make sure my children are safe and have a place to call home.' Dixon said she is grateful for help from neighbors and community members, but more than ever is needed. She has set up a account on GoFundMe to help her family get back on feet.\nSummary: Kalimah Dixon, a mother of six children, lost their home in a fire early morning on Monday morning. she managed to get her children out of the condo but all that was left was a pile of rubble. she is now pleading for help with rent, clothing, etc. she has received some donations but more than ever is needed. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.79 ms /   470 runs   (    0.01 ms per token, 98141.57 tokens per second)\nllama_perf_context_print:        load time =    4508.30 ms\nllama_perf_context_print: prompt eval time =    1081.22 ms /   251 tokens (    4.31 ms per token,   232.14 tokens per second)\nllama_perf_context_print:        eval time =   35557.31 ms /   218 runs   (  163.11 ms per token,     6.13 tokens per second)\nllama_perf_context_print:       total time =   36679.08 ms /   469 tokens\nllama_perf_context_print:  mean exit layer =      26.81\nllama_perf_context_print:       exit ratio =       0.56  = 124 / 220 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 18, 32, 29, 32, 31, 30, 32, 32, 27, 32, 30, 18, 32, 27, 32, 30, 27, 25, 25, 28, 22, 25, 18, 32, 32, 32, 32, 25, 16, 29, 32, 32, 32, 20, 16, 32, 29, 22, 32, 32, 31, 32, 32, 25, 25, 17, 32, 18, 32, 27, 15, 32, 32, 32, 32, 32, 27, 32, 27, 27, 32, 32, 18, 29, 32, 27, 20, 27, 16, 17, 32, 32, 18, 16, 32, 32, 32, 27, 32, 32, 32, 20, 32, 16, 32, 18, 32, 32, 32, 29, 29, 25, 20, 32, 32, 22, 32, 32, 32, 32, 25, 17, 32, 32, 32, 29, 17, 32, 31, 15, 32, 15, 32, 25, 15, 32, 30, 32, 27, 22, 27, 25, 32, 32, 32, 32, 32, 29, 16, 32, 20, 30, 32, 27, 20, 29, 32, 32, 18, 32, 27, 18, 32, 25, 32, 16, 32, 20, 18, 22, 32, 22, 27, 22, 32, 16, 32, 20, 22, 29, 32, 16, 32, 18, 32, 20, 29, 20, 17, 32, 31, 29, 30, 28, 31, 32, 25, 32, 18, 27, 32, 25, 22, 16, 32, 27, 27, 18, 29, 16, 32, 32, 18, 16, 32, 32, 22, 32, 17, 32, 17, 32, 22, 18, 32, 32, 22, 20, 16, 32, 14, 32, 27, 18, 27, 20, }\n"
    },
    {
        "inference_time": "39983.38",
        "tokens": "465",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows. Documents reveal the shocking details of the case that saw a couple return to Australia with a baby girl while leaving her twin brother behind. Australian government officials had full knowledge of the startling ordeal, according to documents obtained\u00a0the ABC. An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows (stock image) The FOI documents reportedly show staff at the Australian High Commission in India and Department of Foreign Affairs and Trade were aware the couple was from New South Wales, where it is illegal to enter into international surrogacy arrangements. Conversations between the Australian High Commission in New Delhi and Government officials in Canberra detail how the couple travelled to India in late 2012 and seeked citizenship for a baby -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows. Documents reveal the shocking details of the case that saw a couple return to Australia with a baby girl while leaving her twin brother behind. Australian government officials had full knowledge of the startling ordeal, according to documents obtained\u00a0the ABC. An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows (stock image) The FOI documents reportedly show staff at the Australian High Commission in India and Department of Foreign Affairs and Trade were aware the couple was from New South Wales, where it is illegal to enter into international surrogacy arrangements. Conversations between the Australian High Commission in New Delhi and Government officials in Canberra detail how the couple travelled to India in late 2012 and seeked citizenship for a baby boy born through a surrogacy arrangement. Australian government officials had full knowledge of the case, which saw a couple return to Australia with a baby girl while leaving her tw tw brother behind. The couple was told the baby boy would be stateless, but they went ahead with their plans to leave him behind, according to documents obtained by the ABC. The couple, who have not yet publicly identified themselves, were reportedly told by Indian authorities that they would not be able to obtain citizenship for their son because they were not married at the time of the birth. The couple then return to Australia with their daughter, who was born in November 2012, but left their tw tw brother behind in India. The documents show that Australian government officials were aware of the situation and raised concerns about the couple's actions with Indian authorities. The ABC reportedly obtained the documents through a Freedom of Freedom of Information (FOI) request. Australian government officials were aware of the situation and raised concerns about the couple's actions with Indian authorities. The couple was told by Indian authorities that they would not be able to obtain citizenship for their son because they were not married at the time of the birth. The couple then return to Australia with their daughter, who\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.49 ms /   466 runs   (    0.01 ms per token, 84819.80 tokens per second)\nllama_perf_context_print:        load time =    4510.54 ms\nllama_perf_context_print: prompt eval time =    1070.82 ms /   210 tokens (    5.10 ms per token,   196.11 tokens per second)\nllama_perf_context_print:        eval time =   38866.39 ms /   255 runs   (  152.42 ms per token,     6.56 tokens per second)\nllama_perf_context_print:       total time =   39983.38 ms /   465 tokens\nllama_perf_context_print:  mean exit layer =      26.02\nllama_perf_context_print:       exit ratio =       0.60  = 155 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 18, 32, 20, 32, 22, 16, 32, 32, 27, 20, 20, 32, 20, 32, 16, 17, 25, 32, 30, 32, 17, 22, 25, 27, 18, 18, 17, 32, 18, 32, 27, 32, 20, 22, 25, 22, 16, 29, 22, 32, 24, 32, 32, 29, 20, 22, 25, 17, 32, 20, 32, 32, 32, 22, 18, 32, 32, 32, 32, 32, 32, 30, 22, 27, 32, 20, 32, 20, 29, 29, 27, 32, 25, 32, 32, 16, 17, 19, 32, 32, 25, 32, 32, 16, 32, 25, 32, 20, 32, 22, 25, 27, 16, 32, 32, 32, 32, 32, 25, 20, 32, 17, 32, 29, 16, 32, 32, 29, 16, 32, 30, 22, 30, 32, 20, 24, 22, 28, 22, 20, 32, 22, 29, 31, 32, 20, 32, 32, 25, 29, 32, 29, 18, 32, 20, 32, 32, 22, 18, 32, 18, 30, 32, 29, 31, 32, 32, 29, 25, 25, 22, 29, 32, 22, 24, 16, 32, 32, 32, 16, 32, 27, 16, 32, 32, 32, 32, 22, 29, 31, 32, 32, 16, 32, 32, 18, 32, 32, 25, 32, 27, 22, 32, 29, 25, 31, 32, 29, 29, 17, 32, 25, 19, 32, 29, 32, 16, 32, 16, 32, 25, 32, 16, 32, 22, 14, 30, 32, 14, 32, 18, 18, 29, 25, 32, 22, 22, 32, 20, 32, 17, 22, 32, 17, 32, 19, 32, 21, 16, 18, 14, 32, 16, 32, 22, 14, 32, 17, 32, 22, 12, 17, 32, 29, 29, 20, 32, 18, 20, 17, 17, 32, 17, 32, 20, }\n"
    },
    {
        "inference_time": "5194.67",
        "tokens": "290",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: We all need a helping hand sometimes, and nobody more so than Kecil the baby orangutan, who had a tough start to life after his natural mother and another surrogate mother rejected him. But the one-year-old ape found love at the third time of asking with second surrogate mother Maggie at Chicago's Brookfield Zoo, and has today been pictured cuddling up to the experienced mother ahead of his first public outing this weekend. Maggie, 53, who has already raised one surrogate child, has proved herself yet again by helping to raise Kecil (pronounced Ka-cheel) for the past 10 months, and the two now have an inseparable bond. Scroll down for video. Taking care: Maggie, 53, wraps a protective arm around Kecil the baby orangutan who she has been raising for the past ten months after his mother and a surrogate mother both rejected him. Inseparable: Zookeepers were nervous that Maggie might reject Kecil after raising four of her own children and another surrogate which was given to her in 2002, but she has pr -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: We all need a helping hand sometimes, and nobody more so than Kecil the baby orangutan, who had a tough start to life after his natural mother and another surrogate mother rejected him. But the one-year-old ape found love at the third time of asking with second surrogate mother Maggie at Chicago's Brookfield Zoo, and has today been pictured cuddling up to the experienced mother ahead of his first public outing this weekend. Maggie, 53, who has already raised one surrogate child, has proved herself yet again by helping to raise Kecil (pronounced Ka-cheel) for the past 10 months, and the two now have an inseparable bond. Scroll down for video. Taking care: Maggie, 53, wraps a protective arm around Kecil the baby orangutan who she has been raising for the past ten months after his mother and a surrogate mother both rejected him. Inseparable: Zookeepers were nervous that Maggie might reject Kecil after raising four of her own children and another surrogate which was given to her in 2002, but she has proudbly proved herself again by helping to raise the little ape.\n\n\n\n\n\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.47 ms /   291 runs   (    0.00 ms per token, 616525.42 tokens per second)\nllama_perf_context_print:        load time =    4512.04 ms\nllama_perf_context_print: prompt eval time =    1089.86 ms /   269 tokens (    4.05 ms per token,   246.82 tokens per second)\nllama_perf_context_print:        eval time =    4099.07 ms /    21 runs   (  195.19 ms per token,     5.12 tokens per second)\nllama_perf_context_print:       total time =    5194.67 ms /   290 tokens\nllama_perf_context_print:  mean exit layer =      29.30\nllama_perf_context_print:       exit ratio =       0.35  = 8 / 23 \nllama_perf_context_print:       exit layer = {32, 32, 32, 18, 32, 32, 18, 22, 32, 18, 32, 32, 29, 32, 32, 27, 31, 32, 32, 32, 32, 31, 32, }\n"
    },
    {
        "inference_time": "37963.45",
        "tokens": "486",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Paris Saint-Germain star Javier Pastore has dismissed Eric Cantona\u2019s suggestion that he is the greatest footballer on the planet. Cantona, the former Manchester United forward, claimed earlier this month that Pastore is \u2018the best player in the world\u2019. But speaking ahead of his side\u2019s second-leg tie against Barcelona in the Nou Camp, Pastore believes Lionel Messi and Cristiano Ronaldo are a class above him. Javier Pastore addresses the world's media ahead of Paris Saint-Germain's European clash in Barcelona. Pastore and his Paris Saint-Germain team-mates are pictured at the Nou Camp on Monday ahead of their tie. \u2018Cantona has his opinion and I thank him for his praise. However, I don't share his view,\u2019 said Castore. \u2018I don't compare myself with the best. Cantona likes good football and goals, but I think Messi and Cristiano Ronaldo are incredible players.\u2019 Pastore scored a brace in PSG\u2019s 3-1 win at Nice on Saturday as the Parisians reclaimed top spot in Ligue 1. Speaking at the Laureus Sports Awards, -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Paris Saint-Germain star Javier Pastore has dismissed Eric Cantona\u2019s suggestion that he is the greatest footballer on the planet. Cantona, the former Manchester United forward, claimed earlier this month that Pastore is \u2018the best player in the world\u2019. But speaking ahead of his side\u2019s second-leg tie against Barcelona in the Nou Camp, Pastore believes Lionel Messi and Cristiano Ronaldo are a class above him. Javier Pastore addresses the world's media ahead of Paris Saint-Germain's European clash in Barcelona. Pastore and his Paris Saint-Germain team-mates are pictured at the Nou Camp on Monday ahead of their tie. \u2018Cantona has his opinion and I thank him for his praise. However, I don't share his view,\u2019 said Castore. \u2018I don't compare myself with the best. Cantona likes good football and goals, but I think Messi and Cristiano Ronaldo are incredible players.\u2019 Pastore scored a brace in PSG\u2019s 3-1 win at Nice on Saturday as the Parisians reclaimed top spot in Ligue 1. Speaking at the Laureus Sports Awards, Cantona said: \u2018Pastore is the best player in world football. He has everything. he has speed, speed, speed, he has speed. he has power, he has technique, he has vision, he has everything. Cantona, who won four Premier League titles with Manchester United, has been a vocal supporter of Pastoral in his role as a PSG ambassador. PSG face Barcelona in their Champions League last-16 match on Tuesday night. PSG star Javier Pastoral has dismissed Eric Cantona\u2019s claim that he is the best player in world football. PSG star Javier Pastoral has dismissed Eric Cantona\u2019s claim that he is the best player in world football. The 26- year old Argentian forward, who has scored 13 goals in his last 10 games for PSG, said he does not compare himself with other top players like Lion Messi and Cristiano Ronaldo. PSG face Barcelona in their Champions League last-16 match on Tuesday night. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.05 ms /   487 runs   (    0.01 ms per token, 96492.97 tokens per second)\nllama_perf_context_print:        load time =    4556.56 ms\nllama_perf_context_print: prompt eval time =    1128.22 ms /   266 tokens (    4.24 ms per token,   235.77 tokens per second)\nllama_perf_context_print:        eval time =   36793.93 ms /   220 runs   (  167.25 ms per token,     5.98 tokens per second)\nllama_perf_context_print:       total time =   37963.45 ms /   486 tokens\nllama_perf_context_print:  mean exit layer =      27.10\nllama_perf_context_print:       exit ratio =       0.57  = 126 / 222 \nllama_perf_context_print:       exit layer = {32, 32, 29, 32, 32, 29, 32, 30, 27, 20, 27, 18, 18, 27, 18, 32, 32, 29, 29, 32, 32, 25, 27, 32, 16, 23, 29, 21, 31, 29, 32, 23, 32, 25, 29, 32, 17, 23, 32, 32, 20, 18, 32, 32, 29, 18, 32, 32, 31, 25, 32, 31, 32, 32, 32, 31, 25, 32, 23, 32, 22, 32, 32, 32, 32, 24, 22, 32, 20, 32, 27, 32, 18, 32, 22, 32, 32, 27, 32, 18, 29, 27, 29, 32, 32, 29, 29, 32, 32, 32, 32, 30, 29, 18, 32, 27, 25, 32, 32, 32, 27, 27, 32, 32, 22, 22, 25, 30, 32, 17, 32, 32, 27, 15, 32, 18, 32, 22, 22, 24, 18, 32, 29, 17, 32, 29, 29, 29, 32, 22, 25, 22, 18, 32, 32, 17, 32, 32, 27, 27, 30, 32, 16, 22, 17, 17, 32, 18, 32, 15, 32, 18, 32, 32, 27, 32, 30, 27, 22, 32, 16, 32, 32, 17, 32, 32, 32, 32, 32, 18, 32, 30, 32, 32, 29, 25, 20, 32, 29, 27, 29, 32, 32, 32, 16, 32, 17, 18, 17, 32, 17, 32, 25, 25, 27, 19, 32, 20, 32, 22, 32, 30, 29, 32, 32, 32, 20, 17, 32, 17, 32, 16, 32, 27, 18, 32, 18, 32, 27, 17, 18, 19, }\n"
    },
    {
        "inference_time": "18219.00",
        "tokens": "382",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Campaign:\u00a0Amanda Burleigh has won her battle. A midwife has won a decade-long battle to give mothers more time attached to their newborns before the umbilical cord is cut. Amanda Burleigh was convinced that clamping the cord within seconds of delivery \u2013 shutting off the blood supply from the placenta \u2013 was wrong. Since the 1950s, when doctors and midwives started giving women a hormonal injection to reduce the risk of haemorrhage, they have clamped the cord within seconds of birth. They feared the drug \u2013 since replaced with a safer substitute \u2013 could harm the baby. But Ms Burleigh, a midwife at Leeds Teaching Hospitals NHS Trust, believed it was unnatural to clamp the cord when it was still visibly pulsating with blood. She said: \u2018One day I had a brainwave and thought, \u201cWe all clamp the cord within seconds of the baby being born, but we are doing it without any evidence about whether it\u2019s good or bad for the baby.\u201d' She contacted other medics, including obstetrician Dr David Hutchon, and together t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Campaign:\u00a0Amanda Burleigh has won her battle. A midwife has won a decade-long battle to give mothers more time attached to their newborns before the umbilical cord is cut. Amanda Burleigh was convinced that clamping the cord within seconds of delivery \u2013 shutting off the blood supply from the placenta \u2013 was wrong. Since the 1950s, when doctors and midwives started giving women a hormonal injection to reduce the risk of haemorrhage, they have clamped the cord within seconds of birth. They feared the drug \u2013 since replaced with a safer substitute \u2013 could harm the baby. But Ms Burleigh, a midwife at Leeds Teaching Hospitals NHS Trust, believed it was unnatural to clamp the cord when it was still visibly pulsating with blood. She said: \u2018One day I had a brainwave and thought, \u201cWe all clamp the cord within seconds of the baby being born, but we are doing it without any evidence about whether it\u2019s good or bad for the baby.\u201d' She contacted other medics, including obstetrician Dr David Hutchon, and together t...\n\nQuestion: What does the above passage suggest about the practice of clamping the umbilical cord after birth?\n\nA answer choices options:\n\n A) It is a safe practice that has been practiced since the 1950s\nB) it is an un natural practice that should be stopped\n C) it is an unnecessary practice that wastes valuable time\nD) it is a practice that has been shown to be harmful to the baby\n\n answer: B) it is an un natural practice that should be stopped [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       2.49 ms /   383 runs   (    0.01 ms per token, 153568.56 tokens per second)\nllama_perf_context_print:        load time =    4514.55 ms\nllama_perf_context_print: prompt eval time =    1093.71 ms /   267 tokens (    4.10 ms per token,   244.12 tokens per second)\nllama_perf_context_print:        eval time =   17103.57 ms /   115 runs   (  148.73 ms per token,     6.72 tokens per second)\nllama_perf_context_print:       total time =   18219.00 ms /   382 tokens\nllama_perf_context_print:  mean exit layer =      25.88\nllama_perf_context_print:       exit ratio =       0.66  = 77 / 117 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 29, 20, 27, 31, 18, 32, 32, 22, 32, 32, 22, 32, 24, 17, 32, 27, 22, 32, 18, 17, 22, 32, 30, 30, 32, 17, 32, 17, 29, 22, 31, 29, 25, 24, 31, 31, 32, 20, 32, 27, 18, 22, 32, 17, 29, 29, 29, 27, 18, 22, 16, 25, 29, 32, 20, 18, 32, 22, 20, 20, 29, 32, 22, 32, 29, 22, 25, 17, 32, 15, 32, 22, 24, 32, 32, 32, 17, 32, 25, 32, 17, 32, 20, 22, 27, 18, 32, 32, 22, 17, 32, 32, 25, 32, 22, 17, 32, 15, 32, 29, 27, 17, 32, 18, 32, 25, 22, 18, 18, 20, 32, 25, }\n"
    },
    {
        "inference_time": "37600.87",
        "tokens": "481",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Francois Hollande has vowed to 'show no mercy' if French peacekeepers are found guilty of sexual assaulting starving children in the Central African Republic. Fourteen soldiers dispatched to restore order after a 2013 coup have been accused of abusing children as young as nine as they begged for something to eat, according\u00a0to a French judicial source. The investigation has been underway since last year but was only made public yesterday. The French president today said: 'If some soldiers have behaved badly, I will show no mercy.' President Francois Hollande has vowed to 'show no mercy' if French peacekeepers in Central African Republic are found guilty of raping hungry children in exchange for food. The French defence ministry has vehemently denied covering up the scandal following revelations it was made aware of the allegations in July last year when it received a leaked UN report on the subject. The ministry said it immediately launched a probe into the case when it received the news, sending in -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Francois Hollande has vowed to 'show no mercy' if French peacekeepers are found guilty of sexual assaulting starving children in the Central African Republic. Fourteen soldiers dispatched to restore order after a 2013 coup have been accused of abusing children as young as nine as they begged for something to eat, according\u00a0to a French judicial source. The investigation has been underway since last year but was only made public yesterday. The French president today said: 'If some soldiers have behaved badly, I will show no mercy.' President Francois Hollande has vowed to 'show no mercy' if French peacekeepers in Central African Republic are found guilty of raping hungry children in exchange for food. The French defence ministry has vehemently denied covering up the scandal following revelations it was made aware of the allegations in July last year when it received a leaked UN report on the subject. The ministry said it immediately launched a probe into the case when it received the news, sending in a team of investigators to the country to investigate the matter. The UN has also launched its own investigation into the allegations, which are said to have occurred in the capital city of capital city of capital city of Bangui in 2013. French soldiers have been accused of abusing children as they begged for food in the Central African Republic. French President Francois Hollande has vowed to 'show no mercy' if French peacekeepers are found guilty of sexually assaulting children in the country. The French judicial source said that French soldiers had sexually assaulted children as they begged for food in the country. French President Francois Hollande has vowed to 'show no mercy' if French peacekeepers are found guilty of sexually assaulting children in the country. The French soldiers were part of a UN-backed missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary missionary mission\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.76 ms /   482 runs   (    0.01 ms per token, 83622.48 tokens per second)\nllama_perf_context_print:        load time =    4567.16 ms\nllama_perf_context_print: prompt eval time =    1094.36 ms /   226 tokens (    4.84 ms per token,   206.51 tokens per second)\nllama_perf_context_print:        eval time =   36459.05 ms /   255 runs   (  142.98 ms per token,     6.99 tokens per second)\nllama_perf_context_print:       total time =   37600.87 ms /   481 tokens\nllama_perf_context_print:  mean exit layer =      25.21\nllama_perf_context_print:       exit ratio =       0.57  = 147 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 27, 22, 20, 24, 26, 32, 20, 32, 20, 29, 17, 30, 31, 22, 32, 16, 32, 32, 14, 32, 17, 27, 25, 17, 32, 32, 32, 32, 25, 20, 27, 32, 32, 32, 18, 22, 22, 18, 32, 17, 20, 32, 32, 32, 32, 32, 29, 32, 32, 20, 29, 25, 32, 31, 16, 32, 18, 32, 27, 25, 32, 18, 32, 27, 17, 20, 32, 32, 32, 25, 17, 32, 25, 32, 25, 25, 25, 23, 22, 18, 32, 27, 16, 32, 32, 18, 22, 25, 29, 17, 25, 32, 22, 16, 32, 18, 32, 16, 32, 32, 29, 32, 17, 32, 20, 19, 30, 32, 25, 32, 17, 20, 32, 25, 25, 20, 32, 32, 32, 17, 32, 17, 32, 18, 32, 27, 17, 22, 30, 32, 20, 32, 25, 32, 25, 29, 32, 23, 27, 17, 32, 27, 15, 32, 32, 18, 22, 25, 29, 17, 32, 32, 22, 17, 32, 18, 32, 16, 32, 17, 17, 32, 17, 17, 32, 18, 18, 32, 20, 27, 32, 32, 22, 29, 22, 32, 32, 32, 22, 18, 32, 17, 32, 15, 32, 15, 32, 15, 32, 15, 32, 16, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, 15, 32, }\n"
    },
    {
        "inference_time": "39861.00",
        "tokens": "511",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A shocking 'resurrection ceremony' for a  two-year-old dead boy at a Texas church has been caught on camera. In the clip capturing the attempted resurrection, the boy identified by a witness as Benjamin, is being held in the arms of Texas pastor's wife Aracely Meza as others, including a man who appears to be her husband Pastor Daniel Meza, are gathered around. The witness, who said she knew Benjamin's mother since she was pregnant with him, said the child was possessed by demons, according to pastors. A shocking 'resurrection ceremony' for a two-year-old dead boy at a Texas church has been caught on camera. Aracely Meza, who is reportedly not the child's mother, was arrested on Monday and has been charged with injury to a child by omission, said police (above Meza in the video) In the video, Aracely Meza is seen next to a man who appears to be her husband Pastor Daniel Meza, as she uses oils and prays while trying to bring the boy, identified by a witness as Benjamin, back to life (above scenes fr -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A shocking 'resurrection ceremony' for a  two-year-old dead boy at a Texas church has been caught on camera. In the clip capturing the attempted resurrection, the boy identified by a witness as Benjamin, is being held in the arms of Texas pastor's wife Aracely Meza as others, including a man who appears to be her husband Pastor Daniel Meza, are gathered around. The witness, who said she knew Benjamin's mother since she was pregnant with him, said the child was possessed by demons, according to pastors. A shocking 'resurrection ceremony' for a two-year-old dead boy at a Texas church has been caught on camera. Aracely Meza, who is reportedly not the child's mother, was arrested on Monday and has been charged with injury to a child by omission, said police (above Meza in the video) In the video, Aracely Meza is seen next to a man who appears to be her husband Pastor Daniel Meza, as she uses oils and prays while trying to bring the boy, identified by a witness as Benjamin, back to life (above scenes fr om the church where the incident occurred) The witness, who said she knew Benjamin's mother since she was pregn with him, said the child was possessed by demons, according to pastors. The child's mother, who was not present at the scene, has been contacted by police and is now cooperating with investigators, said police. The child's body was found in a church bathroom by a church member who went to check on him after he did not show up for services, said police. The child's death is currently investigated as a homicide by police. Benjamin was found dead in a church bathroom by a church member who went to check on him after he did not show up for services, said police. Benjamin was found dead in a church bathroom by a church member who went to check on him after he did not show up for services, said police. Benjamin was found dead in a church bathroom by a church member who went to check on him after he did not show up for services, said police. Benjamin's mother was not present at the scene and has been contacted by police, who said she is now cooperating with investigators. The child's death is currently investig\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.76 ms /   512 runs   (    0.01 ms per token, 88873.46 tokens per second)\nllama_perf_context_print:        load time =    4491.68 ms\nllama_perf_context_print: prompt eval time =    1073.83 ms /   256 tokens (    4.19 ms per token,   238.40 tokens per second)\nllama_perf_context_print:        eval time =   38740.85 ms /   255 runs   (  151.92 ms per token,     6.58 tokens per second)\nllama_perf_context_print:       total time =   39861.00 ms /   511 tokens\nllama_perf_context_print:  mean exit layer =      26.00\nllama_perf_context_print:       exit ratio =       0.60  = 155 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 22, 25, 29, 32, 30, 29, 31, 32, 30, 16, 32, 32, 25, 32, 27, 32, 18, 22, 20, 22, 18, 32, 27, 17, 20, 17, 32, 20, 17, 32, 17, 25, 27, 29, 17, 32, 22, 22, 31, 32, 22, 29, 29, 32, 30, 22, 32, 28, 32, 30, 20, 20, 28, 32, 20, 20, 32, 16, 32, 30, 32, 18, 32, 32, 32, 22, 18, 32, 25, 32, 18, 32, 32, 22, 29, 27, 32, 24, 24, 32, 29, 32, 32, 32, 32, 31, 32, 32, 22, 32, 27, 32, 32, 32, 25, 32, 32, 20, 32, 25, 32, 32, 32, 32, 18, 18, 32, 22, 29, 29, 32, 29, 20, 17, 32, 25, 31, 32, 32, 32, 25, 23, 30, 27, 32, 32, 32, 30, 29, 22, 22, 32, 20, 17, 32, 19, 17, 32, 19, 32, 20, 32, 17, 32, 25, 18, 32, 20, 32, 20, 25, 29, 17, 32, 27, 25, 19, 32, 27, 17, 32, 25, 32, 17, 32, 18, 32, 17, 32, 17, 32, 20, 32, 18, 32, 25, 18, 32, 18, 32, 18, 17, 32, 17, 32, 27, 19, 20, 32, 18, 32, 18, 32, 18, 17, 18, 20, 19, 17, 32, 17, 32, 18, 32, 25, 32, 27, 18, 32, 22, 18, 18, 16, 32, 17, 32, 22, 14, 32, 22, 30, 32, 16, 32, 27, 16, 32, 32, 14, 25, 32, 14, 32, 32, 27, 20, 32, 29, 16, 32, 25, 20, 18, 32, 20, 27, 32, 20, 17, 29, 32, 15, 32, 17, }\n"
    },
    {
        "inference_time": "45263.23",
        "tokens": "526",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Crucible thoroughbreds John Higgins and Graeme Dott led the Scottish charge on Sunday at the Betfred World Championship. Four-time winner Higgins impressed with a 10-5 first-round victory over Robert Milkins, and looked to be approaching the form that last saw him take the title four years ago. The 39-year-old fired breaks of 77, 75, 69 and a clinching 106 in the second session as he coasted home against Gloucester potter Milkins, who took the match beyond its evening interval but was thoroughly outplayed. John Higgins plays a shot during his match against Robert Milkins during the Betfred World Championships. Milkins said of his conqueror: 'It's going to take a good man to beat him on that form. 'He's playing really good stuff. I think I did well to get five frames.' Dott, unseeded this year but a proven Crucible class act and champion in 2006, was a 10-8 winner earlier as he ousted Ricky Walden. Higgins has sprung back to competitive form level this year, notably in taking the Welsh Open title in -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Crucible thoroughbreds John Higgins and Graeme Dott led the Scottish charge on Sunday at the Betfred World Championship. Four-time winner Higgins impressed with a 10-5 first-round victory over Robert Milkins, and looked to be approaching the form that last saw him take the title four years ago. The 39-year-old fired breaks of 77, 75, 69 and a clinching 106 in the second session as he coasted home against Gloucester potter Milkins, who took the match beyond its evening interval but was thoroughly outplayed. John Higgins plays a shot during his match against Robert Milkins during the Betfred World Championships. Milkins said of his conqueror: 'It's going to take a good man to beat him on that form. 'He's playing really good stuff. I think I did well to get five frames.' Dott, unseeded this year but a proven Crucible class act and champion in 2006, was a 10-8 winner earlier as he ousted Ricky Walden. Higgins has sprung back to competitive form level this year, notably in taking the Welsh Open title in March, and his victory over Milkins was his third in four matches since the Crucial Crucible qualifiers. He said: 'I feel like I'm getting back to my best form. I've been working hard on my practice and it's paying off now.' Higgins, who has won the World Championship four times, is now set to face either Mark Selby or Joe Perry in the second round. The Scot, who has been struggling with a back injury in recent years, said: 'I've been struggling with my back for years, but I've been working hard on my fitness and it's starting nowhere near as bad as it was last year.' Higgins's victory was his third in four matches since the Crucial Crucial qualifiers, and he is now set to face either Mark Selby or Joe Perry in the second round. The Scot, who has been struggling with a back injury for years, said: 'I'He's playing really good stuff. I think I did well to get five frames.' Dott, who won the World Championship title in 2006, was a 10-8 winner earlier as he ousted Ricky Walden. H\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.83 ms /   527 runs   (    0.01 ms per token, 90348.02 tokens per second)\nllama_perf_context_print:        load time =    4478.42 ms\nllama_perf_context_print: prompt eval time =    1090.28 ms /   271 tokens (    4.02 ms per token,   248.56 tokens per second)\nllama_perf_context_print:        eval time =   44126.53 ms /   255 runs   (  173.05 ms per token,     5.78 tokens per second)\nllama_perf_context_print:       total time =   45263.23 ms /   526 tokens\nllama_perf_context_print:  mean exit layer =      27.51\nllama_perf_context_print:       exit ratio =       0.54  = 138 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 29, 32, 29, 32, 32, 32, 27, 32, 32, 32, 32, 32, 25, 32, 32, 32, 20, 32, 29, 32, 32, 31, 32, 32, 32, 29, 31, 27, 32, 25, 27, 31, 32, 29, 27, 29, 32, 25, 32, 20, 29, 29, 32, 32, 32, 29, 32, 25, 31, 29, 29, 29, 32, 16, 32, 20, 30, 32, 25, 23, 32, 27, 30, 32, 30, 22, 29, 32, 17, 32, 32, 16, 32, 25, 32, 32, 32, 32, 32, 32, 32, 32, 29, 31, 29, 20, 31, 32, 32, 31, 32, 29, 32, 32, 29, 32, 30, 32, 32, 16, 14, 25, 32, 32, 29, 29, 29, 30, 16, 32, 22, 32, 18, 32, 25, 32, 16, 27, 29, 32, 14, 32, 29, 29, 27, 27, 18, 31, 32, 29, 29, 32, 16, 18, 32, 32, 32, 32, 18, 32, 29, 18, 29, 32, 25, 20, 29, 16, 32, 32, 32, 18, 32, 18, 22, 17, 32, 32, 18, 32, 22, 32, 25, 25, 32, 17, 32, 15, 32, 15, 32, 32, 32, 25, 29, 32, 22, 32, 17, 32, 19, 18, 32, 32, 32, 32, 17, 15, 32, 22, 15, 32, 20, 32, 32, 15, 32, 17, 32, 27, 27, 17, 32, 32, 16, 32, 32, 20, 32, 18, 18, 32, 25, 32, 22, 32, 22, 22, 22, 32, 27, 25, 32, 18, 32, 32, 25, 27, 25, 29, 20, 23, 32, 32, 18, 32, 16, 18, 32, 18, 29, 16, 32, 32, 32, 18, 32, 25, 25, 27, 22, 32, 32, 27, 32, }\n"
    },
    {
        "inference_time": "34178.91",
        "tokens": "472",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Barcelona stars Neymar, Dani Alves and Adriano looked delighted following their vital win over Celta Vigo in La Liga - but their choice of matching denim was somewhat questionable. The flamboyant Brazilians all played their part in helping the Catalonian side return home with all three points, following Jeremy Mathieu's winning goal. Neymar took to Instagram after his side returned to the summit of La Liga: 'We got home...important victory...Congratulations guys!!! Vamo to top.' Dani Alves (from left), Adriano and Neymar were delighted with their hard-fought victory over Celta Vigo. Dani Alves, who's being linked with a move away from the Nou Camp, also posted a similar picture of the trio. He said: 'A great day at work, a little trairagem in the picture lol Return to home with my brothas @neymarjr @adriano21c. Joy, joy , one is that every day.' Game winner Mathieu admits his side were poor and puts it down to the international break. The three Brazilians all started for Barcelona and played a cruc -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Barcelona stars Neymar, Dani Alves and Adriano looked delighted following their vital win over Celta Vigo in La Liga - but their choice of matching denim was somewhat questionable. The flamboyant Brazilians all played their part in helping the Catalonian side return home with all three points, following Jeremy Mathieu's winning goal. Neymar took to Instagram after his side returned to the summit of La Liga: 'We got home...important victory...Congratulations guys!!! Vamo to top.' Dani Alves (from left), Adriano and Neymar were delighted with their hard-fought victory over Celta Vigo. Dani Alves, who's being linked with a move away from the Nou Camp, also posted a similar picture of the trio. He said: 'A great day at work, a little trairagem in the picture lol Return to home with my brothas @neymarjr @adriano21c. Joy, joy , one is that every day.' Game winner Mathieu admits his side were poor and puts it down to the international break. The three Brazilians all started for Barcelona and played a crucila role in securing the win over Celta Vigo.\n\nMy thoughts:\n\nIt seems like Barcelona players Neymar, Dani Alves and Adriano are quite pleased with themselves after their win over Celta Vigo in La Liga. Their choice of matching denim outfits, however, has raised some question marks. The three players all played important role in their team's victory, with Neymar scoring the winning goal. Dani Alves, who is reportedly being linked with a move away from Barcelona, also posting a similar picture of the trio. He expressed his joy and gratitude for the team's victory, using the Portuguese term \"trairagem\" which means \"travesty\" or \"farce\". Meanwhile, game winner Mathieu acknowledged that their side was poor and attributed it to the international break. Overall, it seems like the Brazilian players are happy with their hard-fought victory over Celta Vigo. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.65 ms /   473 runs   (    0.01 ms per token, 101829.92 tokens per second)\nllama_perf_context_print:        load time =    4515.28 ms\nllama_perf_context_print: prompt eval time =    1111.13 ms /   267 tokens (    4.16 ms per token,   240.30 tokens per second)\nllama_perf_context_print:        eval time =   33028.89 ms /   205 runs   (  161.12 ms per token,     6.21 tokens per second)\nllama_perf_context_print:       total time =   34178.91 ms /   472 tokens\nllama_perf_context_print:  mean exit layer =      26.59\nllama_perf_context_print:       exit ratio =       0.61  = 127 / 207 \nllama_perf_context_print:       exit layer = {32, 32, 20, 25, 27, 32, 32, 32, 22, 25, 32, 22, 23, 27, 30, 32, 32, 28, 32, 29, 31, 31, 32, 18, 27, 25, 27, 25, 25, 29, 25, 22, 22, 32, 20, 16, 32, 18, 17, 32, 29, 32, 15, 32, 20, 25, 20, 32, 22, 25, 25, 27, 32, 20, 29, 30, 32, 22, 20, 32, 27, 32, 32, 32, 32, 16, 29, 32, 32, 30, 20, 32, 30, 32, 32, 20, 32, 30, 32, 17, 22, 28, 18, 32, 32, 18, 32, 29, 25, 22, 25, 32, 32, 20, 22, 32, 32, 20, 25, 20, 30, 32, 27, 32, 16, 32, 25, 32, 17, 18, 32, 18, 32, 27, 18, 18, 17, 16, 32, 15, 27, 22, 27, 32, 32, 32, 32, 25, 32, 32, 18, 27, 29, 20, 29, 31, 22, 32, 32, 32, 32, 24, 20, 32, 25, 24, 22, 30, 18, 32, 32, 32, 32, 32, 29, 17, 27, 32, 32, 29, 32, 30, 22, 24, 32, 18, 32, 32, 24, 22, 32, 20, 32, 32, 32, 32, 25, 18, 32, 20, 32, 32, 16, 32, 27, 16, 25, 32, 25, 32, 20, 29, 32, 22, 17, 32, 16, 32, 29, 20, 20, 25, 22, 22, 24, 29, 16, }\n"
    },
    {
        "inference_time": "30493.25",
        "tokens": "458",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Chelsea boss Jose Mourinho is planning a summer swoop for Atletico Madrid midfielder Koke. The club's technical director Michael Emenalo travelled to Spain on Saturday to watch the 23-year-old during Atletico's 2-1 victory against Deportivo, according to Spanish newspaper AS. The highly-rated Spaniard signed a long-term deal with Diego Simeone's side only last year, but Mourinho is reportedly planning to offer second-choice left-back Filipe Luis to his former club as a makeweight for the deal. Chelsea boss Jose Mourinho could be planning a summer swoop for Atletico Madrid midfielder Koke (above) Spanish newspaper AS report that Mourinho could offer Filipe Luis in a part-exchange deal this summer. Luis has struggled to make an impact at Stamford Bridge this season after failing to displace Spanish defender Cesar Azpilicueta from the left-back spot. AS also report that Koke's release clause is around \u00a350million which Chelsea could be willing to shell out as Mourinho looks to add to a midfield that ha -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Chelsea boss Jose Mourinho is planning a summer swoop for Atletico Madrid midfielder Koke. The club's technical director Michael Emenalo travelled to Spain on Saturday to watch the 23-year-old during Atletico's 2-1 victory against Deportivo, according to Spanish newspaper AS. The highly-rated Spaniard signed a long-term deal with Diego Simeone's side only last year, but Mourinho is reportedly planning to offer second-choice left-back Filipe Luis to his former club as a makeweight for the deal. Chelsea boss Jose Mourinho could be planning a summer swoop for Atletico Madrid midfielder Koke (above) Spanish newspaper AS report that Mourinho could offer Filipe Luis in a part-exchange deal this summer. Luis has struggled to make an impact at Stamford Bridge this season after failing to displace Spanish defender Cesar Azpilicueta from the left-back spot. AS also report that Koke's release clause is around \u00a350million which Chelsea could be willing to shell out as Mourinho looks to add to a midfield that ha already been bolstered by the signings of N'Golo Kante and Pedro.\n\nJose Mourinho is planning a summer swoop for Atletico Madrid midfielder Koke, according to reports.\n\nThe Chelsea boss sent his technical director Michael Emenalo to watch him during Atletico's 2-1 victory against Deportivo on Saturday, according to sources in Spanish newspaper AS.\n\nKoke, who signed a long-term contract with Diego Simeone's side last year, could be available for transfer this summer with a price tag of around \u00a350million. Chelsea could be willingness to shell out that amount as part of a deal that could also include second-choice left-back Filipe Luis, who has struggled to make an impact at Stamford Bridge since transferring from Atletico in 2014.\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.35 ms /   459 runs   (    0.01 ms per token, 105541.50 tokens per second)\nllama_perf_context_print:        load time =    4515.87 ms\nllama_perf_context_print: prompt eval time =    1094.31 ms /   266 tokens (    4.11 ms per token,   243.08 tokens per second)\nllama_perf_context_print:        eval time =   29362.89 ms /   192 runs   (  152.93 ms per token,     6.54 tokens per second)\nllama_perf_context_print:       total time =   30493.25 ms /   458 tokens\nllama_perf_context_print:  mean exit layer =      26.10\nllama_perf_context_print:       exit ratio =       0.59  = 115 / 194 \nllama_perf_context_print:       exit layer = {32, 32, 17, 32, 32, 16, 17, 32, 32, 32, 14, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 25, 22, 17, 32, 18, 32, 18, 25, 27, 16, 32, 25, 18, 18, 20, 32, 27, 32, 25, 18, 30, 20, 22, 20, 32, 32, 32, 32, 32, 32, 27, 29, 18, 32, 32, 22, 32, 24, 32, 24, 25, 32, 24, 18, 32, 24, 24, 17, 32, 32, 17, 18, 18, 19, 32, 17, 32, 17, 32, 19, 32, 22, 27, 16, 32, 24, 32, 22, 26, 26, 22, 32, 22, 32, 18, 32, 27, 20, 18, 18, 17, 27, 27, 32, 25, 25, 20, 30, 32, 18, 18, 28, 32, 16, 32, 22, 27, 32, 22, 32, 32, 20, 32, 22, 25, 22, 32, 17, 32, 16, 32, 25, 32, 27, 27, 32, 20, 16, 29, 17, 32, 32, 32, 29, 15, 27, 18, 27, 32, 30, 27, 18, 32, 16, 32, 20, 16, 32, 32, 22, 25, 32, 20, 27, 32, 25, 29, 30, 27, 17, 32, 32, 27, 17, 32, 20, 18, 32, 16, 32, 25, 22, 32, 32, 29, 30, 29, 32, 20, 32, 28, }\n"
    },
    {
        "inference_time": "29360.36",
        "tokens": "535",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Tom Ballard plans to climb K2, 20 years after his mother died descending from its peak. A climber has set his sights on the summit of K2 - the second highest mountain in the world - 20 years after his mother died attempting the same feat. Alison Hargreaves, 32, was swept to her death in 260mph winds on her descent from the Himalayan mountain in 1995. Now her son Tom Ballard, one of the world's most accomplished climbers, plans to conquer K2 himself. Mr Ballard, 26, told The Times\u00a0that he saw it as his destiny to follow in his mother's footsteps. He said: 'I think it must be. At one stage I thought I would go and climb mountains for her. Then I realised that was actually a bit stupid because she had already climbed them herself. I realised I was climbing for myself. That's how I've gone on.' Last month Mr Ballard became the first to solo climb all six of the great north faces of the Alps in the winter season - and now he is training for K2. The climber was just six years old when his mother died whi -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Tom Ballard plans to climb K2, 20 years after his mother died descending from its peak. A climber has set his sights on the summit of K2 - the second highest mountain in the world - 20 years after his mother died attempting the same feat. Alison Hargreaves, 32, was swept to her death in 260mph winds on her descent from the Himalayan mountain in 1995. Now her son Tom Ballard, one of the world's most accomplished climbers, plans to conquer K2 himself. Mr Ballard, 26, told The Times\u00a0that he saw it as his destiny to follow in his mother's footsteps. He said: 'I think it must be. At one stage I thought I would go and climb mountains for her. Then I realised that was actually a bit stupid because she had already climbed them herself. I realised I was climbing for myself. That's how I've gone on.' Last month Mr Ballard became the first to solo climb all six of the great north faces of the Alps in the winter season - and now he is training for K2. The climber was just six years old when his mother died whi...\nTom Ballard, a climbers, plans to climb Mount Mount Everest, the highest mountain in the world, after his mother died attempting to climb it 20 years ago. Alison Hargreaves, 32, was swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe swe\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.62 ms /   536 runs   (    0.01 ms per token, 95441.60 tokens per second)\nllama_perf_context_print:        load time =    4488.94 ms\nllama_perf_context_print: prompt eval time =    1091.34 ms /   280 tokens (    3.90 ms per token,   256.57 tokens per second)\nllama_perf_context_print:        eval time =   28221.97 ms /   255 runs   (  110.67 ms per token,     9.04 tokens per second)\nllama_perf_context_print:       total time =   29360.36 ms /   535 tokens\nllama_perf_context_print:  mean exit layer =      23.09\nllama_perf_context_print:       exit ratio =       0.92  = 236 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 25, 27, 32, 32, 27, 27, 32, 27, 16, 32, 29, 25, 22, 25, 32, 32, 29, 22, 22, 29, 30, 16, 28, 20, 32, 18, 27, 32, 29, 18, 32, 25, 32, 27, 25, 16, 18, 30, 32, 25, 32, 23, 25, 23, 32, 32, 18, 32, 18, 32, 32, 22, 20, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 22, 22, 22, 22, 22, 24, 24, 22, 22, 22, 22, 22, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, }\n"
    },
    {
        "inference_time": "23108.58",
        "tokens": "423",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Trillions of pounds worth of oil \u2013 as much as the entire North Sea fields \u2013 lies beneath an area of England dubbed \u2018Britain\u2019s Dallas\u2019, it was claimed yesterday. Analysis suggests there is up to 100billion barrels of the fossil fuel under the Home Counties. The potential goldmine in the Weald Basin, across Surrey, Sussex, Hampshire and Kent, could meet up to a third of Britain\u2019s oil demand within 15 years, according to the consortium exploring the area. Scroll down for video. UK Oil & Gas Investments say they have discovered 100billion barrels worth of oil reserves in the Weald Basin, near Gatwick airport (pictured, the well head at Horse Hill) A map showing the location of the oil and gas deposits within the Weald Basin, which are thought to lie at between 2,500ft and 3,000ft below the ground. This could reduce reliance on oil from the Middle East and Russia and could be a major boost for British industry at a time when the North Sea supply is dwindling. But industry experts remained sceptical of t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Trillions of pounds worth of oil \u2013 as much as the entire North Sea fields \u2013 lies beneath an area of England dubbed \u2018Britain\u2019s Dallas\u2019, it was claimed yesterday. Analysis suggests there is up to 100billion barrels of the fossil fuel under the Home Counties. The potential goldmine in the Weald Basin, across Surrey, Sussex, Hampshire and Kent, could meet up to a third of Britain\u2019s oil demand within 15 years, according to the consortium exploring the area. Scroll down for video. UK Oil & Gas Investments say they have discovered 100billion barrels worth of oil reserves in the Weald Basin, near Gatwick airport (pictured, the well head at Horse Hill) A map showing the location of the oil and gas deposits within the Weald Basin, which are thought to lie at between 2,500ft and 3,000ft below the ground. This could reduce reliance on oil from the Middle East and Russia and could be a major boost for British industry at a time when the North Sea supply is dwindling. But industry experts remained sceptical of t\nSummary: Tr tr trillions of pounds worth of oil have been found beneath an area called 'Britain\u2019s Dallas', which covers parts of England including the Home Counties. The potential oil reserves are estimated to be worth up to 100 billion barrels, which could meet up to third of Britain's oil demand within 15 years. The discovery is made by UK oil and gas investors in the Weald Basin, which covers Surrey, Sussex, Hampshire and Kent. This could be a major boost for British industry at a time when the North Sea supply is dwindling. However, industry experts remain scept ske ske ske ske ske skeleton about the findings. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.21 ms /   424 runs   (    0.01 ms per token, 131881.80 tokens per second)\nllama_perf_context_print:        load time =    4507.60 ms\nllama_perf_context_print: prompt eval time =    1094.38 ms /   272 tokens (    4.02 ms per token,   248.54 tokens per second)\nllama_perf_context_print:        eval time =   21986.28 ms /   151 runs   (  145.60 ms per token,     6.87 tokens per second)\nllama_perf_context_print:       total time =   23108.58 ms /   423 tokens\nllama_perf_context_print:  mean exit layer =      25.69\nllama_perf_context_print:       exit ratio =       0.65  = 100 / 153 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 25, 22, 22, 20, 32, 22, 18, 32, 16, 18, 32, 20, 18, 32, 32, 17, 15, 32, 29, 31, 32, 17, 16, 32, 27, 32, 32, 32, 20, 32, 22, 32, 32, 24, 24, 20, 32, 22, 22, 32, 20, 32, 32, 22, 29, 32, 24, 26, 28, 28, 18, 32, 24, 32, 20, 32, 29, 30, 18, 32, 29, 27, 18, 32, 18, 27, 32, 25, 22, 18, 32, 22, 18, 32, 29, 31, 32, 32, 17, 19, 32, 22, 32, 20, 32, 16, 32, 27, 29, 25, 23, 27, 31, 18, 32, 32, 18, 16, 32, 18, 18, 32, 14, 16, 32, 30, 30, 18, 16, 29, 18, 20, 17, 32, 22, 20, 30, 32, 20, 32, 20, 32, 20, 32, 18, 29, 32, 20, 32, 29, 22, 18, 32, 29, 31, 27, 22, 22, 20, 22, 20, 20, 17, 32, 27, 32, 32, 30, 20, }\n"
    },
    {
        "inference_time": "37954.56",
        "tokens": "496",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The British Veterinary Association has withdrawn its support for the shooting of free-running badgers in controversial pilot culls. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. It concluded that the first two years of culling had not demonstrated conclusively that \u2018controlled shooting\u2019 of free-running badgers could be effective and humane. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. The BVA said it still supported badger culling to tackle tuberculosis in cattle. Animal welfare campaigners welcomed the news but said the BVA\u2019s ongoing support for cage-trapping and shooting was \u2018regrettable\u2019. It comes after the Environment Secretary promised to roll out more culls across the country if the Tories win the election. Addressing the National Farmers Union conference in February, Liz Truss sai -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The British Veterinary Association has withdrawn its support for the shooting of free-running badgers in controversial pilot culls. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. It concluded that the first two years of culling had not demonstrated conclusively that \u2018controlled shooting\u2019 of free-running badgers could be effective and humane. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. The BVA said it still supported badger culling to tackle tuberculosis in cattle. Animal welfare campaigners welcomed the news but said the BVA\u2019s ongoing support for cage-trapping and shooting was \u2018regrettable\u2019. It comes after the Environment Secretary promised to roll out more culls across the country if the Tories win the election. Addressing the National Farmers Union conference in February, Liz Truss sai...\nThe British Veterinary Association (BVA) has withdrawn its support for the shooting of free-zing badgers in controversial pilot c culls in Somerset and Gloucestershire. The association concluded that the first two years of culling had not demonstrated conclusively that 'controlled shooting of free-zing badgers could be effective and humane.\nThe BVA said it still supported badger culling to tackle tuberculosis in cattle, but it would now instead instead of using the controversial method of cage-trapping and then shooting badgers, it instead advocated for the use of non-leth methods to control the population.\nAnimal welfare campaigners welcomed the news but said the BVA\u2019s ongoing support for cage-trapping and then shooting was still 'regrettable'.\nThe decision comes after the Environment Secretary promised to roll out more culling across the country if the Tories win the election. Addressing the National Farmers Union conference in February, Liz Truss said the government would continue to support badger culling to prevent the spread of tuberculosis in cattle. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.46 ms /   497 runs   (    0.01 ms per token, 91109.07 tokens per second)\nllama_perf_context_print:        load time =    4583.86 ms\nllama_perf_context_print: prompt eval time =    1086.02 ms /   247 tokens (    4.40 ms per token,   227.44 tokens per second)\nllama_perf_context_print:        eval time =   36822.63 ms /   249 runs   (  147.88 ms per token,     6.76 tokens per second)\nllama_perf_context_print:       total time =   37954.56 ms /   496 tokens\nllama_perf_context_print:  mean exit layer =      25.69\nllama_perf_context_print:       exit ratio =       0.59  = 147 / 251 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 20, 32, 22, 16, 20, 29, 29, 25, 32, 17, 29, 32, 32, 32, 18, 32, 20, 32, 17, 32, 18, 17, 25, 32, 18, 32, 18, 32, 22, 22, 25, 23, 27, 21, 17, 32, 21, 27, 32, 29, 20, 32, 25, 32, 18, 32, 18, 16, 32, 16, 22, 24, 16, 29, 17, 32, 24, 18, 32, 32, 30, 16, 32, 22, 32, 18, 16, 32, 18, 32, 20, 25, 18, 32, 27, 20, 32, 32, 27, 25, 32, 32, 16, 32, 32, 20, 32, 25, 17, 22, 22, 25, 22, 18, 32, 16, 32, 18, 32, 16, 32, 32, 22, 17, 14, 32, 19, 32, 32, 21, 17, 32, 29, 32, 16, 32, 27, 29, 16, 32, 32, 17, 29, 29, 18, 32, 32, 16, 32, 17, 22, 32, 15, 32, 32, 18, 32, 25, 32, 18, 32, 32, 29, 32, 27, 27, 32, 22, 32, 32, 27, 32, 27, 18, 32, 32, 25, 32, 25, 25, 20, 32, 16, 32, 17, 32, 22, 15, 32, 22, 32, 18, 32, 20, 16, 32, 32, 29, 17, 32, 32, 31, 32, 32, 18, 16, 32, 22, 32, 24, 32, 24, 16, 32, 24, 14, 32, 29, 17, 32, 27, 29, 32, 27, 29, 18, 22, 32, 18, 32, 17, 32, 18, 32, 22, 17, 32, 17, 32, 20, 32, 32, 22, 32, 24, 32, 20, 32, 32, 32, 17, 32, 27, 15, 32, 17, 32, 15, 32, 29, 22, 18, 32, 20, 20, 30, 20, }\n"
    },
    {
        "inference_time": "11965.25",
        "tokens": "305",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Modern technology is supposed to be making our lives easier, but often all it does is create more pitfalls. 21st century additions like a Facebook page, an Instagram profile and a smart phone often end up doing more damage to a budding relationship than good. These days we are more connected than ever, but that regularly pushes a potential partner further away. Smart phones and social media often cause nightmare for men and women on dates (picture posed by models) On a date in the 1990s, the biggest fails a person could be responsible for were a lack of conversation, food in the teeth or a man asking\u00a0the\u00a0woman to split the bill. Nowadays, the man or woman is also likely to try and\u00a0photograph\u00a0their food or keep\u00a0checking\u00a0their phone at\u00a0the\u00a0table. Girls are often caught out spending too much time in the bathroom, while they compare notes on their evening with friends as their date is sat back at the table. And after the evening is over both parties can fall foul of the temptation to get in touch too q -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Modern technology is supposed to be making our lives easier, but often all it does is create more pitfalls. 21st century additions like a Facebook page, an Instagram profile and a smart phone often end up doing more damage to a budding relationship than good. These days we are more connected than ever, but that regularly pushes a potential partner further away. Smart phones and social media often cause nightmare for men and women on dates (picture posed by models) On a date in the 1990s, the biggest fails a person could be responsible for were a lack of conversation, food in the teeth or a man asking\u00a0the\u00a0woman to split the bill. Nowadays, the man or woman is also likely to try and\u00a0photograph\u00a0their food or keep\u00a0checking\u00a0their phone at\u00a0the\u00a0table. Girls are often caught out spending too much time in the bathroom, while they compare notes on their evening with friends as their date is sat back at the table. And after the evening is over both parties can fall foul of the temptation to get in touch too q...\n\nQuestion: What does the author suggest about modern technology's effectiveness in improving relationships?\n\nAnswer answer: The article suggests that modern technology is not effective in improving relationships. Instead it creates more pitfalls and often pushes potential partner further away rather than bringing them together. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.35 ms /   306 runs   (    0.00 ms per token, 225997.05 tokens per second)\nllama_perf_context_print:        load time =    4543.72 ms\nllama_perf_context_print: prompt eval time =    1104.05 ms /   244 tokens (    4.52 ms per token,   221.01 tokens per second)\nllama_perf_context_print:        eval time =   10847.89 ms /    61 runs   (  177.83 ms per token,     5.62 tokens per second)\nllama_perf_context_print:       total time =   11965.25 ms /   305 tokens\nllama_perf_context_print:  mean exit layer =      26.70\nllama_perf_context_print:       exit ratio =       0.52  = 33 / 63 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 18, 18, 27, 32, 32, 32, 27, 20, 29, 32, 22, 22, 29, 31, 24, 18, 32, 32, 31, 32, 17, 29, 31, 19, 32, 16, 32, 18, 32, 27, 32, 25, 32, 22, 20, 32, 32, 22, 32, 20, 32, 16, 32, 32, 32, 16, 32, 20, 32, 18, 20, 17, 32, 32, 15, 32, 18, }\n"
    },
    {
        "inference_time": "61476.28",
        "tokens": "492",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Ahead of this weekend's Premier League action, Sportsmail will be providing you with all you need to know about every fixture, with team news, provisional squads, betting odds and Opta stats. Here is all the information you need for Tottenham's home clash with Aston Villa... Tottenham Hotspur vs Aston Villa (White Hart Lane) Team news. Tottenham. Kyle Walker and Hugo Lloris are absent for Tottenham, while Jan Vertonghen is a doubt for this weekend's match against Aston Villa. England right-back Walker is unavailable after sustaining a foot injury in last week's draw at Burnley - a match goalkeeper Lloris missed through a knee injury which continues to keep him sidelined. Kyle Walker (left) was injured for Tottenham after clashing with Burnley's Kieran Tripper last Sunday. Vertonghen (virus) is one of several doubts for the Premier League encounter at White Hart Lane, where Harry Kane again wears the captain's armband. Provisional squad: Vorm, Friedel, Yedlin, Chiriches, Kaboul, Dier, Vertonghen, Ro -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Ahead of this weekend's Premier League action, Sportsmail will be providing you with all you need to know about every fixture, with team news, provisional squads, betting odds and Opta stats. Here is all the information you need for Tottenham's home clash with Aston Villa... Tottenham Hotspur vs Aston Villa (White Hart Lane) Team news. Tottenham. Kyle Walker and Hugo Lloris are absent for Tottenham, while Jan Vertonghen is a doubt for this weekend's match against Aston Villa. England right-back Walker is unavailable after sustaining a foot injury in last week's draw at Burnley - a match goalkeeper Lloris missed through a knee injury which continues to keep him sidelined. Kyle Walker (left) was injured for Tottenham after clashing with Burnley's Kieran Tripper last Sunday. Vertonghen (virus) is one of several doubts for the Premier League encounter at White Hart Lane, where Harry Kane again wears the captain's armband. Provisional squad: Vorm, Friedel, Yedlin, Chiriches, Kaboul, Dier, Vertonghen, Rooney, Eriksen, Alli, Kane, Tottenham Hotspur vs Aston Villa (White Hart Lane) Team news. Aston Villa. Villa boss Remi Garde has no fresh injury concerns ahead of the trip to Tottenham, with the only players unavailable due to suspension being midfielder Carles Gil (three games) and defender Tommy Elphick (one game). Provisional squads: Guzan, Johnstone, Bunn, Hutton, Chester, Amavi, Elmohamady, Veretout, Gueye, Gil, Ayew, Gestede, Agbonlahor, Villa. Betting odds. Tottenham are 1/2 to win the match against Aston Villa, with the odds on a draw at 13/5 and a victory for Villa at 7/1. Opta stat. Harry Kane has scored 10 goals in his last 10 Premier League appearances home and away against Aston Villa. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.86 ms /   493 runs   (    0.01 ms per token, 101502.99 tokens per second)\nllama_perf_context_print:        load time =    5370.84 ms\nllama_perf_context_print: prompt eval time =    1630.65 ms /   274 tokens (    5.95 ms per token,   168.03 tokens per second)\nllama_perf_context_print:        eval time =   59798.04 ms /   218 runs   (  274.30 ms per token,     3.65 tokens per second)\nllama_perf_context_print:       total time =   61476.28 ms /   492 tokens\nllama_perf_context_print:  mean exit layer =      28.84\nllama_perf_context_print:       exit ratio =       0.38  = 83 / 220 \nllama_perf_context_print:       exit layer = {32, 32, 22, 32, 32, 32, 32, 20, 32, 32, 32, 32, 25, 29, 22, 17, 32, 32, 32, 27, 32, 25, 22, 20, 32, 32, 18, 32, 17, 32, 32, 32, 20, 22, 20, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 25, 32, 32, 16, 32, 25, 32, 32, 18, 32, 32, 32, 32, 32, 17, 32, 18, 15, 32, 32, 32, 32, 27, 32, 32, 32, 32, 32, 32, 32, 32, 32, 30, 17, 32, 32, 32, 32, 32, 32, 32, 32, 32, 29, 32, 25, 16, 32, 16, 32, 32, 32, 20, 32, 32, 20, 32, 32, 18, 32, 32, 20, 32, 32, 30, 32, 32, 22, 32, 32, 32, 32, 32, 17, 32, 32, 32, 22, 32, 32, 32, 18, 32, 30, 32, 32, 27, 32, 32, 17, 32, 32, 32, 32, 20, 32, 32, 32, 32, 20, 29, 29, 32, 18, 32, 32, 31, 32, 30, 31, 29, 32, 32, 22, 29, 32, 22, 22, 29, 32, 32, 16, 32, 29, 32, 20, 29, 29, 32, 29, 27, 32, 29, 32, 22, 24, 22, 32, 27, 32, 26, 32, 32, 32, 32, 25, 30, 32, 25, 27, 32, 32, 32, 32, 32, 27, 29, 32, 32, 31, 31, 27, 32, 20, 32, 20, 32, 32, 25, 32, 22, 20, 30, 32, }\n"
    },
    {
        "inference_time": "27895.17",
        "tokens": "425",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A mother who made the heartbreaking decision to give birth to her sick baby at just 20 weeks had her wedding dress redesigned so her tiny daughter could be buried in it. Jaye Cox and her husband, Matthew, decided to deliver their daughter Annabelle prematurely on May 5 last year after scans showed she had developed a serious form of Spina Bifida. Annabelle was born weighing just 290 grams at Canberra Hospital and her distraught parents were able to cradle her in the short five minutes she survived. Jaye and Matthew Cox delivered their daughter Annabelle\u00a0at just 20 weeks in Canberra Hospital after scans showed she had developed a serious form of Spina Bifida. 'We were planning a life for her and you think you're going to see her laugh, walk, talk, go to school, get married,' Ms Cox told Daily Mail Australia. 'But we were faced with this decision - do we let her go and free her of the suffering, or do we go ahead and bring her into the world and watch her suffer. 'My husband and I had to make the dec -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A mother who made the heartbreaking decision to give birth to her sick baby at just 20 weeks had her wedding dress redesigned so her tiny daughter could be buried in it. Jaye Cox and her husband, Matthew, decided to deliver their daughter Annabelle prematurely on May 5 last year after scans showed she had developed a serious form of Spina Bifida. Annabelle was born weighing just 290 grams at Canberra Hospital and her distraught parents were able to cradle her in the short five minutes she survived. Jaye and Matthew Cox delivered their daughter Annabelle\u00a0at just 20 weeks in Canberra Hospital after scans showed she had developed a serious form of Spina Bifida. 'We were planning a life for her and you think you're going to see her laugh, walk, talk, go to school, get married,' Ms Cox told Daily Mail Australia. 'But we were faced with this decision - do we let her go and free her of the suffering, or do we go ahead and bring her into the world and watch her suffer. 'My husband and I had to make the decisions that we thought would best for our little girl, but it was the hardest decision we ever had to make in our life.'\n\nThe couple decided to deliver their daughter prematurely so that her tiny body could be buried in their wedding dress redesigned for her funeral purposes. Jaye Cox, who was 20 weeks pre pregnant when she gave her daughter birth, said that she and her husband had to make the decision to end their daughter's life because they didn't want her to suffer. Annabelle was born weighing just 290 grams and lived for only five minutes before passing away. The couple decided to redesign their wed wedding dress so that their daughter could be buried in it instead of in a traditional casket. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.94 ms /   426 runs   (    0.01 ms per token, 108231.71 tokens per second)\nllama_perf_context_print:        load time =    5002.78 ms\nllama_perf_context_print: prompt eval time =    1113.70 ms /   260 tokens (    4.28 ms per token,   233.46 tokens per second)\nllama_perf_context_print:        eval time =   26747.16 ms /   165 runs   (  162.10 ms per token,     6.17 tokens per second)\nllama_perf_context_print:       total time =   27895.17 ms /   425 tokens\nllama_perf_context_print:  mean exit layer =      26.63\nllama_perf_context_print:       exit ratio =       0.61  = 102 / 167 \nllama_perf_context_print:       exit layer = {32, 32, 32, 29, 32, 25, 18, 29, 22, 22, 32, 32, 17, 22, 29, 31, 25, 32, 18, 32, 17, 32, 25, 25, 27, 25, 20, 29, 32, 31, 32, 32, 32, 20, 32, 20, 18, 32, 25, 32, 27, 17, 20, 23, 27, 27, 15, 32, 29, 25, 23, 20, 32, 21, 29, 17, 32, 21, 32, 32, 16, 32, 30, 25, 28, 22, 32, 17, 32, 32, 29, 29, 32, 32, 27, 25, 32, 16, 32, 27, 20, 22, 22, 30, 32, 32, 29, 32, 28, 16, 32, 26, 24, 32, 22, 28, 32, 22, 20, 32, 32, 16, 24, 32, 32, 29, 32, 32, 29, 27, 20, 30, 32, 18, 22, 29, 16, 32, 32, 14, 32, 29, 25, 17, 32, 23, 20, 32, 32, 32, 18, 32, 17, 32, 32, 32, 32, 32, 27, 32, 20, 32, 30, 32, 18, 25, 17, 32, 23, 32, 17, 32, 18, 32, 22, 24, 29, 20, 24, 18, 32, 30, 20, 32, 32, 28, 25, }\n"
    },
    {
        "inference_time": "1080.48",
        "tokens": "255",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: PC Ronnie Lungu has won a race discrimination case against Wiltshire Police. A black police constable has won a race discrimination case against his force after they did not promote him due to the colour of his skin. Ronnie Lungu was singled out as 'a marked man' by Wiltshire Police solely due to his race, the employment tribunal ruled. It found the 40-year-old was passed over for promotion in favour of white colleagues after his internal assessments were secretly downgraded to make him appear unworthy. It also heard evidence that derogatory comments were made about Mr Lungu based on his colour violating his dignity. When the PC raised concerns with his superiors they were not taken seriously and his complaints were brushed off as trivial. The force - which has only 16 ethnic minority officers out of a total of just over 1,000 - had denied the claims. But the tribunal ruled that the Zimbabwe-born officer was unlawfully discriminated against on grounds of race. Mr Lungu said: 'When I realised I was  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: PC Ronnie Lungu has won a race discrimination case against Wiltshire Police. A black police constable has won a race discrimination case against his force after they did not promote him due to the colour of his skin. Ronnie Lungu was singled out as 'a marked man' by Wiltshire Police solely due to his race, the employment tribunal ruled. It found the 40-year-old was passed over for promotion in favour of white colleagues after his internal assessments were secretly downgraded to make him appear unworthy. It also heard evidence that derogatory comments were made about Mr Lungu based on his colour violating his dignity. When the PC raised concerns with his superiors they were not taken seriously and his complaints were brushed off as trivial. The force - which has only 16 ethnic minority officers out of a total of just over 1,000 - had denied the claims. But the tribunal ruled that the Zimbabwe-born officer was unlawfully discriminated against on grounds of race. Mr Lungu said: 'When I realised I was  [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.03 ms /   255 runs   (    0.00 ms per token, 8225806.45 tokens per second)\nllama_perf_context_print:        load time =    4497.07 ms\nllama_perf_context_print: prompt eval time =    1078.35 ms /   254 tokens (    4.25 ms per token,   235.55 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    1080.48 ms /   255 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 2 \nllama_perf_context_print:       exit layer = {32, 32, }\n"
    },
    {
        "inference_time": "11295.68",
        "tokens": "328",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Usually parents get to work teaching children their ABCs. But one parent decided to teach their baby an altogether less practical trick. American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Footage shows him gnawing on the side of a drinks container before stopping and prompting his child to follow. The infant then obediently performs the same stunt with a water bottle of her own. The duo then sit together enjoying some bonding time with their simple props,. To date the clip has been watched more than 190,000 times, with many viewers deeming it 'cute' and 'adorable'. 'That's so awesome. I'm amazed at how she pays attention to you and copies. It must feel great to have a daughter like her,' one commenter wrote. Repeat after me: American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Home tutoring: Footage shows him gnawing on the side of a drinks container before sto -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Usually parents get to work teaching children their ABCs. But one parent decided to teach their baby an altogether less practical trick. American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Footage shows him gnawing on the side of a drinks container before stopping and prompting his child to follow. The infant then obediently performs the same stunt with a water bottle of her own. The duo then sit together enjoying some bonding time with their simple props,. To date the clip has been watched more than 190,000 times, with many viewers deeming it 'cute' and 'adorable'. 'That's so awesome. I'm amazed at how she pays attention to you and copies. It must feel great to have a daughter like her,' one commenter wrote. Repeat after me: American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Home tutoring: Footage shows him gnawing on the side of a drinks container before stooping and prompting his child to follow. The infant then obediently performs the same st trick with a water bottle of her own. The du duo then sit together enjoying some bonding time with their own simple props. The video has been viewed more than 19000 times with many viewers deemed it 'cute' and 'adorable. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.77 ms /   329 runs   (    0.01 ms per token, 186085.97 tokens per second)\nllama_perf_context_print:        load time =    4595.64 ms\nllama_perf_context_print: prompt eval time =    1107.94 ms /   248 tokens (    4.47 ms per token,   223.84 tokens per second)\nllama_perf_context_print:        eval time =   10171.54 ms /    80 runs   (  127.14 ms per token,     7.87 tokens per second)\nllama_perf_context_print:       total time =   11295.68 ms /   328 tokens\nllama_perf_context_print:  mean exit layer =      24.17\nllama_perf_context_print:       exit ratio =       0.67  = 55 / 82 \nllama_perf_context_print:       exit layer = {32, 32, 18, 25, 22, 17, 32, 19, 21, 20, 29, 31, 32, 16, 32, 25, 32, 16, 32, 32, 14, 32, 25, 18, 18, 32, 22, 25, 18, 18, 16, 16, 32, 22, 22, 32, 14, 32, 20, 32, 25, 32, 20, 32, 20, 16, 16, 14, 32, 22, 27, 32, 27, 27, 16, 32, 32, 14, 30, 27, 32, 14, 17, 14, 32, 29, 20, 32, 14, 32, 27, 27, 18, 32, 20, 32, 16, 20, 32, 14, 20, 20, }\n"
    },
    {
        "inference_time": "4589.07",
        "tokens": "276",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Taken at the dawn of photography, these unique pictures are some of the first ever taken of the city of Bristol, nearly 100 years before much of its historic centre was destroyed by Nazi bombing. The images, captured between 1850 and 1855 by photographer Hugh Owen, have surfaced after being put up for auction - and could fetch up to \u00a330,000. Everything from the waterfront, city centre and St Mary Redcliffe church can be seen in the shots, as well as\u00a0The Fourteen Stars Tavern building which was pulled down in 1857. It became part of the site for The Bristol Brewery Georges before that was closed in 1999 and later turned into a block of flats. Many of the buildings pictured have changed dramatically between the 19th century and today, particularly the area of High Street. Other shots are almost identical, including the one of St John's Gate at the Church of St John the Baptist, which shows that little is different in certain areas that escaped the heavy bombing during the Second World War. The albume -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Taken at the dawn of photography, these unique pictures are some of the first ever taken of the city of Bristol, nearly 100 years before much of its historic centre was destroyed by Nazi bombing. The images, captured between 1850 and 1855 by photographer Hugh Owen, have surfaced after being put up for auction - and could fetch up to \u00a330,000. Everything from the waterfront, city centre and St Mary Redcliffe church can be seen in the shots, as well as\u00a0The Fourteen Stars Tavern building which was pulled down in 1857. It became part of the site for The Bristol Brewery Georges before that was closed in 1999 and later turned into a block of flats. Many of the buildings pictured have changed dramatically between the 19th century and today, particularly the area of High Street. Other shots are almost identical, including the one of St John's Gate at the Church of St John the Baptist, which shows that little is different in certain areas that escaped the heavy bombing during the Second World War. The albume are expected to fetch upwards of \u00a3200-\u00a3300 at auction. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.49 ms /   277 runs   (    0.00 ms per token, 563008.13 tokens per second)\nllama_perf_context_print:        load time =    4548.28 ms\nllama_perf_context_print: prompt eval time =    1105.34 ms /   256 tokens (    4.32 ms per token,   231.60 tokens per second)\nllama_perf_context_print:        eval time =    3477.74 ms /    20 runs   (  173.89 ms per token,     5.75 tokens per second)\nllama_perf_context_print:       total time =    4589.07 ms /   276 tokens\nllama_perf_context_print:  mean exit layer =      27.86\nllama_perf_context_print:       exit ratio =       0.45  = 10 / 22 \nllama_perf_context_print:       exit layer = {32, 32, 32, 25, 32, 30, 28, 32, 20, 32, 25, 17, 32, 32, 22, 20, 32, 32, 32, 22, 32, 20, }\n"
    },
    {
        "inference_time": "41284.32",
        "tokens": "538",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A pro-Ukip businessman who forced Liam Fox to resign as Defence Secretary is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tycoon Harvey Boulter has already given \u00a330,000 to the Ukip candidate fighting Fox in his Somerset seat in a continued vendetta against him. Boulter is also spending another \u00a330,000 to try to defeat Tom Tugendhat, who is standing for the Tories in Tonbridge and West Malling, Kent. Scroll down for video. Pro-Ukip businessman Harvey Boulter (above), who forced Liam Fox to resign as Defence Secretary, is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tugendhat\u2019s father, Sir Michael Tugendhat, is the High Court judge who awarded Fox a six-figure sum in damages after he sued Boulter for libellous remarks made on Sky TV. Fox resigned from the Cabinet in 2011 after Boulter revealed details of how Fox\u2019s close friend, businessman Adam Werritty, accompanied him on overseas trips as Defence Secretary. Mr Boulter said: \u2018This may look like naked  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A pro-Ukip businessman who forced Liam Fox to resign as Defence Secretary is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tycoon Harvey Boulter has already given \u00a330,000 to the Ukip candidate fighting Fox in his Somerset seat in a continued vendetta against him. Boulter is also spending another \u00a330,000 to try to defeat Tom Tugendhat, who is standing for the Tories in Tonbridge and West Malling, Kent. Scroll down for video. Pro-Ukip businessman Harvey Boulter (above), who forced Liam Fox to resign as Defence Secretary, is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tugendhat\u2019s father, Sir Michael Tugendhat, is the High Court judge who awarded Fox a six-figure sum in damages after he sued Boulter for libellous remarks made on Sky TV. Fox resigned from the Cabinet in 2011 after Boulter revealed details of how Fox\u2019s close friend, businessman Adam Werritty, accompanied him on overseas trips as Defence Secretary. Mr Boulter said: \u2018This may look like naked \nambition but I am determined to defeat Tom Tugendhat because he is a Blairite who will betray the Tory Party\u2019s values.' Mr Tugendhat, who is standing for the Tories in Tonbridge and West Malling, Kent, is seen as a \u2018g golden boy\u2019 candidate who is likely to win his seat easily. Mr Boulter, who has already given \u00a330,0.00 to the Ukip candidate fighting Fox in his Somerset seat, is also spending another \u00a330,000 to try to defeat him. Mr Boulster claims he is motivated by a desire to revenge against Fox, who he believes wronged him by revealing details of how Mr Werritty accompanied him on overseas trips as Defence Secretary. Mr Boulter believes Mr Tugendhat will similar betray the party's values and is therefore a target target of his campaign. Mr Tugendhat's father, Sir Michael Togendhat, is the High Court judge who awarded Fox a six-figure sum in damages after he sued Boulster for libellous remarks made on Sky TV. Mr Tugendhat has\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.97 ms /   539 runs   (    0.01 ms per token, 90330.15 tokens per second)\nllama_perf_context_print:        load time =    4555.08 ms\nllama_perf_context_print: prompt eval time =    1121.27 ms /   283 tokens (    3.96 ms per token,   252.39 tokens per second)\nllama_perf_context_print:        eval time =   40112.96 ms /   255 runs   (  157.31 ms per token,     6.36 tokens per second)\nllama_perf_context_print:       total time =   41284.32 ms /   538 tokens\nllama_perf_context_print:  mean exit layer =      26.32\nllama_perf_context_print:       exit ratio =       0.58  = 149 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 17, 29, 32, 25, 27, 32, 25, 25, 27, 20, 32, 18, 29, 30, 32, 32, 32, 32, 32, 32, 32, 32, 32, 25, 20, 32, 29, 32, 30, 27, 20, 32, 25, 23, 32, 16, 32, 32, 17, 32, 27, 30, 18, 32, 27, 20, 32, 25, 25, 32, 22, 18, 32, 32, 24, 20, 32, 29, 24, 32, 30, 25, 32, 29, 18, 32, 22, 30, 20, 32, 29, 25, 32, 22, 20, 32, 16, 30, 32, 27, 20, 32, 17, 29, 29, 32, 32, 19, 27, 31, 32, 32, 17, 32, 32, 18, 17, 32, 17, 32, 25, 22, 16, 32, 25, 32, 20, 32, 17, 32, 19, 25, 19, 32, 21, 21, 32, 20, 19, 25, 32, 20, 21, 32, 32, 20, 32, 32, 25, 29, 20, 29, 17, 32, 16, 32, 32, 15, 32, 32, 32, 32, 32, 32, 32, 32, 27, 32, 20, 20, 20, 32, 22, 22, 32, 16, 32, 32, 20, 32, 32, 25, 18, 32, 27, 22, 29, 24, 32, 20, 32, 27, 32, 32, 20, 25, 27, 25, 27, 18, 32, 22, 32, 18, 17, 32, 16, 32, 32, 16, 32, 20, 17, 32, 22, 18, 32, 24, 22, 32, 22, 24, 16, 32, 17, 32, 18, 32, 17, 19, 25, 21, 32, 17, 32, 20, 19, 32, 18, 32, 25, 17, 32, 18, 32, 32, 18, 32, 18, 32, 29, 32, 27, 29, 20, 20, 20, 32, 27, 17, 32, 17, 32, 32, 18, 22, 24, 24, 32, 24, 25, 32, }\n"
    },
    {
        "inference_time": "39465.89",
        "tokens": "500",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Aston Villa boss Tim Sherwood has offered encouragement to a six-year-old fan he beat to the job. Charlie Pye, from Leamington Spa, applied for the Villa hotseat after Paul Lambert was sacked in February. The Villa fan wanted a chocolate bar as a win bonus and would have agreed terms for a signed claret and blue football, a WWE wrestling figure and some sweets. Sherwood wrote Charlie a letter congratulating him on his credentials, despite missing out on the job. Aston Villa boss Tim Sherwood (centre) showed his softer side by replying to 6-year-old Charlie Pye. He wrote: 'From your letter it is clear to see that you would indeed be an ideal candidate for the job due to your knowledge of the game - and your very capable backroom staff, of course! 'Hopefully you're not too disappointed at missing out and I'm informed that I just managed to pip you to the post so you were very close to being offered the job. 'Our chief executive Tom Fox has informed me that the club would like your permission to keep  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Aston Villa boss Tim Sherwood has offered encouragement to a six-year-old fan he beat to the job. Charlie Pye, from Leamington Spa, applied for the Villa hotseat after Paul Lambert was sacked in February. The Villa fan wanted a chocolate bar as a win bonus and would have agreed terms for a signed claret and blue football, a WWE wrestling figure and some sweets. Sherwood wrote Charlie a letter congratulating him on his credentials, despite missing out on the job. Aston Villa boss Tim Sherwood (centre) showed his softer side by replying to 6-year-old Charlie Pye. He wrote: 'From your letter it is clear to see that you would indeed be an ideal candidate for the job due to your knowledge of the game - and your very capable backroom staff, of course! 'Hopefully you're not too disappointed at missing out and I'm informed that I just managed to pip you to the post so you were very close to being offered the job. 'Our chief executive Tom Fox has informed me that the club would like your permission to keep 50 copies of your application letter on file for future reference purposes. 'Well done on a fantastic application and I hope you continue to follow the fortunes of our club with great interest. 'Best regards, Tim Sherwood'\n\nAnalysis: Tim Sherwood, the manager of Aston Villa, has shown a kind and humorous response to a letter of application from a six-year old fan named Charlie Pye who applied for the position of manager after Paul Lambert was sacked in February. Charlie Pye, who is from Leamington Spa, outlined his credentials for the job, including a desire for a chocolate bar as a bonus, a signed claret and blue football, a WWE wrestling figure and some sweet treats for his backroom team members. Sherwood responded to Charlie's letter with a letter con congratulate him on his credentials and expressed disappointment that he missed out on the job, but also offered to keep copies of his application letter for future reference purposes. The letter is humorous and lighthearted, showcasing Sherwood\u2019s softer side and his ability to humorously respond to a letter from a child. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.60 ms /   501 runs   (    0.01 ms per token, 89480.26 tokens per second)\nllama_perf_context_print:        load time =    4543.58 ms\nllama_perf_context_print: prompt eval time =    1100.67 ms /   253 tokens (    4.35 ms per token,   229.86 tokens per second)\nllama_perf_context_print:        eval time =   38316.58 ms /   247 runs   (  155.13 ms per token,     6.45 tokens per second)\nllama_perf_context_print:       total time =   39465.89 ms /   500 tokens\nllama_perf_context_print:  mean exit layer =      26.10\nllama_perf_context_print:       exit ratio =       0.61  = 151 / 249 \nllama_perf_context_print:       exit layer = {32, 32, 32, 17, 25, 17, 32, 25, 32, 32, 30, 17, 32, 17, 32, 32, 29, 32, 32, 32, 32, 32, 32, 32, 29, 16, 27, 30, 25, 25, 32, 32, 32, 23, 18, 23, 29, 31, 20, 31, 31, 32, 17, 31, 22, 32, 27, 31, 32, 32, 32, 18, 17, 32, 25, 30, 32, 29, 22, 24, 24, 22, 20, 29, 18, 32, 32, 29, 32, 17, 32, 18, 32, 17, 32, 32, 22, 19, 18, 27, 30, 22, 22, 32, 22, 22, 20, 22, 18, 32, 22, 27, 18, 16, 32, 32, 32, 32, 16, 32, 25, 27, 16, 27, 17, 25, 32, 27, 22, 32, 16, 32, 18, 32, 27, 17, 32, 29, 19, 32, 32, 32, 21, 29, 25, 17, 32, 15, 18, 32, 20, 17, 32, 25, 15, 32, 18, 32, 25, 32, 17, 32, 23, 32, 19, 32, 18, 30, 16, 32, 22, 32, 16, 32, 22, 32, 22, 32, 32, 27, 29, 25, 23, 18, 20, 29, 32, 32, 17, 32, 29, 32, 22, 32, 18, 32, 32, 32, 22, 25, 27, 29, 32, 32, 18, 32, 27, 29, 32, 32, 16, 17, 32, 32, 19, 15, 19, 17, 32, 17, 32, 29, 20, 22, 20, 16, 32, 24, 22, 17, 32, 20, 15, 32, 32, 25, 32, 25, 32, 27, 32, 22, 32, 29, 29, 32, 32, 32, 22, 24, 16, 32, 14, 32, 30, 32, 12, 32, 14, 32, 32, 25, 32, 25, 22, 25, 25, 29, 18, }\n"
    },
    {
        "inference_time": "4049.18",
        "tokens": "281",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Atletico Madrid turned up the heat on Valencia in the battle for La Liga's third automatic Champions League berth with club top scorer Antoine Griezmann setting them on the way to a 2-0 stroll at bottom side Cordoba on Saturday. The victory for the champions at Cordoba's El Arcangel stadium in balmy Andalucia lifted Atletico above Valencia and onto 62 points from 29 matches. Valencia have 60 points in fourth and can reclaim third spot with a win at home to sixth-placed Villarreal on Sunday. France forward Griezmann struck his 15th league goal of the campaign in the fifth minute after stealing the ball, surging forward from midfield and cracking a precise low shot into the bottom corner. Atletico Madrid forward Antoine Griezmann scored his 15th goal of the season by netting opener. Saul Niguez doubled Atletico's lead in the 39th minute to ensure his side leapfrogged Valencia. Valencia are currently two points behind Atletico Madrid but do have a game in hand over their top four rivals. Midfielder Sa -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Atletico Madrid turned up the heat on Valencia in the battle for La Liga's third automatic Champions League berth with club top scorer Antoine Griezmann setting them on the way to a 2-0 stroll at bottom side Cordoba on Saturday. The victory for the champions at Cordoba's El Arcangel stadium in balmy Andalucia lifted Atletico above Valencia and onto 62 points from 29 matches. Valencia have 60 points in fourth and can reclaim third spot with a win at home to sixth-placed Villarreal on Sunday. France forward Griezmann struck his 15th league goal of the campaign in the fifth minute after stealing the ball, surging forward from midfield and cracking a precise low shot into the bottom corner. Atletico Madrid forward Antoine Griezmann scored his 15th goal of the season by netting opener. Saul Niguez doubled Atletico's lead in the 39th minute to ensure his side leapfrogged Valencia. Valencia are currently two points behind Atletico Madrid but do have a game in hand over their top four rivals. Midfielder Saul Niguez scored his second goal of the season to second goal of the game. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.44 ms /   282 runs   (    0.00 ms per token, 638009.05 tokens per second)\nllama_perf_context_print:        load time =    4516.99 ms\nllama_perf_context_print: prompt eval time =    1090.94 ms /   263 tokens (    4.15 ms per token,   241.08 tokens per second)\nllama_perf_context_print:        eval time =    2952.77 ms /    18 runs   (  164.04 ms per token,     6.10 tokens per second)\nllama_perf_context_print:       total time =    4049.18 ms /   281 tokens\nllama_perf_context_print:  mean exit layer =      27.25\nllama_perf_context_print:       exit ratio =       0.70  = 14 / 20 \nllama_perf_context_print:       exit layer = {32, 32, 27, 22, 32, 32, 29, 25, 24, 23, 29, 20, 32, 18, 32, 27, 29, 20, 29, 31, }\n"
    },
    {
        "inference_time": "38883.47",
        "tokens": "530",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: An 18-year-old mother-of-one had two heart attacks before dying from multiple organ failure after taking M-Cat at a house party, her family have said. Abby Swinfield, from\u00a0Castle Donington, in Derbyshire, was partying with friends when she was rushed to hospital in the early hours of March 30. Miss Swinfield, who is believed to have a son, three, was taken to Good Hope Hospital in Sutton Coldfield, West Midlands, where she was placed in a coma, but died a week later. Abby Swinfield, 18, a mother-of-one from\u00a0Castle Donington, in Derbyshire, was rushed to hospital in the early hours of March 30 after a house party in Tamworth where it is thought she took M-Cat. Following Miss Swinfield's death, police have arrested one girl for supplying drugs, but Miss Swinfield's cousin has claimed she didn't take the drug voluntarily, saying her drink was spiked. Neighbours say ambulances arrived at this property at 1am. After arriving at hospital Miss Swinfield was put into a coma, but died a week later. Police a -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: An 18-year-old mother-of-one had two heart attacks before dying from multiple organ failure after taking M-Cat at a house party, her family have said. Abby Swinfield, from\u00a0Castle Donington, in Derbyshire, was partying with friends when she was rushed to hospital in the early hours of March 30. Miss Swinfield, who is believed to have a son, three, was taken to Good Hope Hospital in Sutton Coldfield, West Midlands, where she was placed in a coma, but died a week later. Abby Swinfield, 18, a mother-of-one from\u00a0Castle Donington, in Derbyshire, was rushed to hospital in the early hours of March 30 after a house party in Tamworth where it is thought she took M-Cat. Following Miss Swinfield's death, police have arrested one girl for supplying drugs, but Miss Swinfield's cousin has claimed she didn't take the drug voluntarily, saying her drink was spiked. Neighbours say ambulances arrived at this property at 1am. After arriving at hospital Miss Swinfield was put into a coma, but died a week later. Police ae investigating after an 18-year old mother died after taking M Cat at a party. Abby Swinfield was rushed to hospital after a houseparty in Tamworth where it is thought she took M Cat. She died a week later from multiple organ failure after suffering two heart heart attacks. Police have arrested one girl for supplementing drugs. Abby Swinfield's family have said she had two heart attack before she die from multiple organs failure after taking M Cat at a party. Abby Swinfield, 18, a mother of one from Castle Donington, in Derbyshire, was rushed to hospital after a party in Tamworth where she is believed to have taken M Cat. She died a week later from multiple organs failure after suffering two heart heart attack. Police have been investigating after an 18- year old mother died after taking M Cat at a party. Abbie Swinfield, 18, a mother of one from Castle Donington, in Derbyshire, was rushed to hospital after a party in Tamworth where she is believed to have taken M Cat. She died a week later from multiple organs failure after suffering two heart heart attack. Police have been investigating\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.90 ms /   531 runs   (    0.01 ms per token, 90076.34 tokens per second)\nllama_perf_context_print:        load time =    4504.77 ms\nllama_perf_context_print: prompt eval time =    1101.27 ms /   275 tokens (    4.00 ms per token,   249.71 tokens per second)\nllama_perf_context_print:        eval time =   37732.34 ms /   255 runs   (  147.97 ms per token,     6.76 tokens per second)\nllama_perf_context_print:       total time =   38883.47 ms /   530 tokens\nllama_perf_context_print:  mean exit layer =      25.66\nllama_perf_context_print:       exit ratio =       0.61  = 156 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 30, 25, 32, 17, 32, 23, 32, 25, 32, 29, 30, 30, 32, 29, 16, 32, 27, 18, 32, 25, 18, 20, 27, 22, 24, 18, 20, 32, 24, 16, 32, 32, 25, 18, 32, 27, 20, 18, 32, 29, 30, 28, 32, 32, 27, 18, 32, 32, 32, 25, 18, 32, 32, 32, 20, 18, 27, 29, 32, 32, 32, 17, 32, 27, 32, 18, 29, 25, 17, 32, 27, 18, 32, 25, 18, 17, 32, 20, 32, 22, 32, 32, 18, 18, 22, 32, 29, 18, 18, 32, 22, 32, 18, 32, 32, 22, 22, 32, 20, 17, 29, 27, 18, 32, 25, 18, 32, 32, 25, 32, 27, 17, 32, 22, 32, 17, 32, 24, 17, 32, 18, 32, 32, 16, 32, 25, 22, 25, 18, 20, 32, 18, 32, 32, 22, 18, 24, 29, 29, 22, 18, 32, 30, 24, 32, 30, 18, 32, 18, 32, 32, 32, 22, 32, 18, 32, 32, 16, 20, 17, 22, 22, 32, 32, 15, 32, 25, 22, 27, 30, 32, 20, 32, 28, 17, 32, 25, 32, 23, 25, 22, 32, 17, 32, 27, 27, 20, 32, 25, 18, 32, 32, 20, 18, 27, 18, 32, 22, 32, 17, 32, 22, 18, 32, 17, 32, 32, 17, 18, 32, 22, 24, 17, 32, 17, 32, 17, 32, 22, 17, 32, 17, 32, 18, 32, 25, 25, 23, 17, 32, 18, 32, 18, 32, 18, 32, 17, 32, 22, 32, 32, 15, 32, 18, 20, 17, 32, 18, 15, 32, 17, }\n"
    },
    {
        "inference_time": "38038.68",
        "tokens": "490",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A playground in Tokyo has been found to have dangerously high levels of radiation, which has raised fears for the health of children in the area. Soil underneath a slide at the park in the Toshima ward in the north-east of the Japanese capital, showed radiation readings of up to 480 microsieverts per hour. This is nearly half the recommended annual limit of exposure for a person. Danger: Japanese authorities detected an unusually high level of radiation around playground equipment in this Tokyo park (pictured) Safety: Authorities cordoned off the park after high levels of radiation were detected underneath a slide. It was not immediately clear why radiation levels were so high in the park, which opened two years after the 2011 Fukushima disaster. Authorities believe that the radiation could be coming from something buried underneath the park. The playground has now been fenced off with an orange barricade. Park representative Kyoko Tsujimura said: 'We are still checking what caused these elevated r -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A playground in Tokyo has been found to have dangerously high levels of radiation, which has raised fears for the health of children in the area. Soil underneath a slide at the park in the Toshima ward in the north-east of the Japanese capital, showed radiation readings of up to 480 microsieverts per hour. This is nearly half the recommended annual limit of exposure for a person. Danger: Japanese authorities detected an unusually high level of radiation around playground equipment in this Tokyo park (pictured) Safety: Authorities cordoned off the park after high levels of radiation were detected underneath a slide. It was not immediately clear why radiation levels were so high in the park, which opened two years after the 2011 Fukushima disaster. Authorities believe that the radiation could be coming from something buried underneath the park. The playground has now been fenced off with an orange barricade. Park representative Kyoko Tsujimura said: 'We are still checking what caused these elevated radaition levels, but we are taking all necessary steps to safety of children.'\n\nRadiation levels at a playground in Tokyo have been found to have been dangerously high levels of radiation, which has raised fears for the health of children in the area. Soil samples taken from underneath a slide at a park in the T Tokyo ward in the north-east of the city showed radiation levels of up to 480 microsieverts per hour. This is nearly half the recommended annual limit of exposure for a person.\nJ Japanese authorities detected the high levels of radiation around playground equipment in the park, which opened two years ago after the Fukushima nuclear disaster in 2011. The park has nowhere near the nuclear power plant. Authorities believe that the radiation could be coming from something buried undernequently, but we are taking all necessary steps to safety of children. Park representative Kyoko Tsujimura said: ' we are still checking what caused these elevated radiation levels, but we are taking all necessary steps to ensure safety children. Park representative Kyoto Tsujimura said: ' we still checking what caused these elevated radiation levels but we are taking all\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.17 ms /   491 runs   (    0.01 ms per token, 79630.23 tokens per second)\nllama_perf_context_print:        load time =    4550.43 ms\nllama_perf_context_print: prompt eval time =    1085.40 ms /   235 tokens (    4.62 ms per token,   216.51 tokens per second)\nllama_perf_context_print:        eval time =   36902.93 ms /   255 runs   (  144.72 ms per token,     6.91 tokens per second)\nllama_perf_context_print:       total time =   38038.68 ms /   490 tokens\nllama_perf_context_print:  mean exit layer =      24.88\nllama_perf_context_print:       exit ratio =       0.62  = 160 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 17, 32, 17, 19, 32, 32, 32, 15, 13, 27, 20, 32, 22, 32, 32, 32, 32, 32, 17, 30, 32, 22, 15, 32, 22, 32, 15, 32, 22, 17, 15, 32, 27, 20, 15, 32, 20, 29, 17, 32, 22, 29, 15, 32, 17, 32, 20, 32, 29, 17, 32, 20, 32, 27, 27, 32, 27, 32, 16, 32, 18, 32, 17, 29, 25, 18, 32, 23, 21, 32, 18, 30, 17, 32, 27, 17, 16, 17, 17, 32, 22, 20, 29, 18, 32, 29, 22, 18, 32, 22, 16, 32, 17, 32, 17, 20, 32, 17, 32, 27, 15, 32, 22, 18, 17, 32, 20, 16, 32, 19, 32, 29, 32, 29, 20, 18, 32, 32, 22, 20, 16, 27, 32, 27, 14, 17, 18, 29, 18, 32, 19, 32, 18, 32, 17, 18, 32, 27, 22, 32, 24, 25, 27, 22, 32, 29, 29, 30, 32, 27, 20, 32, 27, 32, 17, 15, 32, 32, 19, 27, 32, 32, 32, 18, 32, 17, 17, 32, 18, 16, 32, 19, 18, 32, 21, 16, 32, 32, 32, 29, 18, 17, 32, 29, 15, 32, 15, 17, 32, 17, 32, 32, 20, 29, 32, 32, 20, 32, 22, 32, 16, 32, 27, 18, 20, 16, 32, 20, 25, 18, 32, 17, 32, 18, 32, 16, 32, 14, 32, 29, 14, 32, 14, 32, 20, 27, 30, 27, 20, 32, 27, 32, 20, 32, 25, 32, 16, 32, 20, 18, 18, 32, 17, 32, 18, 32, 18, 32, 17, 15, 15, 32, 32, 18, }\n"
    },
    {
        "inference_time": "22335.86",
        "tokens": "530",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Amir Khan believes Kell Brook has called him out so that he can have the 'one big payday' which has so far eluded him during his 10-and-a-half year career. Brook, who defeated Jo Jo Dan to retain his IBF welterweight title last month, immediately challenged Khan in a post-fight interview but the Bolton-born boxer has instead agreed a bout on May 30, with his opponent yet to be confirmed. Khan has reiterated his intention to take on Brook within the next 12 months, although he insists that the undefeated 28-year-old only wants the fight because of the money it will earn him. Amir Khan (left), pictured during his last fight against Devon Alexander, has blasted Kell Brook's motives. Brook (right) shakes hands with British Superbike champion Shane Byrne at Donington Park on Tuesday. In an interview with iFL TV, Khan said: 'He's only faced one decent opponent in his whole career, so he has no room to talk. It seems to me that he's just calling me out for that one big payday. 'If he keeps going on like t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Amir Khan believes Kell Brook has called him out so that he can have the 'one big payday' which has so far eluded him during his 10-and-a-half year career. Brook, who defeated Jo Jo Dan to retain his IBF welterweight title last month, immediately challenged Khan in a post-fight interview but the Bolton-born boxer has instead agreed a bout on May 30, with his opponent yet to be confirmed. Khan has reiterated his intention to take on Brook within the next 12 months, although he insists that the undefeated 28-year-old only wants the fight because of the money it will earn him. Amir Khan (left), pictured during his last fight against Devon Alexander, has blasted Kell Brook's motives. Brook (right) shakes hands with British Superbike champion Shane Byrne at Donington Park on Tuesday. In an interview with iFL TV, Khan said: 'He's only faced one decent opponent in his whole career, so he has no room to talk. It seems to me that he's just calling me out for that one big payday. 'If he keeps going on like t...\nAmir Khan believes Kell Brook has been motivated by a desire for a \"one big payday\" rather rather than any genuine desire to face him in box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box box\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.07 ms /   531 runs   (    0.01 ms per token, 87522.66 tokens per second)\nllama_perf_context_print:        load time =    4549.64 ms\nllama_perf_context_print: prompt eval time =    1120.30 ms /   275 tokens (    4.07 ms per token,   245.47 tokens per second)\nllama_perf_context_print:        eval time =   21166.63 ms /   255 runs   (   83.01 ms per token,    12.05 tokens per second)\nllama_perf_context_print:       total time =   22335.86 ms /   530 tokens\nllama_perf_context_print:  mean exit layer =      21.12\nllama_perf_context_print:       exit ratio =       0.92  = 237 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 18, 32, 32, 20, 32, 25, 32, 29, 16, 32, 27, 22, 32, 16, 32, 27, 20, 32, 22, 32, 18, 18, 16, 17, 32, 20, 32, 32, 18, 27, 32, 32, 29, 22, 22, 22, 18, 32, 22, 22, 22, 24, 24, 24, 22, 25, 22, 23, 21, 21, 21, 21, 21, 21, 18, 32, 20, 20, 20, 22, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 20, 20, 20, 18, 20, 18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, }\n"
    },
    {
        "inference_time": "10097.05",
        "tokens": "341",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A Dallas architect has taken 14 shipping containers and transformed them into a stunningly modern 3,700-square-foot three-bedroom home. The home, dubbed 'PV14', also boasts a 1,400 sq ft roof deck that can fit up to 150 people, a 40ft long swimming pool on the ground floor and a 360 degree view of White Rock Lake and the sparkling lights of the city's downtown. Matt Mooney wanted to stay true to the materials that became the foundation for his home. Dallas architects Matt Mooney and Michael Gooden have transformed 14 shipping containers into a stunningly modern home. The 3,700 sq ft three-bedroom home, dubbed 'PV14,' boasts a 40ft long swimming pool on the ground floor that reflects the two-story glass-paneled tower, also built out of the containers. The home also features a 1,400 sq ft roof deck that can fit up to 150 people and has a 360 degree view of White Rock Lake and downtown Dallas. Mooney wanted to stay true to the materials that \u00a0built his home and thus all the ceilings were left exposed. -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A Dallas architect has taken 14 shipping containers and transformed them into a stunningly modern 3,700-square-foot three-bedroom home. The home, dubbed 'PV14', also boasts a 1,400 sq ft roof deck that can fit up to 150 people, a 40ft long swimming pool on the ground floor and a 360 degree view of White Rock Lake and the sparkling lights of the city's downtown. Matt Mooney wanted to stay true to the materials that became the foundation for his home. Dallas architects Matt Mooney and Michael Gooden have transformed 14 shipping containers into a stunningly modern home. The 3,700 sq ft three-bedroom home, dubbed 'PV14,' boasts a 40ft long swimming pool on the ground floor that reflects the two-story glass-paneled tower, also built out of the containers. The home also features a 1,400 sq ft roof deck that can fit up to 150 people and has a 360 degree view of White Rock Lake and downtown Dallas. Mooney wanted to stay true to the materials that \u00a0built his home and thus all the ceilings were left exposed. The containers were stacked stacked stacked to create a three- story structure with each level having its own unique features. Mooney wanted to create a home that would be a 'modern interpretation of what a containerized home could be'.\n\n\n\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.19 ms /   342 runs   (    0.00 ms per token, 288121.31 tokens per second)\nllama_perf_context_print:        load time =    4532.03 ms\nllama_perf_context_print: prompt eval time =    1107.84 ms /   288 tokens (    3.85 ms per token,   259.97 tokens per second)\nllama_perf_context_print:        eval time =    8977.35 ms /    53 runs   (  169.38 ms per token,     5.90 tokens per second)\nllama_perf_context_print:       total time =   10097.05 ms /   341 tokens\nllama_perf_context_print:  mean exit layer =      27.27\nllama_perf_context_print:       exit ratio =       0.58  = 32 / 55 \nllama_perf_context_print:       exit layer = {32, 32, 32, 27, 32, 16, 17, 32, 17, 17, 32, 32, 29, 30, 30, 28, 32, 28, 29, 32, 32, 27, 16, 18, 32, 30, 27, 18, 25, 17, 32, 29, 23, 22, 19, 32, 31, 31, 32, 32, 32, 25, 20, 27, 23, 18, 32, 20, 32, 32, 32, 32, 32, 32, 30, }\n"
    },
    {
        "inference_time": "16252.91",
        "tokens": "332",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Jordon Ibe is poised to sign a new five-year contract with Liverpool by the end of the week. The 19-year-old has agreed terms on a deal which will keep him at Anfield until 2020. Ibe spent the first half of this season on loan at Derby County but returned to Liverpool in January and has made a notable impact in his eight appearances. Jordon Ibe has impressed since returning to Liverpool this season and will sign a new contract this week. Ibe battles with Emre Can (right) for the ball as Liverpool train at Melwood last week. A knee injury disrupted his run in the side and he was cup-tied for Sunday\u2019s FA Cup semi-final defeat to Aston Villa, but he is expected to return against West Brom this weekend. Liverpool also expect to announce Jordan Henderson's new contract this week and are close to finalising an extension for Martin Skrtel. Talks are ongoing with young defender Jon Flanagan too whose contract expires in June. Liverpool yesterday agreed a new three-year shirt sponsorship deal with Standard  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Jordon Ibe is poised to sign a new five-year contract with Liverpool by the end of the week. The 19-year-old has agreed terms on a deal which will keep him at Anfield until 2020. Ibe spent the first half of this season on loan at Derby County but returned to Liverpool in January and has made a notable impact in his eight appearances. Jordon Ibe has impressed since returning to Liverpool this season and will sign a new contract this week. Ibe battles with Emre Can (right) for the ball as Liverpool train at Melwood last week. A knee injury disrupted his run in the side and he was cup-tied for Sunday\u2019s FA Cup semi-final defeat to Aston Villa, but he is expected to return against West Brom this weekend. Liverpool also expect to announce Jordan Henderson's new contract this week and are close to finalising an extension for Martin Skrtel. Talks are ongoing with young defender Jon Flanagan too whose contract expires in June. Liverpool yesterday agreed a new three-year shirt sponsorship deal with Standard  Chartered worth \u00a340m.\n\nMy questions:\n\n1. What is the duration of new contract that Jordon Ibe will signed?\n2. What is the current status of Ibe's contract negotiations with Liverpool?\n3rd. What is the reason why Ibe was cup-tied for Sunday\u2019s FA Cup semi-semi final defeat to Aston Villa?\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.87 ms /   333 runs   (    0.01 ms per token, 177789.64 tokens per second)\nllama_perf_context_print:        load time =    4558.23 ms\nllama_perf_context_print: prompt eval time =    1101.00 ms /   247 tokens (    4.46 ms per token,   224.34 tokens per second)\nllama_perf_context_print:        eval time =   15133.23 ms /    85 runs   (  178.04 ms per token,     5.62 tokens per second)\nllama_perf_context_print:       total time =   16252.91 ms /   332 tokens\nllama_perf_context_print:  mean exit layer =      27.71\nllama_perf_context_print:       exit ratio =       0.55  = 48 / 87 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 30, 32, 32, 32, 28, 32, 30, 26, 25, 27, 30, 18, 32, 32, 32, 22, 20, 32, 30, 32, 27, 27, 25, 27, 22, 32, 32, 23, 32, 30, 30, 30, 32, 30, 25, 32, 25, 32, 32, 22, 32, 32, 27, 32, 20, 18, 22, 18, 32, 17, 29, 29, 20, 15, 29, 25, 32, 32, 16, 30, 27, 17, 32, 17, 32, 32, 17, 32, 18, 32, 32, 25, 32, 27, 22, 32, 22, 32, 32, 27, }\n"
    },
    {
        "inference_time": "36164.10",
        "tokens": "515",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Comedian Rik Mayall failed to leave a valid will to deal with his \u00a31.2 million estate, which experts say could leave his family with a large inheritance tax bill. Mayall, who played scheming Tory MP Alan B\u2019Stard in ITV series The New Statesman, and a host of other comedy characters, died suddenly aged 56 last June. But probate records reveal he had no will \u2013 even though he suffered serious head injuries and nearly died in a quad-bike accident in 1998 \u2013 which could mean complications for his wife of 29 years, Barbara, and their three adult children. Scroll down for video. Legacy: Rik Mayall is pictured in 1999 with his wife Barbara and their children after his quad bike accident. His estate could be liable for tax of tens of thousands of pounds which would not have been payable if he had left everything to his wife. Lawyers have always advised that people should have a valid will to avoid tax pitfalls. Mayall, one of the leading \u2018alternative\u2019 comics of the 1980s, surprised doctors by making a good r -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Comedian Rik Mayall failed to leave a valid will to deal with his \u00a31.2 million estate, which experts say could leave his family with a large inheritance tax bill. Mayall, who played scheming Tory MP Alan B\u2019Stard in ITV series The New Statesman, and a host of other comedy characters, died suddenly aged 56 last June. But probate records reveal he had no will \u2013 even though he suffered serious head injuries and nearly died in a quad-bike accident in 1998 \u2013 which could mean complications for his wife of 29 years, Barbara, and their three adult children. Scroll down for video. Legacy: Rik Mayall is pictured in 1999 with his wife Barbara and their children after his quad bike accident. His estate could be liable for tax of tens of thousands of pounds which would not have been payable if he had left everything to his wife. Lawyers have always advised that people should have a valid will to avoid tax pitfalls. Mayall, one of the leading \u2018alternative\u2019 comics of the 1980s, surprised doctors by making a good rally after recovering from his quad-bike accident. Mayall's death was described as ' sudden and unexpected' by his family, who said he had been in health problems since the incident. Mayall's wife Barbara, who he married in 1987, could inherit the entire estate if there were no other benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef benef benef benef benef Benef benef benef benef benef\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.55 ms /   516 runs   (    0.01 ms per token, 93006.49 tokens per second)\nllama_perf_context_print:        load time =    4710.09 ms\nllama_perf_context_print: prompt eval time =    1117.43 ms /   260 tokens (    4.30 ms per token,   232.68 tokens per second)\nllama_perf_context_print:        eval time =   34997.39 ms /   255 runs   (  137.24 ms per token,     7.29 tokens per second)\nllama_perf_context_print:       total time =   36164.10 ms /   515 tokens\nllama_perf_context_print:  mean exit layer =      24.79\nllama_perf_context_print:       exit ratio =       0.85  = 218 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 20, 25, 18, 20, 32, 27, 16, 32, 27, 17, 32, 30, 25, 19, 32, 32, 32, 32, 21, 32, 17, 32, 29, 32, 20, 29, 30, 32, 25, 32, 27, 32, 29, 29, 17, 32, 19, 32, 17, 32, 29, 25, 16, 32, 17, 32, 32, 14, 29, 32, 27, 27, 29, 30, 29, 32, 28, 25, 22, 32, 18, 32, 27, 32, 18, 17, 32, 27, 19, 19, 25, 25, 25, 25, 25, 25, 20, 22, 27, 25, 24, 23, 27, 26, 27, 27, 29, 24, 22, 17, 32, 24, 26, 25, 25, 25, 25, 22, 20, 25, 25, 25, 23, 25, 25, 25, 22, 20, 32, 22, 25, 23, 25, 25, 25, 27, 27, 22, 18, 22, 22, 22, 25, 23, 25, 24, 32, 25, 26, 22, 32, 24, 25, 22, 32, 24, 25, 25, 25, 20, 18, 32, 22, 22, 22, 25, 25, 25, 27, 22, 18, 25, 22, 22, 22, 25, 25, 25, 27, 25, 18, 20, 22, 22, 25, 25, 25, 25, 27, 25, 18, 20, 22, 24, 24, 24, 25, 25, 32, 25, 18, 32, 22, 22, 22, 25, 25, 25, 25, 25, 20, 22, 22, 22, 25, 25, 25, 25, 25, 25, 20, 23, 25, 22, 22, 25, 25, 25, 25, 25, 20, 20, 22, 22, 22, 25, 25, 25, 25, 25, 17, 22, 22, 22, 22, 24, 24, 24, 32, 29, 20, 32, 22, 22, 22, 24, 24, 24, 27, 29, 18, 22, 25, 22, 22, 25, 24, 24, 25, 32, 20, 32, 22, 25, 25, }\n"
    },
    {
        "inference_time": "41031.16",
        "tokens": "523",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Sanaa, Yemen (CNN)As soon as the buses parked, the people scrambled onto the airport tarmac. They carried duffel bags, plastic bags full of clothes and small suitcases. They'd already abandoned larger bags in a mound. Only carry-on luggage was allowed on this Air India plane. The passengers were mostly Indian nationals, plus Yemenis and people from other countries who had been working in Yemen's capital, Sanaa. They sprinted or walked with deliberation to the airplanes. The stress of living in a war zone showed on the passengers' faces. Nobody wanted to be left behind. They climbed the steps to the airliner and were greeted by the Air India crew, all wearing pressed uniforms, the flight attendants fully made up. After takeoff, the flight attendants went up and down the aisle offering drinks to the 188 evacuees as if they were on a vacation flight. But some passengers were already fast asleep, exhausted by the push to get out of Yemen, site of some of the most chaotic fighting in the Mideast. This s -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Sanaa, Yemen (CNN)As soon as the buses parked, the people scrambled onto the airport tarmac. They carried duffel bags, plastic bags full of clothes and small suitcases. They'd already abandoned larger bags in a mound. Only carry-on luggage was allowed on this Air India plane. The passengers were mostly Indian nationals, plus Yemenis and people from other countries who had been working in Yemen's capital, Sanaa. They sprinted or walked with deliberation to the airplanes. The stress of living in a war zone showed on the passengers' faces. Nobody wanted to be left behind. They climbed the steps to the airliner and were greeted by the Air India crew, all wearing pressed uniforms, the flight attendants fully made up. After takeoff, the flight attendants went up and down the aisle offering drinks to the 188 evacuees as if they were on a vacation flight. But some passengers were already fast asleep, exhausted by the push to get out of Yemen, site of some of the most chaotic fighting in the Mideast. This saga began in late March, when the Saudi Arabia-led coalition began airstrikes against Houthi rebels who had control of Sanaa. The airport had been closed since then, and Yemenis had been trying to flee ever since. The evacuation flights were organized by the government of Yemen, with help from the United Arab Emirates and India. The evacuation was a race against time because the situation on ground was deteriorating. The airport was under attack, and the situation was getting more dangerous every day. The airliner flew over the desert, the passengers sleeping or sleep- deprived, as they flew to safety. The flight attendants served tea and sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn sn\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.86 ms /   524 runs   (    0.01 ms per token, 89358.80 tokens per second)\nllama_perf_context_print:        load time =    4581.70 ms\nllama_perf_context_print: prompt eval time =    1119.33 ms /   268 tokens (    4.18 ms per token,   239.43 tokens per second)\nllama_perf_context_print:        eval time =   39859.78 ms /   255 runs   (  156.31 ms per token,     6.40 tokens per second)\nllama_perf_context_print:       total time =   41031.16 ms /   523 tokens\nllama_perf_context_print:  mean exit layer =      26.12\nllama_perf_context_print:       exit ratio =       0.68  = 174 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 16, 32, 32, 32, 16, 32, 29, 32, 29, 32, 20, 31, 32, 32, 25, 18, 32, 32, 32, 32, 32, 29, 32, 16, 32, 29, 22, 20, 31, 31, 27, 16, 32, 29, 32, 32, 32, 30, 31, 29, 25, 32, 32, 16, 32, 27, 32, 32, 32, 32, 29, 31, 32, 27, 29, 31, 32, 27, 32, 16, 31, 20, 32, 31, 25, 27, 29, 32, 32, 31, 29, 31, 32, 32, 32, 30, 32, 28, 32, 25, 28, 18, 32, 32, 32, 32, 25, 23, 32, 32, 25, 18, 32, 30, 32, 32, 32, 32, 32, 22, 32, 32, 32, 32, 32, 25, 32, 22, 29, 32, 31, 32, 24, 32, 20, 31, 22, 32, 25, 32, 29, 32, 32, 22, 31, 32, 32, 32, 32, 17, 19, 32, 27, 32, 15, 29, 32, 22, 25, 31, 32, 32, 30, 32, 28, 27, 32, 32, 32, 27, 32, 18, 16, 18, 18, 22, 20, 17, 18, 22, 20, 20, 22, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 20, 22, 20, 22, 20, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, }\n"
    },
    {
        "inference_time": "43228.90",
        "tokens": "512",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: An Italian priest has been sacked amid claims that he took part in gay orgies with a Swiss Guard. The prelate was sacked from a religious order in Taranto after one of his online lovers contacted the church authorities with a dossier containing alleged evidence of sex with prostitutes, role play and explicit gay web chats. The priest allegedly favoured sexual role-play scenarios in which his lover played the disciple Judas Escariot, and had to atone for his betrayal of Jesus by becoming his slave. Controversy: News of\u00a0prelate being sacked after an online lover contacted Taranto church authorities with a dossier containing alleged evidence of sex with prostitutes is just the latest sex scandal to engulf the Vatican. The diocese confirmed that the priest had been dismissed but claimed there were no other prelates involved. Turn-on: The priest allegedly favoured sexual role-play in which his lover played Judas Iscariot, and had to atone for his betrayal of Jesus (pictured) A 32-year-old man, from Rovi -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: An Italian priest has been sacked amid claims that he took part in gay orgies with a Swiss Guard. The prelate was sacked from a religious order in Taranto after one of his online lovers contacted the church authorities with a dossier containing alleged evidence of sex with prostitutes, role play and explicit gay web chats. The priest allegedly favoured sexual role-play scenarios in which his lover played the disciple Judas Escariot, and had to atone for his betrayal of Jesus by becoming his slave. Controversy: News of\u00a0prelate being sacked after an online lover contacted Taranto church authorities with a dossier containing alleged evidence of sex with prostitutes is just the latest sex scandal to engulf the Vatican. The diocese confirmed that the priest had been dismissed but claimed there were no other prelates involved. Turn-on: The priest allegedly favoured sexual role-play in which his lover played Judas Iscariot, and had to atone for his betrayal of Jesus (pictured) A 32-year-old man, from Rovi, in Italy, contacted the church authorities with a dossier containing evidence of sex with prostitutes, explicit gay web chat logs and gay webcam videos chat logs. The priest allegedly met the man on a gay online chat site and exchanged messages with him for several months. The man claimed that he and the priest had sex in a hotel room in Rome and that he was paid 500 euros for his services. The priest allegedly asked the man to sign a contract agreeing to keep their relationship secret and to pay him 500 euros each time they meet sexually. The priest allegedly also gave the man a gold cross as a gift for his loyalty. The man also claim that he was asked to play the role role of Judas Iscariot in sexual role-play scenarios, where he would have to atone for his bet betrayal of Jesus by becoming his slave. The priest allegedly favored sexual role-play scenarios in which his lover played role role-play scenarios where his lover played role role-play scenarios where his lo lover play play role role role play scenarios where his lo lover play role role-play scenario where his lo lover play role-play scenario\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.81 ms /   513 runs   (    0.01 ms per token, 88341.66 tokens per second)\nllama_perf_context_print:        load time =    4536.92 ms\nllama_perf_context_print: prompt eval time =    1110.49 ms /   257 tokens (    4.32 ms per token,   231.43 tokens per second)\nllama_perf_context_print:        eval time =   42069.75 ms /   255 runs   (  164.98 ms per token,     6.06 tokens per second)\nllama_perf_context_print:       total time =   43228.90 ms /   512 tokens\nllama_perf_context_print:  mean exit layer =      26.74\nllama_perf_context_print:       exit ratio =       0.58  = 148 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 22, 32, 32, 16, 32, 22, 24, 30, 30, 32, 22, 22, 17, 17, 32, 24, 16, 32, 32, 32, 18, 32, 25, 22, 18, 32, 25, 32, 30, 32, 32, 27, 32, 30, 32, 32, 25, 18, 32, 32, 23, 32, 32, 21, 21, 17, 32, 32, 32, 32, 17, 32, 32, 32, 32, 15, 32, 32, 29, 32, 32, 29, 31, 27, 20, 32, 20, 32, 32, 17, 32, 29, 32, 32, 32, 18, 32, 32, 32, 32, 29, 30, 32, 32, 27, 28, 22, 27, 32, 27, 29, 17, 32, 32, 22, 22, 32, 32, 22, 32, 32, 22, 32, 32, 32, 18, 32, 32, 32, 32, 32, 29, 17, 19, 32, 20, 27, 32, 17, 22, 27, 32, 27, 30, 32, 25, 17, 32, 27, 32, 20, 27, 32, 32, 32, 29, 32, 30, 18, 32, 16, 32, 32, 22, 20, 22, 32, 18, 32, 32, 27, 32, 32, 16, 14, 32, 32, 27, 32, 25, 18, 22, 29, 20, 32, 16, 32, 22, 32, 29, 24, 24, 32, 20, 32, 32, 18, 32, 22, 22, 29, 32, 16, 32, 18, 32, 18, 32, 29, 31, 27, 22, 16, 32, 18, 32, 22, 16, 32, 20, 30, 32, 20, 32, 32, 29, 18, 16, 31, 32, 20, 18, 20, 32, 27, 29, 17, 17, 32, 22, 27, 18, 32, 24, 24, 26, 28, 27, 22, 22, 22, 27, 32, 18, 20, 24, 24, 26, 24, 29, 18, 32, 24, 24, 29, 20, 32, 24, 31, 22, 17, 32, 26, 25, }\n"
    },
    {
        "inference_time": "31710.81",
        "tokens": "505",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Yet another scientific study has found no link between autism and the vaccine against measles, mumps and rubella (MMR) - even among high risk children. The findings, in the Journal of the American Medical Association (JAMA), are based on a study of about 95,000 young people. All those in the study had older siblings. Some of the elder children had autism and some did not. Numerous studies over the last 15 years have ruled out a link between the MMR vaccine and autism. Since autism can run in families, researchers decided to look at whether vaccines could make autism even more likely in children who had siblings with autism. They found that vaccines had no effect on autism risk, whether or not a sibling in the family was diagnosed. 'Consistent with studies in other populations, we observed no association between MMR vaccination and increased ASD risk among privately insured children,' said the study, led by Anjali Jain, a doctor in Falls Church, Virginia. 'We also found no evidence that receipt of e -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Yet another scientific study has found no link between autism and the vaccine against measles, mumps and rubella (MMR) - even among high risk children. The findings, in the Journal of the American Medical Association (JAMA), are based on a study of about 95,000 young people. All those in the study had older siblings. Some of the elder children had autism and some did not. Numerous studies over the last 15 years have ruled out a link between the MMR vaccine and autism. Since autism can run in families, researchers decided to look at whether vaccines could make autism even more likely in children who had siblings with autism. They found that vaccines had no effect on autism risk, whether or not a sibling in the family was diagnosed. 'Consistent with studies in other populations, we observed no association between MMR vaccination and increased ASD risk among privately insured children,' said the study, led by Anjali Jain, a doctor in Falls Church, Virginia. 'We also found no evidence that receipt of e...\nSource: Health News - Medical News Today - http://www.medicalnewstoday.com/articles/317204#_\nSummary: A study published in JAMA found no link between the MMR vaccine and autism, even among high risk children who have older siblings who are aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut aut\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.04 ms /   506 runs   (    0.01 ms per token, 83844.24 tokens per second)\nllama_perf_context_print:        load time =    4589.90 ms\nllama_perf_context_print: prompt eval time =    1100.94 ms /   250 tokens (    4.40 ms per token,   227.08 tokens per second)\nllama_perf_context_print:        eval time =   30560.60 ms /   255 runs   (  119.85 ms per token,     8.34 tokens per second)\nllama_perf_context_print:       total time =   31710.81 ms /   505 tokens\nllama_perf_context_print:  mean exit layer =      23.62\nllama_perf_context_print:       exit ratio =       0.86  = 221 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 29, 32, 29, 32, 32, 16, 32, 32, 32, 32, 32, 32, 32, 30, 16, 25, 32, 29, 32, 30, 32, 30, 29, 30, 32, 32, 32, 32, 28, 32, 17, 25, 28, 29, 28, 22, 32, 17, 32, 19, 32, 29, 25, 19, 32, 25, 21, 32, 18, 32, 18, 19, 21, 18, 17, 32, 20, 32, 22, 16, 25, 32, 22, 22, 22, 22, 25, 24, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, }\n"
    },
    {
        "inference_time": "20658.48",
        "tokens": "500",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A 12-year-old girl was tied up, beaten and raped for by seven ISIS different fighters when she was kidnapped from her home in the Iraq region of Yazidi, a shocking new report has revealed. The suffering of Jalila - whose name has been changed - is just one of a number of harrowing accounts given by women who have escaped captured by the brutal Islamist organisation which is taking over swathes of territory in Syria and Iraq. The study undertaken by Human Rights Watch\u00a0found fighters were kidnapping women and children as young as eight, forcing them to marry and convert to Islam and raping them repeatedly \u2013 sometimes picking their victims\u2019 names out of a hand in a sick lottery. Scroll down for video. Yazidi women and girls have been separated from their families, forced to convert to Islam and repeatedly raped by ISIS fighters, the study from Human Rights Watch says. Their actions amount to war crimes. it adds. Yazidi children as young as eight have been abducted from their homes in northern Iraq, ra -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A 12-year-old girl was tied up, beaten and raped for by seven ISIS different fighters when she was kidnapped from her home in the Iraq region of Yazidi, a shocking new report has revealed. The suffering of Jalila - whose name has been changed - is just one of a number of harrowing accounts given by women who have escaped captured by the brutal Islamist organisation which is taking over swathes of territory in Syria and Iraq. The study undertaken by Human Rights Watch\u00a0found fighters were kidnapping women and children as young as eight, forcing them to marry and convert to Islam and raping them repeatedly \u2013 sometimes picking their victims\u2019 names out of a hand in a sick lottery. Scroll down for video. Yazidi women and girls have been separated from their families, forced to convert to Islam and repeatedly raped by ISIS fighters, the study from Human Rights Watch says. Their actions amount to war crimes. it adds. Yazidi children as young as eight have been abducted from their homes in northern Iraq, raped and forced to convert to fit in with ISIS's brutal ideology.\n\nThe Yazidi community has been targeted by ISIS, who have been kidnapping and raping women and children as young as eight. The women are forced to convert to Islam and are subjected to repeated ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra ra\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.66 ms /   501 runs   (    0.01 ms per token, 88531.54 tokens per second)\nllama_perf_context_print:        load time =    4562.07 ms\nllama_perf_context_print: prompt eval time =    1103.35 ms /   245 tokens (    4.50 ms per token,   222.05 tokens per second)\nllama_perf_context_print:        eval time =   19506.45 ms /   255 runs   (   76.50 ms per token,    13.07 tokens per second)\nllama_perf_context_print:       total time =   20658.48 ms /   500 tokens\nllama_perf_context_print:  mean exit layer =      20.72\nllama_perf_context_print:       exit ratio =       0.83  = 214 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 25, 32, 32, 22, 27, 17, 32, 32, 27, 17, 20, 15, 32, 27, 22, 32, 20, 32, 32, 32, 29, 32, 22, 27, 27, 16, 32, 14, 14, 32, 17, 32, 18, 32, 17, 32, 29, 25, 31, 32, 27, 22, 20, 32, 29, 27, 22, 32, 32, 31, 32, 30, 22, 17, 32, 22, 22, 29, 22, 29, 32, 16, 32, 22, 22, 22, 22, 22, 22, 22, 22, 22, 20, 20, 20, 20, 20, 20, 17, 20, 18, 32, 17, 32, 17, 32, 19, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 22, 17, 32, 17, 32, 17, 32, 17, 32, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, }\n"
    },
    {
        "inference_time": "36970.06",
        "tokens": "496",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: This is the man who allegedly opened fire in the campus library of a North Carolina college on Monday morning, leaving one person dead. Kenneth Stancil, 20, is a former student who did not graduate from Wayne Community College in Goldsboro, where he is accused of shooting dead a man who working in the school's print shop. According to police, Stancil walked into the building at 8am carrying a long-barreled\u00a0gun, went up to the third floor, and fired one shot, which killed long-time employee Ron Lane. It is not known whether Stancil and Lane are connected or related. Scroll Down for Video. Manhunt: Police believe the shooter was Kenneth Stancil, a former student who did not graduate from Wayne Community College in Goldsboro, North Carolina. He allegedly entered the building at 8am and fired one shot. Killed: The single bullet fired on Monday morning killed Rodney Lane (pictured left and right) who was a long-time employee at the Wayne Community College print shop. It is not known if he was connected  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: This is the man who allegedly opened fire in the campus library of a North Carolina college on Monday morning, leaving one person dead. Kenneth Stancil, 20, is a former student who did not graduate from Wayne Community College in Goldsboro, where he is accused of shooting dead a man who working in the school's print shop. According to police, Stancil walked into the building at 8am carrying a long-barreled\u00a0gun, went up to the third floor, and fired one shot, which killed long-time employee Ron Lane. It is not known whether Stancil and Lane are connected or related. Scroll Down for Video. Manhunt: Police believe the shooter was Kenneth Stancil, a former student who did not graduate from Wayne Community College in Goldsboro, North Carolina. He allegedly entered the building at 8am and fired one shot. Killed: The single bullet fired on Monday morning killed Rodney Lane (pictured left and right) who was a long-time employee at the Wayne Community College print shop. It is not known if he was connected  to Stancil or the shooters motive.\n\nKenneth Stancil is the man who allegedly opened fire in the campus library of a North Carolina college on Monday morning, leaving one person dead.\n\n Stancil is a former student who did not graduate from Wayne Community College in Goldsboro where he is accused of shooting dead a man whom whom he had worked with for years. according to police, Stancil walked into the building at 8am carrying a long-barreled gun, went up to the third floor, and fired one shot which killed long-time employee Ron Lane.\n\nIt is not known whether Stancil and Lane are connected or related. Police believe they have identified the shooter, but they are still investigating the mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot mot\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.67 ms /   497 runs   (    0.01 ms per token, 87623.41 tokens per second)\nllama_perf_context_print:        load time =    4552.29 ms\nllama_perf_context_print: prompt eval time =    1105.44 ms /   241 tokens (    4.59 ms per token,   218.01 tokens per second)\nllama_perf_context_print:        eval time =   35814.73 ms /   255 runs   (  140.45 ms per token,     7.12 tokens per second)\nllama_perf_context_print:       total time =   36970.06 ms /   496 tokens\nllama_perf_context_print:  mean exit layer =      25.01\nllama_perf_context_print:       exit ratio =       0.74  = 190 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 29, 25, 25, 18, 32, 25, 18, 32, 32, 30, 32, 32, 32, 32, 32, 22, 25, 25, 20, 29, 31, 20, 32, 16, 32, 22, 18, 27, 22, 20, 18, 17, 32, 18, 32, 29, 32, 16, 31, 32, 17, 22, 19, 32, 32, 29, 30, 22, 25, 20, 32, 17, 17, 32, 22, 17, 17, 16, 18, 32, 32, 20, 32, 22, 22, 22, 27, 18, 20, 20, 22, 18, 32, 32, 16, 20, 16, 20, 32, 32, 32, 32, 32, 32, 30, 28, 22, 17, 32, 20, 32, 27, 32, 18, 20, 20, 32, 32, 18, 32, 32, 16, 25, 16, 32, 32, 29, 32, 17, 32, 22, 30, 18, 32, 17, 24, 17, 32, 17, 32, 19, 32, 32, 16, 32, 18, 32, 32, 32, 32, 32, 32, 18, 16, 32, 14, 22, 25, 25, 32, 23, 18, 32, 16, 32, 32, 32, 32, 20, 32, 32, 29, 25, 17, 32, 16, 32, 27, 18, 32, 25, 32, 20, 20, 20, 20, 22, 22, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, 24, 25, }\n"
    },
    {
        "inference_time": "20380.25",
        "tokens": "380",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss, because of 'negative publicity' from the grooming scandal. The council will now offer up to \u00a3200,000 to candidates, rather than the \u00a3160,000 enjoyed by former chief boss Martin Kimber. He left in December after a council report said 1,400 children had suffered horrific sexual abuse over a 16-year period. The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss. The rest of the cabinet resigned in February, after a Government report said the council was 'not fit for purpose' and 'in denial' about exploitation, mainly of white girls by men of Pakistani origin. HR chief Phil Howe said: 'Due to recent negative publicity, it may be necessary to consider additional incentives.' Councillor Caven Vines, leader of the Ukip group, said he was uncomfortable with the idea. -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss, because of 'negative publicity' from the grooming scandal. The council will now offer up to \u00a3200,000 to candidates, rather than the \u00a3160,000 enjoyed by former chief boss Martin Kimber. He left in December after a council report said 1,400 children had suffered horrific sexual abuse over a 16-year period. The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss. The rest of the cabinet resigned in February, after a Government report said the council was 'not fit for purpose' and 'in denial' about exploitation, mainly of white girls by men of Pakistani origin. HR chief Phil Howe said: 'Due to recent negative publicity, it may be necessary to consider additional incentives.' Councillor Caven Vines, leader of the Ukip group, said he was uncomfortable with the idea. 'I don't think it's right to compensate people for the negative publicity that they've been subjected to due to their job,' he said.\n\nMy questions:\n\n1. What is the reason why the new chief executive of Rotherham council could be paid up to \u00a340,000 more than the previous boss?\n2. What is the reason why the rest of the cabinet resigned in February?\n3. What is the meaning of \"negative publicity\" according to HR chief Phil Howe?\n4. What do you think about this situation situation?\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.09 ms /   381 runs   (    0.01 ms per token, 123261.08 tokens per second)\nllama_perf_context_print:        load time =    4551.21 ms\nllama_perf_context_print: prompt eval time =    1157.01 ms /   250 tokens (    4.63 ms per token,   216.07 tokens per second)\nllama_perf_context_print:        eval time =   19196.89 ms /   130 runs   (  147.67 ms per token,     6.77 tokens per second)\nllama_perf_context_print:       total time =   20380.25 ms /   380 tokens\nllama_perf_context_print:  mean exit layer =      25.67\nllama_perf_context_print:       exit ratio =       0.66  = 87 / 132 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 20, 32, 27, 32, 27, 32, 20, 32, 32, 29, 32, 25, 32, 16, 32, 18, 29, 27, 22, 32, 32, 25, 22, 32, 18, 22, 32, 32, 17, 20, 32, 32, 29, 32, 29, 27, 29, 27, 30, 18, 32, 27, 20, 16, 22, 16, 32, 22, 18, 32, 20, 18, 32, 32, 17, 32, 20, 27, 22, 20, 22, 32, 25, 32, 20, 32, 16, 25, 17, 32, 23, 21, 30, 21, 29, 16, 32, 29, 17, 32, 29, 22, 17, 20, 27, 22, 27, 27, 18, 24, 18, 22, 29, 18, 32, 31, 32, 25, 20, 32, 22, 16, 27, 27, 24, 32, 25, 20, 32, 32, 32, 20, 32, 22, 29, 18, 18, 32, 20, 17, 17, 19, 16, 32, 32, 27, }\n"
    },
    {
        "inference_time": "17020.38",
        "tokens": "523",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose last weekend - and Sportsmail can exclusively reveal what the Spanish midfielder's shield will look like. Fabregas suffered the break during Chelsea's win over Stoke City on Saturday, taking a hit to the face from Charlie Adam's forearm minutes before the Scot struck a 66-yard wondergoal at Stamford Bridge. The Spaniard has since travelled to Milan to see a specialist, who produced masks for Fernando Torres, Petr Cech and Demba Ba previously, in the hope of returning to action as soon as possible. Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose, and this is it. The mask, branded with Fabregas's initial and squad number 'C4', was made by the Ortholabsport. Fabregas wrote this note to the orthopedic centre and its craftsmen\u00a0Stefano Duchini and Lucilla Pezzoni. The Spaniard travelled to Milan on Tuesday with Chelsea medical director Paco Biosca to have the mask fitted.  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose last weekend - and Sportsmail can exclusively reveal what the Spanish midfielder's shield will look like. Fabregas suffered the break during Chelsea's win over Stoke City on Saturday, taking a hit to the face from Charlie Adam's forearm minutes before the Scot struck a 66-yard wondergoal at Stamford Bridge. The Spaniard has since travelled to Milan to see a specialist, who produced masks for Fernando Torres, Petr Cech and Demba Ba previously, in the hope of returning to action as soon as possible. Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose, and this is it. The mask, branded with Fabregas's initial and squad number 'C4', was made by the Ortholabsport. Fabregas wrote this note to the orthopedic centre and its craftsmen\u00a0Stefano Duchini and Lucilla Pezzoni. The Spaniard travelled to Milan on Tuesday with Chelsea medical director Paco Biosca to have the mask fitted.  Fabregas's face will be covered by a protective shield, which will be custom- made to fit his face perfectly. The mask will be made of lightweight material and will be designed to protect Fabregas's nose and other fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.49 ms /   524 runs   (    0.01 ms per token, 95446.27 tokens per second)\nllama_perf_context_print:        load time =    4530.54 ms\nllama_perf_context_print: prompt eval time =    1114.11 ms /   268 tokens (    4.16 ms per token,   240.55 tokens per second)\nllama_perf_context_print:        eval time =   15860.05 ms /   255 runs   (   62.20 ms per token,    16.08 tokens per second)\nllama_perf_context_print:       total time =   17020.38 ms /   523 tokens\nllama_perf_context_print:  mean exit layer =      19.69\nllama_perf_context_print:       exit ratio =       0.91  = 234 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 25, 22, 16, 32, 22, 32, 18, 32, 27, 29, 22, 16, 32, 32, 30, 28, 18, 32, 32, 28, 26, 32, 32, 22, 22, 32, 32, 32, 24, 18, 32, 32, 27, 32, 27, 32, 18, 27, 32, 27, 17, 32, 25, 27, 16, 14, 32, 32, 16, 29, 18, 18, 17, 32, 22, 24, 22, 22, 22, 22, 18, 20, 17, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 17, 18, 17, 17, 17, 17, 17, 17, 18, 18, 19, 18, 17, 19, 17, 17, 18, 18, 18, 18, 17, 17, 18, 17, 18, 17, 18, 18, 18, 17, 18, 17, 18, 17, 18, 19, 17, 18, 17, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 20, 18, 17, 17, 17, 20, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, }\n"
    },
    {
        "inference_time": "31730.28",
        "tokens": "432",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Andros Townsend enjoyed silencing the critics with his wonder strike for England, saying naysayers like Paul Merson provided the perfect motivation for him in Italy. This has been a topsy-turvy season for the 23-year-old, who has yet to reach the heights he scaled when he first burst onto the international scene. Three Lions manager Roy Hodgson has, however, kept faith with the Tottenham winger - belief he paid back in quite exceptional fashion at the Juventus Stadium. Andros Townsend scores England's equaliser in their 1-1 friendly draw with Italy in Turin on Tuesday night. Townsend celebrates his strike with Tottenham Hotspur team-mates Ryan Mason (left) and Kyle Walker. -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Andros Townsend enjoyed silencing the critics with his wonder strike for England, saying naysayers like Paul Merson provided the perfect motivation for him in Italy. This has been a topsy-turvy season for the 23-year-old, who has yet to reach the heights he scaled when he first burst onto the international scene. Three Lions manager Roy Hodgson has, however, kept faith with the Tottenham winger - belief he paid back in quite exceptional fashion at the Juventus Stadium. Andros Townsend scores England's equaliser in their 1-1 friendly draw with Italy in Turin on Tuesday night. Townsend celebrates his strike with Tottenham Hotspur team-mates Ryan Mason (left) and Kyle Walker. \"I've been under a lot pressure this season,\" he said. \"People like Paul Merson have been saying I've not been playing enough, so it's good to prove them wrong.\"\n\nAndros Townsend enjoyed sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil Sil sil sil sil sil sil sil sil Sil sil sil sil sil sil sil Sil sil sil sil sil sil Sil sil sil sil sil Sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil Sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil sil\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.83 ms /   433 runs   (    0.01 ms per token, 74296.50 tokens per second)\nllama_perf_context_print:        load time =    4545.35 ms\nllama_perf_context_print: prompt eval time =    1077.68 ms /   177 tokens (    6.09 ms per token,   164.24 tokens per second)\nllama_perf_context_print:        eval time =   30604.03 ms /   255 runs   (  120.02 ms per token,     8.33 tokens per second)\nllama_perf_context_print:       total time =   31730.28 ms /   432 tokens\nllama_perf_context_print:  mean exit layer =      23.65\nllama_perf_context_print:       exit ratio =       0.92  = 237 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 29, 27, 25, 30, 32, 27, 17, 32, 25, 27, 32, 18, 29, 22, 32, 18, 32, 22, 25, 20, 29, 17, 32, 32, 31, 30, 31, 32, 32, 20, 31, 29, 32, 29, 29, 31, 25, 18, 32, 32, 29, 32, 32, 32, 22, 32, 18, 32, 22, 25, 24, 22, 22, 22, 23, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 29, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 21, 21, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 22, 25, 20, 22, 22, 22, 22, 22, 24, 24, 24, 24, 24, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, }\n"
    },
    {
        "inference_time": "6547.31",
        "tokens": "309",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A husband accused of trying to murder his cheating wife wrote her an extraordinary letter while remanded in prison asking her to send him a suit for his trial, a court heard yesterday. Joseph O'Riordan, 73, stabbed his 47-year-old wife Amanda eight times after he discovered she had been having an affair with a postman. She was left with life-threatening injuries to her torso, chest, arms and back. Joseph O'Riordan stabbed his wife of ten years Amanda (left) with a seven inch kitchen knife eight times. O'Riordan, who was a councillor at the time, rang an ambulance after the attack in October last year and admitted what he had done during the 999 call. Yesterday it emerged that four months later, while on remand, O'Riordan wrote to his wife from jail asking her to 'get my things together' before for his trial. Meanwhile Mrs O'Riordan, during questioning by her husband's defence team in court yesterday, sobbed: 'I did not want to hurt Joe. I never wanted to hurt him\u2026 even now. All of this is my fault  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A husband accused of trying to murder his cheating wife wrote her an extraordinary letter while remanded in prison asking her to send him a suit for his trial, a court heard yesterday. Joseph O'Riordan, 73, stabbed his 47-year-old wife Amanda eight times after he discovered she had been having an affair with a postman. She was left with life-threatening injuries to her torso, chest, arms and back. Joseph O'Riordan stabbed his wife of ten years Amanda (left) with a seven inch kitchen knife eight times. O'Riordan, who was a councillor at the time, rang an ambulance after the attack in October last year and admitted what he had done during the 999 call. Yesterday it emerged that four months later, while on remand, O'Riordan wrote to his wife from jail asking her to 'get my things together' before for his trial. Meanwhile Mrs O'Riordan, during questioning by her husband's defence team in court yesterday, sobbed: 'I did not want to hurt Joe. I never wanted to hurt him\u2026 even now. All of this is my fault 100 per cent.'\n\nQuestion: What did Joseph O'Riordan ask his wife to do in his letter from prison? [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.74 ms /   310 runs   (    0.00 ms per token, 421195.65 tokens per second)\nllama_perf_context_print:        load time =    4865.03 ms\nllama_perf_context_print: prompt eval time =    1119.01 ms /   279 tokens (    4.01 ms per token,   249.33 tokens per second)\nllama_perf_context_print:        eval time =    5420.41 ms /    30 runs   (  180.68 ms per token,     5.53 tokens per second)\nllama_perf_context_print:       total time =    6547.31 ms /   309 tokens\nllama_perf_context_print:  mean exit layer =      28.16\nllama_perf_context_print:       exit ratio =       0.53  = 17 / 32 \nllama_perf_context_print:       exit layer = {32, 32, 29, 29, 32, 32, 29, 32, 31, 32, 32, 18, 32, 32, 25, 17, 32, 29, 22, 29, 32, 22, 17, 32, 31, 32, 30, 19, 32, 16, 32, 28, }\n"
    },
    {
        "inference_time": "32258.88",
        "tokens": "503",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The UK is to open its first three proton beam therapy centres after cancer-stricken five-year-old Ashya King made a miracle recovery after receiving the treatment. Cardiff-based Proton Partners International Ltd, is to open the treatment centres, which will be available to NHS cancer patients, in Cardiff, London and Northumberland by 2017, with the first in the Welsh capital being operational by next year. The announcement comes just weeks after the parents of brain cancer sufferer Ashya told of how he was given the 'all-clear' after receiving proton bean therapy at a centre in Prague. Scroll down for video. Five-year-old Ashya King, pictured in October, who suffered from brain cancer but has recently been given the all-clear after receiving proton beam therapy. Three centres in the UK in Cardiff, London and Northumberland, are set to open offering proton beam therapy, which Ashya, pictured, received. Both Brett and Naghmeh King sparked an international manhunt last year after taking their son out  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The UK is to open its first three proton beam therapy centres after cancer-stricken five-year-old Ashya King made a miracle recovery after receiving the treatment. Cardiff-based Proton Partners International Ltd, is to open the treatment centres, which will be available to NHS cancer patients, in Cardiff, London and Northumberland by 2017, with the first in the Welsh capital being operational by next year. The announcement comes just weeks after the parents of brain cancer sufferer Ashya told of how he was given the 'all-clear' after receiving proton bean therapy at a centre in Prague. Scroll down for video. Five-year-old Ashya King, pictured in October, who suffered from brain cancer but has recently been given the all-clear after receiving proton beam therapy. Three centres in the UK in Cardiff, London and Northumberland, are set to open offering proton beam therapy, which Ashya, pictured, received. Both Brett and Naghmeh King sparked an international manhunt last year after taking their son out 200 miles to Prague for treatment. Ashya's treatment at the Proton Therapy Centre in Prague, which is not yet NHS-funded treatment, was funded by a public fundra campaign funded by a fundraising campaign. Ashya's parents, who are both doctors, had been told by doctors in the UK that their son's cancer was too advanced to treat with proton beam therapy, but they were determined to find a way to save him. Ashya's recovery has been ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.59 ms /   504 runs   (    0.01 ms per token, 90112.64 tokens per second)\nllama_perf_context_print:        load time =    4671.98 ms\nllama_perf_context_print: prompt eval time =    1078.46 ms /   248 tokens (    4.35 ms per token,   229.96 tokens per second)\nllama_perf_context_print:        eval time =   31132.08 ms /   255 runs   (  122.09 ms per token,     8.19 tokens per second)\nllama_perf_context_print:       total time =   32258.88 ms /   503 tokens\nllama_perf_context_print:  mean exit layer =      23.84\nllama_perf_context_print:       exit ratio =       0.81  = 208 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 29, 29, 32, 31, 32, 18, 29, 18, 32, 27, 22, 17, 32, 32, 32, 32, 29, 22, 32, 16, 32, 29, 22, 18, 32, 18, 32, 32, 32, 29, 32, 31, 32, 18, 25, 30, 32, 32, 27, 16, 32, 32, 18, 32, 18, 22, 32, 16, 32, 20, 32, 32, 17, 32, 27, 22, 15, 30, 32, 32, 22, 30, 32, 28, 20, 30, 32, 32, 32, 32, 32, 20, 29, 31, 17, 32, 30, 16, 30, 32, 32, 29, 20, 25, 27, 23, 32, 32, 23, 32, 20, 32, 29, 17, 32, 29, 29, 16, 32, 30, 32, 27, 32, 22, 22, 27, 22, 17, 29, 32, 27, 29, 32, 22, 22, 22, 30, 25, 22, 22, 25, 23, 22, 23, 24, 23, 23, 27, 27, 23, 25, 25, 23, 25, 23, 25, 25, 23, 21, 23, 23, 23, 23, 25, 25, 23, 23, 23, 23, 23, 23, 20, 25, 20, 32, 20, 22, 20, 22, 20, 20, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 17, 17, 20, 20, 20, 20, 17, 20, 20, 17, 20, 17, 20, 20, 20, 20, 20, 20, 20, 17, 20, 20, 20, 20, 20, 17, 20, 20, 20, 20, 17, 20, 18, 25, 22, 20, 25, 22, 20, 27, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 17, 17, 20, 20, }\n"
    },
    {
        "inference_time": "16044.18",
        "tokens": "350",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments because it is far cheaper than training them in Britain. Managers from the London Ambulance Service, the largest in the NHS, have just filled 225 vacant posts with applicants from Sydney and Melbourne. They were recruited on the back of a series of practical demonstrations on dummies, a written exam and an interview that lasted in all between four and six hours. Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments. The selection process, held at five-star hotels, was so quick that by 8am the next day applicants had received an email informing them if they had a job. It is the latest example of how the NHS is recruiting overseas because it has failed to train enough staff in Britain to meet the needs of the rising and ageing population. Hospital managers have been flying out to Spain and Portugal t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments because it is far cheaper than training them in Britain. Managers from the London Ambulance Service, the largest in the NHS, have just filled 225 vacant posts with applicants from Sydney and Melbourne. They were recruited on the back of a series of practical demonstrations on dummies, a written exam and an interview that lasted in all between four and six hours. Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments. The selection process, held at five-star hotels, was so quick that by 8am the next day applicants had received an email informing them if they had a job. It is the latest example of how the NHS is recruiting overseas because it has failed to train enough staff in Britain to meet the needs of the rising and ageing population. Hospital managers have been flying out to Spain and Portugal t...\nSource: The Telegraph\nSource Date: 2018-02-26\nSource URL:https://www.telegraph.com/news/2018/02/26th/ambulance-bosses-make-21500-mile round trip-australia-hire-paramedics/ [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.80 ms /   351 runs   (    0.01 ms per token, 195108.39 tokens per second)\nllama_perf_context_print:        load time =    4496.55 ms\nllama_perf_context_print: prompt eval time =    1092.92 ms /   271 tokens (    4.03 ms per token,   247.96 tokens per second)\nllama_perf_context_print:        eval time =   14934.42 ms /    79 runs   (  189.04 ms per token,     5.29 tokens per second)\nllama_perf_context_print:       total time =   16044.18 ms /   350 tokens\nllama_perf_context_print:  mean exit layer =      28.09\nllama_perf_context_print:       exit ratio =       0.53  = 43 / 81 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 29, 32, 27, 31, 27, 30, 29, 32, 32, 29, 32, 27, 31, 30, 30, 29, 18, 32, 18, 32, 17, 32, 32, 25, 32, 15, 32, 32, 30, 32, 30, 32, 20, 32, 25, 32, 22, 22, 20, 18, 32, 32, 27, 20, 18, 32, 22, 16, 32, 32, 27, 27, 25, 32, 17, 17, 32, 30, 27, 15, 32, 32, 32, 32, 29, 32, 30, 31, 32, 32, 30, 22, }\n"
    },
    {
        "inference_time": "30220.10",
        "tokens": "434",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Chelsea have turned to the latest fitness technology to carry them over the line towards the Premier League title following a relentless campaign. Having led the chasing pack for most of the season, the Blues are currently  seven points clear with a game in hand over their closest rivals and seven games remaining ahead of the weekend's visit of Manchester United. Despite their commanding position, Jose Mourinho's side is leaving nothing to chance as players hit the gym following the announcement of a partnership with leading wellness solutions provider Technogym - who have kitted out their Cobham training ground with state-of-the-art equipment. Kurt Zouma (left), Nemanja Matic (top right) and Eden Hazard (bottom right) work out at Cobham. Players including Eden Hazard and Nemanja Matic have been pictured being put through their paces while the Stamford Bridge club's Twitter account features a video of young defender Kurt Zouma working out at Cobham. Carlos Lalin, Chelsea assistant first team fitnes -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Chelsea have turned to the latest fitness technology to carry them over the line towards the Premier League title following a relentless campaign. Having led the chasing pack for most of the season, the Blues are currently  seven points clear with a game in hand over their closest rivals and seven games remaining ahead of the weekend's visit of Manchester United. Despite their commanding position, Jose Mourinho's side is leaving nothing to chance as players hit the gym following the announcement of a partnership with leading wellness solutions provider Technogym - who have kitted out their Cobham training ground with state-of-the-art equipment. Kurt Zouma (left), Nemanja Matic (top right) and Eden Hazard (bottom right) work out at Cobham. Players including Eden Hazard and Nemanja Matic have been pictured being put through their paces while the Stamford Bridge club's Twitter account features a video of young defender Kurt Zouma working out at Cobham. Carlos Lalin, Chelsea assistant first team fitnes coach, said: \"We are delighted to be working with Technogym to help our team prepare for the final stretch of the season. We are always looking ways ways to improve our fitness and fitness levels so we are excited about this new partnership.\"\n\nAnalysis: Chelsea have been using the latest fitness technology to help them maintain their position at top of the Premier League table. They have partnered with Technogym, which has kitted out their training ground with state-of- the-art equipment. This has allowed players such as Eden Hazard and Nemanja Matic to workout and improve their fitness levels. Chelsea's fitness coaches are also using this technology to monitor their players' fitness levels and make adjustments to their training regime accordingly. This partnership is seen as a way to gain an extra edge over their rivals as the team looks to secure their Premier League title. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.35 ms /   435 runs   (    0.01 ms per token, 99977.02 tokens per second)\nllama_perf_context_print:        load time =    4548.24 ms\nllama_perf_context_print: prompt eval time =    1095.71 ms /   237 tokens (    4.62 ms per token,   216.30 tokens per second)\nllama_perf_context_print:        eval time =   29085.66 ms /   197 runs   (  147.64 ms per token,     6.77 tokens per second)\nllama_perf_context_print:       total time =   30220.10 ms /   434 tokens\nllama_perf_context_print:  mean exit layer =      25.64\nllama_perf_context_print:       exit ratio =       0.66  = 131 / 199 \nllama_perf_context_print:       exit layer = {32, 32, 27, 20, 20, 29, 29, 27, 32, 16, 27, 27, 27, 27, 32, 27, 25, 32, 32, 17, 17, 32, 29, 31, 17, 32, 22, 29, 16, 32, 18, 32, 22, 22, 24, 18, 22, 29, 18, 24, 16, 31, 18, 32, 29, 25, 27, 32, 32, 27, 29, 20, 25, 16, 32, 32, 32, 32, 18, 17, 32, 22, 27, 32, 16, 32, 32, 18, 32, 16, 20, 27, 32, 29, 32, 20, 17, 32, 20, 32, 30, 32, 17, 32, 29, 18, 18, 32, 25, 16, 32, 27, 22, 32, 14, 32, 22, 32, 20, 17, 32, 18, 32, 25, 15, 32, 32, 29, 18, 31, 17, 29, 32, 32, 32, 25, 17, 32, 23, 22, 23, 32, 17, 32, 22, 22, 32, 20, 32, 22, 27, 32, 32, 16, 18, 14, 25, 30, 27, 20, 32, 16, 32, 29, 14, 25, 12, 32, 12, 22, 20, 30, 22, 32, 30, 20, 30, 17, 32, 22, 29, 32, 32, 22, 30, 18, 22, 22, 18, 32, 18, 32, 16, 32, 32, 14, 32, 29, 27, 32, 32, 18, 20, 32, 27, 22, 32, 27, 32, 32, 32, 25, 32, 29, 27, 17, 32, 30, 19, }\n"
    },
    {
        "inference_time": "23713.34",
        "tokens": "398",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Conversation is limited to \u2018mummy\u2019 and \u2018bye bye\u2019 and simply standing up unaided is a major achievement. But 11-month-old babies have already mastered the fundamentals of physics. Baby boys and girls were surprised when a ball seemed to disappear through a solid wall. Their curiosity did not end there. When they were then given the ball to play with, they repeatedly banged it on a table in an apparent attempt to work out its secret. Scroll down for video. Babies in the study reacted with surprise when objects like this toy car appeared to break the laws of gravity. The babies were similarly perplexed by a toy car that appeared to defy the laws of gravity and float in thin air. And given it to play with later, they repeatedly dropped it on the floor. Importantly, the babies shown the magic tricks were more interested in playing with the ball than a brand new toy that they had never seen before. Babies might be smarter than we had previously thought and can understand complex social situations from th -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Conversation is limited to \u2018mummy\u2019 and \u2018bye bye\u2019 and simply standing up unaided is a major achievement. But 11-month-old babies have already mastered the fundamentals of physics. Baby boys and girls were surprised when a ball seemed to disappear through a solid wall. Their curiosity did not end there. When they were then given the ball to play with, they repeatedly banged it on a table in an apparent attempt to work out its secret. Scroll down for video. Babies in the study reacted with surprise when objects like this toy car appeared to break the laws of gravity. The babies were similarly perplexed by a toy car that appeared to defy the laws of gravity and float in thin air. And given it to play with later, they repeatedly dropped it on the floor. Importantly, the babies shown the magic tricks were more interested in playing with the ball than a brand new toy that they had never seen before. Babies might be smarter than we had previously thought and can understand complex social situations from th early age of just months. Researchers have found that even 11-month old babies can master the fundamentally of physics. The study found that babies are born with inn inn innate understanding of physics and can use this knowledge to understand the world around them. The study also found that babies can use their own observations to work out the laws of physics. The study showed that babies can even use their own observations to work out the laws of physics. The study showed that babies can even use their own observations to work out the laws of physics. The study showed that babies can even use their own observations to work out the laws of physics. The study showed that babies can even use their own observations to work out the laws of physics. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.65 ms /   399 runs   (    0.01 ms per token, 109465.02 tokens per second)\nllama_perf_context_print:        load time =    4547.59 ms\nllama_perf_context_print: prompt eval time =    1090.62 ms /   234 tokens (    4.66 ms per token,   214.56 tokens per second)\nllama_perf_context_print:        eval time =   22590.24 ms /   164 runs   (  137.75 ms per token,     7.26 tokens per second)\nllama_perf_context_print:       total time =   23713.34 ms /   398 tokens\nllama_perf_context_print:  mean exit layer =      24.98\nllama_perf_context_print:       exit ratio =       0.61  = 101 / 166 \nllama_perf_context_print:       exit layer = {32, 32, 20, 32, 29, 22, 32, 30, 16, 32, 32, 18, 18, 32, 32, 32, 20, 25, 18, 32, 27, 22, 20, 32, 32, 24, 16, 32, 30, 17, 32, 32, 18, 32, 16, 32, 27, 20, 32, 32, 29, 25, 22, 20, 32, 27, 17, 18, 32, 32, 29, 32, 16, 22, 20, 32, 17, 32, 32, 22, 32, 18, 32, 24, 17, 32, 27, 20, 18, 32, 32, 17, 32, 27, 32, 25, 32, 25, 27, 17, 30, 32, 18, 32, 20, 32, 27, 20, 17, 18, 32, 32, 15, 32, 20, 32, 22, 32, 18, 16, 17, 32, 32, 18, 32, 18, 32, 27, 18, 17, 17, 32, 18, 15, 32, 22, 32, 24, 32, 24, 16, 20, 29, 32, 18, 32, 16, 32, 27, 17, 15, 32, 18, 18, 15, 32, 13, 32, 27, 18, 20, 13, 17, 27, 32, 17, 32, 16, 32, 27, 17, 32, 18, 18, 17, 32, 32, 17, 32, 27, 17, 32, 15, 32, 17, 25, }\n"
    },
    {
        "inference_time": "23109.52",
        "tokens": "396",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Manny Pacquiao decided to train on the streets of Los Angeles on Friday as he ran along the trail at Griffith Park ahead of his highly-anticipated fight against Floyd Mayweather. The Filipino swapped the gym for the great outdoors as he worked on his fitness and stamina in preparation for what could be a lengthy bout against his welterweight rival. Pacquiao was spotted with several fitness fanatics during his run around one of the North America's largest urban parks. Manny Pacquiao is joined by several fitness fanatics as he jogs around Griffith Park. Pacquiao was joined by his dog Pacman as he ran around one of North America's largest urban parks. The Filipino boxer shows off his skills by shadow boxing in front of interested spectators. The 36-year-old was joined by his pet dog Pacman as he continued to gear up for his May 2 encounter against the undefeated Mayweather. As well as working on his aerobic endurance, Pacquiao was also tested on his core strength and showed off his impressive footwork -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Manny Pacquiao decided to train on the streets of Los Angeles on Friday as he ran along the trail at Griffith Park ahead of his highly-anticipated fight against Floyd Mayweather. The Filipino swapped the gym for the great outdoors as he worked on his fitness and stamina in preparation for what could be a lengthy bout against his welterweight rival. Pacquiao was spotted with several fitness fanatics during his run around one of the North America's largest urban parks. Manny Pacquiao is joined by several fitness fanatics as he jogs around Griffith Park. Pacquiao was joined by his dog Pacman as he ran around one of North America's largest urban parks. The Filipino boxer shows off his skills by shadow boxing in front of interested spectators. The 36-year-old was joined by his pet dog Pacman as he continued to gear up for his May 2 encounter against the undefeated Mayweather. As well as working on his aerobic endurance, Pacquiao was also tested on his core strength and showed off his impressive footwork by shadowboxing in front of a crowd of spectators who were eager to catch a glimpcis of the boxing world's most bankable boxer in action.\n\n Summary: Manny Pacquiao decided to train outside of the gym on Friday by running along the trail at Griffith park in Los Angeles. He was joined by several fitness fanatic spectators who were eagerly watching him train. Pacquiao was also accompanied by his dog Pacman as he worked on his fitness and stamina for his upcoming fight against Floyd Mayweather. He also showed off his impressive footwork by shadowboxing in front of the crowd spectators. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.22 ms /   397 runs   (    0.01 ms per token, 123138.96 tokens per second)\nllama_perf_context_print:        load time =    4526.32 ms\nllama_perf_context_print: prompt eval time =    1094.26 ms /   254 tokens (    4.31 ms per token,   232.12 tokens per second)\nllama_perf_context_print:        eval time =   21985.92 ms /   142 runs   (  154.83 ms per token,     6.46 tokens per second)\nllama_perf_context_print:       total time =   23109.52 ms /   396 tokens\nllama_perf_context_print:  mean exit layer =      26.03\nllama_perf_context_print:       exit ratio =       0.63  = 91 / 144 \nllama_perf_context_print:       exit layer = {32, 32, 25, 27, 16, 29, 18, 32, 32, 20, 22, 22, 32, 27, 30, 32, 28, 28, 28, 25, 20, 32, 32, 30, 32, 16, 17, 28, 29, 32, 32, 32, 22, 31, 32, 17, 29, 32, 31, 25, 18, 32, 32, 22, 20, 32, 27, 32, 16, 25, 25, 29, 29, 27, 27, 32, 22, 29, 32, 30, 30, 22, 18, 24, 18, 22, 32, 22, 16, 32, 32, 22, 32, 16, 32, 22, 14, 32, 20, 25, 32, 23, 32, 32, 20, 32, 32, 32, 29, 25, 22, 27, 32, 16, 32, 16, 32, 20, 32, 18, 32, 22, 32, 18, 20, 32, 16, 27, 25, 32, 29, 22, 27, 20, 32, 17, 32, 22, 20, 32, 22, 18, 32, 18, 32, 25, 18, 32, 16, 32, 18, 32, 22, 32, 16, 32, 18, 18, 32, 20, 25, 32, 32, 20, }\n"
    },
    {
        "inference_time": "59911.88",
        "tokens": "491",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A cat who had been stuck in a wall at a Cairo train station for five years has been freed at last. Biso had squeezed into the hole in the wall at the Mohamed Naguib metro station as a kitten in 2010 - but then soon became to big to escape. He survived thanks to an elderly man named Uncle Abdo, who gave the trapped cat water and fed him scraps of food  every day. Scroll down for video. Biso the cat, who was stuck behind a wall in a Cairo train station for five years, has finally been freed. The cat survived thanks to an elderly man called Uncle Abdo who fed him scraps of food every day. Now, the feline in finally free thanks to a social media campaign, according to the Cairo Post. Animal rights activist\u00a0Mounira Shehata told the newspaper the smell when the wall was removed was 'heinous'. She said: 'It was like a tomb. There were black worms along four meters of the edge, where Uncle Abdo was able to put in food for Biso.' Uncle Abdo, who named the cat, said Biso would regularly hide from bigger cats -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A cat who had been stuck in a wall at a Cairo train station for five years has been freed at last. Biso had squeezed into the hole in the wall at the Mohamed Naguib metro station as a kitten in 2010 - but then soon became to big to escape. He survived thanks to an elderly man named Uncle Abdo, who gave the trapped cat water and fed him scraps of food  every day. Scroll down for video. Biso the cat, who was stuck behind a wall in a Cairo train station for five years, has finally been freed. The cat survived thanks to an elderly man called Uncle Abdo who fed him scraps of food every day. Now, the feline in finally free thanks to a social media campaign, according to the Cairo Post. Animal rights activist\u00a0Mounira Shehata told the newspaper the smell when the wall was removed was 'heinous'. She said: 'It was like a tomb. There were black worms along four meters of the edge, where Uncle Abdo was able to put in food for Biso.' Uncle Abdo, who named the cat, said Biso would regularly hide from bigger cats in the area because they would bully him because he size. He said: 'He was very scared and would hide in the wall. He was like a child trapped in a cage.' Biso was rescued after a social media campaign, which was started by a group of animal lovers who were moved by the story of the cat stuck in the wall. The cat is now in a shelter, where he will be cared for until he can be adopted.\n\nVideo:\nhttps://www.dailymail.com/news/article-9253375/amp/Trapped cat- stuck-Cairo- train station-wall- five years-res finally-freed.html?__amp=true&amp=format&amp=text&amp;amp=true&amp=format&amp=text&amp;amp=true&amp=format&amp=text&amp;amp=true&amp=format&amp=text&amp;amp=true&amp=format&amp=textamp=true\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.95 ms /   492 runs   (    0.01 ms per token, 99494.44 tokens per second)\nllama_perf_context_print:        load time =    5522.95 ms\nllama_perf_context_print: prompt eval time =    1679.48 ms /   267 tokens (    6.29 ms per token,   158.98 tokens per second)\nllama_perf_context_print:        eval time =   58184.68 ms /   224 runs   (  259.75 ms per token,     3.85 tokens per second)\nllama_perf_context_print:       total time =   59911.88 ms /   491 tokens\nllama_perf_context_print:  mean exit layer =      27.02\nllama_perf_context_print:       exit ratio =       0.50  = 114 / 226 \nllama_perf_context_print:       exit layer = {32, 32, 27, 20, 25, 17, 32, 27, 32, 32, 22, 22, 18, 30, 32, 24, 32, 29, 32, 32, 32, 32, 18, 32, 32, 32, 32, 30, 32, 32, 29, 32, 32, 31, 32, 25, 16, 32, 29, 32, 32, 32, 29, 27, 32, 32, 27, 22, 32, 32, 24, 22, 17, 32, 26, 32, 32, 15, 32, 32, 15, 32, 32, 32, 18, 32, 32, 22, 32, 17, 24, 27, 22, 32, 30, 32, 18, 32, 32, 22, 32, 16, 32, 30, 32, 32, 28, 25, 22, 32, 32, 26, 32, 20, 32, 26, 28, 24, 30, 32, 32, 32, 32, 32, 16, 32, 14, 32, 32, 32, 32, 32, 17, 29, 32, 30, 32, 30, 32, 29, 32, 32, 32, 32, 29, 31, 32, 32, 30, 32, 25, 32, 23, 32, 32, 32, 29, 22, 17, 32, 32, 29, 24, 18, 32, 32, 18, 32, 32, 27, 30, 32, 29, 32, 22, 32, 32, 29, 16, 32, 32, 29, 14, 32, 32, 27, 14, 32, 12, 32, 32, 25, 10, 32, 32, 27, 10, 32, 27, 25, 16, 32, 16, 32, 32, 25, 16, 32, 32, 27, 14, 32, 20, 25, 12, 32, 10, 32, 27, 25, 8, 32, 30, 27, 8, 32, 20, 25, 8, 32, 8, 32, 18, 22, 8, 32, 27, 29, 8, 32, 20, 32, 17, 17, 32, 19, }\n"
    },
    {
        "inference_time": "15784.28",
        "tokens": "349",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: At least 35 workers have been killed at a dairy factory in Yemen as Saudi Arabian warplanes continue to bomb\u00a0Shiite rebel positions in both the north and the south of the country. Today's airstrikes targeted rebel-controlled army camps in the Red Sea port city of Hodeida, and hit warehouses belonging to a factory that produces dairy products. The jets drew return fire from anti-aircraft guns as the Saudi-led campaign to defeat the Iran-backed Shiite rebels loyal to Yemen's deposed president Ali Abdullah Saleh entered its seventh day. The attack on the dairy factory comes as Saudi Arabia hinted that it was prepared to launch a ground invasion in Yemen in the hope of forcing the Houthi rebels to allow\u00a0current president, Abed Rabbo Mansour Hadi to return to the country, after he fled\u00a0the southern port city of Aden last week. Hit: Smoke rises from the dairy factory hit by an air strike in Yemen's western port city of Houdieda today. Blast:\u00a0Today's airstrikes targeted rebel-controlled army camps in the  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: At least 35 workers have been killed at a dairy factory in Yemen as Saudi Arabian warplanes continue to bomb\u00a0Shiite rebel positions in both the north and the south of the country. Today's airstrikes targeted rebel-controlled army camps in the Red Sea port city of Hodeida, and hit warehouses belonging to a factory that produces dairy products. The jets drew return fire from anti-aircraft guns as the Saudi-led campaign to defeat the Iran-backed Shiite rebels loyal to Yemen's deposed president Ali Abdullah Saleh entered its seventh day. The attack on the dairy factory comes as Saudi Arabia hinted that it was prepared to launch a ground invasion in Yemen in the hope of forcing the Houthi rebels to allow\u00a0current president, Abed Rabbo Mansour Hadi to return to the country, after he fled\u00a0the southern port city of Aden last week. Hit: Smoke rises from the dairy factory hit by an air strike in Yemen's western port city of Houdieda today. Blast:\u00a0Today's airstrikes targeted rebel-controlled army camps in the  Red Sea port city of Hodeida, and hit warehouses belonging to a factory that produces dairy products. Dead: At least 35 workers have been killed at a dairy factory in Yemen as Saudi Arabian warplanes continue to bomb Shiite rebel position in both the north and the south of the country. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.61 ms /   350 runs   (    0.00 ms per token, 217121.59 tokens per second)\nllama_perf_context_print:        load time =    5408.03 ms\nllama_perf_context_print: prompt eval time =    1592.51 ms /   277 tokens (    5.75 ms per token,   173.94 tokens per second)\nllama_perf_context_print:        eval time =   14176.04 ms /    72 runs   (  196.89 ms per token,     5.08 tokens per second)\nllama_perf_context_print:       total time =   15784.28 ms /   349 tokens\nllama_perf_context_print:  mean exit layer =      25.27\nllama_perf_context_print:       exit ratio =       0.61  = 45 / 74 \nllama_perf_context_print:       exit layer = {32, 32, 20, 32, 18, 32, 18, 32, 27, 17, 18, 32, 22, 32, 30, 29, 32, 20, 16, 32, 22, 32, 32, 18, 22, 29, 32, 27, 32, 16, 32, 18, 22, 32, 20, 16, 32, 27, 17, 32, 19, 32, 18, 32, 18, 32, 32, 20, 16, 16, 32, 22, 25, 29, 17, 18, 32, 25, 18, 29, 27, 22, 29, 32, 17, 32, 17, 18, 32, 16, 32, 17, 32, 32, }\n"
    },
    {
        "inference_time": "58125.87",
        "tokens": "507",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. As officials didn't want to risk running trains under the over-hanging truck, replacement shuttle buses were run for southbound passengers with long queues forming. Scroll down for video. Aerieal shot: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. Overhanging: The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. Carol Hullum, who lives near the crash site, said the collision caused headaches for locals trying to get to work. 'I was just thinking too that the com -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. As officials didn't want to risk running trains under the over-hanging truck, replacement shuttle buses were run for southbound passengers with long queues forming. Scroll down for video. Aerieal shot: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. Overhanging: The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. Carol Hullum, who lives near the crash site, said the collision caused headaches for locals trying to get to work. 'I was just thinking too that the comute is going to be a mess mess mess today,' she said. 'It's a mess mess mess mess every day, but this is just a mess mess mess mess on top of that mess mess mess mess.' Commuters: As officials didn't want to risk running trains under the over-hanging truck, replacement shuttle buses were run for southbound passengers with long queues forming. Scroll down for video. 'It's a mess mess mess mess everyday, but this is just a mess mess mess mess on top of that mess mess mess mess,' said Carol Hullum, who lives near the crash site. 'I was just thinking too that the commute is going to be a mess mess mess mess today.' Scroll down for video. The crash happened at about 6am on Interstate 93 southbound in the city's Dorchester neighborhood. The box tr truck collided with a car on the elevated highway over subway and rail tracks. Both drivers were taken to hospital with non-life-threatening injuries. The incident caused del del delay for hundreds of morning commuters who were traveling southbound on Interstate 93. Replacement buses were run for\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.76 ms /   508 runs   (    0.01 ms per token, 88255.73 tokens per second)\nllama_perf_context_print:        load time =    5589.30 ms\nllama_perf_context_print: prompt eval time =    1641.44 ms /   252 tokens (    6.51 ms per token,   153.52 tokens per second)\nllama_perf_context_print:        eval time =   56432.98 ms /   255 runs   (  221.31 ms per token,     4.52 tokens per second)\nllama_perf_context_print:       total time =   58125.87 ms /   507 tokens\nllama_perf_context_print:  mean exit layer =      26.38\nllama_perf_context_print:       exit ratio =       0.58  = 150 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 25, 27, 16, 32, 23, 25, 32, 32, 29, 32, 18, 30, 31, 32, 27, 30, 32, 25, 23, 21, 23, 32, 32, 32, 16, 32, 29, 32, 32, 25, 23, 27, 27, 32, 32, 32, 32, 25, 27, 23, 25, 29, 32, 32, 32, 31, 32, 31, 32, 27, 32, 32, 29, 16, 32, 20, 25, 27, 22, 20, 32, 22, 22, 27, 17, 32, 20, 32, 29, 18, 32, 18, 32, 17, 32, 18, 22, 18, 32, 24, 27, 25, 32, 32, 20, 32, 18, 29, 32, 32, 29, 29, 31, 25, 18, 32, 23, 32, 20, 32, 16, 32, 20, 17, 32, 22, 17, 32, 22, 18, 32, 15, 32, 22, 24, 22, 24, 32, 17, 32, 24, 32, 32, 30, 17, 32, 17, 32, 22, 15, 32, 32, 32, 32, 18, 32, 18, 32, 17, 25, 18, 17, 32, 20, 17, 17, 32, 22, 27, 27, 27, 32, 32, 22, 25, 17, 32, 32, 32, 32, 30, 27, 32, 22, 17, 32, 22, 25, 29, 18, 32, 18, 32, 17, 17, 32, 17, 32, 32, 18, 32, 27, 32, 32, 22, 24, 18, 32, 27, 18, 32, 22, 30, 32, 32, 17, 32, 20, 32, 18, 17, 32, 19, 30, 32, 25, 18, 32, 27, 32, 17, 20, 16, 32, 15, 32, 30, 25, 32, 32, 17, 32, 32, 19, 32, 25, 25, 23, 30, 32, 17, 32, 20, 32, 18, 32, 30, 32, 32, 18, 32, 25, 32, 27, 17, 32, 22, 32, 32, 19, 32, 17, 32, 29, }\n"
    },
    {
        "inference_time": "39593.55",
        "tokens": "515",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Saracens director of rugby Mark McCall lauded his young guns after their latest European heartache before declaring he has no intention of overspending in a competitive post-World Cup transfer market. McCall watched his side, which contained five English-qualified forwards in the starting pack, battle in vain before losing 13-9 to the Clermont on Saturday. Saracens\u2019 millionaire chairman Nigel Wray spent much of last week repeating his belief the cap should be scrapped in order for Saracens to compete at Europe\u2019s top table, raising expectations they could be set to land a \u2018marquee\u2019 player from outside the league whose wages would sit outside next season\u2019s \u00a35.5m cap. Maro Itoje (second left) was one of five England-qualified forwards in the Saracens pack that faced Clermont. Mako Vunipola tries to fend off Clermont lock Jamie Cudmore during a ferocious contest. Saracens director of rugby Mark McCall saw his side come agonisingly close to reaching the final. But McCall said: \u2018We know where we\u2019d like t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Saracens director of rugby Mark McCall lauded his young guns after their latest European heartache before declaring he has no intention of overspending in a competitive post-World Cup transfer market. McCall watched his side, which contained five English-qualified forwards in the starting pack, battle in vain before losing 13-9 to the Clermont on Saturday. Saracens\u2019 millionaire chairman Nigel Wray spent much of last week repeating his belief the cap should be scrapped in order for Saracens to compete at Europe\u2019s top table, raising expectations they could be set to land a \u2018marquee\u2019 player from outside the league whose wages would sit outside next season\u2019s \u00a35.5m cap. Maro Itoje (second left) was one of five England-qualified forwards in the Saracens pack that faced Clermont. Mako Vunipola tries to fend off Clermont lock Jamie Cudmore during a ferocious contest. Saracens director of rugby Mark McCall saw his side come agonisingly close to reaching the final. But McCall said: \u2018We know where we\u2019d like t...\nSource: Sporting Life\nSummary: Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar Sar\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   516 runs   (    0.01 ms per token, 94609.46 tokens per second)\nllama_perf_context_print:        load time =    5545.56 ms\nllama_perf_context_print: prompt eval time =    1669.56 ms /   260 tokens (    6.42 ms per token,   155.73 tokens per second)\nllama_perf_context_print:        eval time =   37874.67 ms /   255 runs   (  148.53 ms per token,     6.73 tokens per second)\nllama_perf_context_print:       total time =   39593.55 ms /   515 tokens\nllama_perf_context_print:  mean exit layer =      22.44\nllama_perf_context_print:       exit ratio =       0.96  = 247 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 29, 22, 22, 25, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, }\n"
    },
    {
        "inference_time": "53507.82",
        "tokens": "512",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The decomposing bodies of the men accused of killing 148 innocent people at a Kenyan university were paraded in front of a large crowd at a primary school today. The authorities drove the naked, bloated corpses of the four alleged terrorists around the town in a pickup truck from the mortuary to Garissa Primary School. It comes as survivor\u00a0Cynthia Cheroitich, 19, who spent two days hiding in a wardrobe and drinking body lotion to survive, was rescued after al-Shabaab gunmen stormed Garissa University College on Thursday. Scroll down for video. Paraded: The naked bodies of the alleged attackers were driven around Garissa before the pickup truck parked at a primary school. In the primary school playground, hundreds gathered to look at the bodies of the men accused of being the attackers. Hundreds of people, some grieving for the loved ones they lost in the siege, gathered in the sweltering heat to catch a glimpse of the men who are believed to be responsible. Pictures of the crowds and the bodies pil -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The decomposing bodies of the men accused of killing 148 innocent people at a Kenyan university were paraded in front of a large crowd at a primary school today. The authorities drove the naked, bloated corpses of the four alleged terrorists around the town in a pickup truck from the mortuary to Garissa Primary School. It comes as survivor\u00a0Cynthia Cheroitich, 19, who spent two days hiding in a wardrobe and drinking body lotion to survive, was rescued after al-Shabaab gunmen stormed Garissa University College on Thursday. Scroll down for video. Paraded: The naked bodies of the alleged attackers were driven around Garissa before the pickup truck parked at a primary school. In the primary school playground, hundreds gathered to look at the bodies of the men accused of being the attackers. Hundreds of people, some grieving for the loved ones they lost in the siege, gathered in the sweltering heat to catch a glimpse of the men who are believed to be responsible. Pictures of the crowds and the bodies pilled up in the playground were sent to MailOnline by a witness who witnessed the scene. The bodies were covered in flies and insects as the heat of the day took its effect on them. The bodies of the four men who carried out the attack were paraded in front of a crowds of people who had gathered at the primary school. The bodies were covered in flies and insects as the heat of the day took its effect on them. The bodies of the four men who carried out the attack were paraded in front of a crowd of people who had gathered at the primary school. The bodies were covered in flies & f insects as the heat of the day took its effect on them. The bodies of the four men who carried out the attack were paraded in front of a crowd of people who had gather at the primary school. The bodies of the four who carried out the attack were paraded in front of a crowd of people who had gather at the primary school. The bodies of the four who carried out the attack were paraded in front of a crowd of people who had gather at the primary school. The body of one of the attackers was seen with a gunshot w w wound to his head, while others\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.10 ms /   513 runs   (    0.01 ms per token, 84057.02 tokens per second)\nllama_perf_context_print:        load time =    5417.68 ms\nllama_perf_context_print: prompt eval time =    1609.04 ms /   257 tokens (    6.26 ms per token,   159.72 tokens per second)\nllama_perf_context_print:        eval time =   51846.42 ms /   255 runs   (  203.32 ms per token,     4.92 tokens per second)\nllama_perf_context_print:       total time =   53507.82 ms /   512 tokens\nllama_perf_context_print:  mean exit layer =      24.98\nllama_perf_context_print:       exit ratio =       0.61  = 156 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 20, 32, 16, 29, 17, 31, 32, 32, 32, 32, 30, 32, 32, 28, 32, 28, 20, 32, 32, 22, 32, 32, 29, 32, 32, 31, 22, 16, 27, 32, 32, 32, 32, 17, 32, 32, 19, 25, 15, 32, 32, 22, 13, 32, 18, 20, 15, 32, 20, 29, 30, 32, 32, 27, 32, 18, 22, 32, 22, 20, 30, 32, 18, 32, 22, 32, 32, 22, 17, 32, 32, 22, 32, 32, 27, 32, 24, 17, 32, 15, 17, 32, 32, 15, 32, 16, 32, 32, 18, 20, 14, 18, 32, 22, 12, 32, 17, 32, 14, 32, 20, 20, 20, 32, 25, 27, 18, 18, 17, 17, 25, 18, 32, 15, 32, 20, 32, 30, 22, 16, 18, 32, 22, 32, 22, 18, 32, 24, 16, 32, 22, 14, 17, 32, 27, 14, 32, 14, 32, 32, 17, 15, 32, 16, 32, 22, 13, 32, 18, 24, 15, 32, 20, 18, 32, 18, 32, 27, 18, 22, 18, 16, 22, 14, 32, 16, 32, 24, 32, 29, 22, 17, 32, 20, 22, 16, 32, 18, 24, 32, 20, 18, 32, 17, 32, 27, 17, 32, 16, 32, 20, 14, 32, 14, 32, 27, 17, 15, 32, 15, 32, 32, 22, 15, 32, 18, 17, 32, 20, 17, 32, 17, 32, 27, 17, 32, 16, 32, 25, 14, 32, 14, 32, 23, 18, 32, 21, 17, 32, 29, 22, 17, 27, 32, 29, 29, 17, 32, 27, 32, 30, 32, 32, 25, 25, 25, 29, 32, 23, 20, 32, 30, 18, }\n"
    },
    {
        "inference_time": "23558.41",
        "tokens": "345",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea shot-stopper looks certain to leave Stamford Bridge at the end of the season after losing the No 1 spot to Thibaut Courtois. Cech will hold talks with manager Jose Mourinho at the end of the season where the keeper will confirm his decision to leave west London. Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea stopper looks certain to leave Stamford Bridge at the end of the season after losing his No 1 spot. Arsene Wenger must be swift with a move for his top goalkeeping target to beat the competition to him. The Gunners are long-term admirers of Cech and are considering a move for the keeper ahead of next season. However, Arsenal will not be the only club interested in luring Cech when the transfer window reopens. Liverpool, Paris Saint-Germain, Roma and Inter Milan are all showing an interest in the 32-year-old and Cech is under -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea shot-stopper looks certain to leave Stamford Bridge at the end of the season after losing the No 1 spot to Thibaut Courtois. Cech will hold talks with manager Jose Mourinho at the end of the season where the keeper will confirm his decision to leave west London. Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea stopper looks certain to leave Stamford Bridge at the end of the season after losing his No 1 spot. Arsene Wenger must be swift with a move for his top goalkeeping target to beat the competition to him. The Gunners are long-term admirers of Cech and are considering a move for the keeper ahead of next season. However, Arsenal will not be the only club interested in luring Cech when the transfer window reopens. Liverpool, Paris Saint-Germain, Roma and Inter Milan are all showing an interest in the 32-year-old and Cech is under no pressure to make a decision on his future. The keeper has been linked with a move to Arsenal for several years and the Gunners are now in pole position to secure his services. Cech has been one of the best goalkeeper in the world for over a decade and would be an excellent addition to any team. Arsenal must act fast to secure his signature before the transfer window window closes. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.92 ms /   346 runs   (    0.01 ms per token, 180020.81 tokens per second)\nllama_perf_context_print:        load time =    5508.24 ms\nllama_perf_context_print: prompt eval time =    1602.63 ms /   258 tokens (    6.21 ms per token,   160.99 tokens per second)\nllama_perf_context_print:        eval time =   21936.52 ms /    87 runs   (  252.14 ms per token,     3.97 tokens per second)\nllama_perf_context_print:       total time =   23558.41 ms /   345 tokens\nllama_perf_context_print:  mean exit layer =      27.51\nllama_perf_context_print:       exit ratio =       0.56  = 50 / 89 \nllama_perf_context_print:       exit layer = {32, 32, 17, 27, 32, 29, 17, 32, 29, 19, 32, 31, 32, 22, 25, 32, 32, 23, 32, 22, 29, 23, 23, 32, 29, 32, 17, 32, 32, 29, 25, 15, 32, 27, 32, 32, 17, 27, 32, 32, 22, 30, 32, 22, 32, 20, 32, 22, 29, 31, 32, 18, 32, 31, 32, 29, 32, 30, 32, 32, 28, 32, 20, 25, 32, 17, 32, 17, 32, 27, 27, 20, 27, 32, 32, 20, 27, 32, 29, 32, 29, 31, 32, 16, 14, 32, 32, 32, 20, }\n"
    },
    {
        "inference_time": "44910.77",
        "tokens": "441",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Frankie Dettori will be left to focus on the British Flat season this summer after boss Sheik Joaan Al Thani signed up Gregory Benoist to ride the horses which race under his Al Shaqab banner in France. The only exception will be Sheik Joaan\u2019s dual Arc winner Treve, who will continue to be partnered by veteran Thierry Jarnet. Benoist is the only jockey to have ridden Elie Lellouche-trained Ectot, who was unplaced for Al Shaqab in last year\u2019s Arc having gone into the race one of the leading fancies having won the Prix Niel. Frankie Dettori is to concentrate on the British flat season and not being picked to ride in France. The recruitment of Benoist could be seen as a snub for Dettori, who was replaced by Jarnet on Treve in the build-up to her Arc success. But Herbert insisted it was no longer logistically feasible to have one jockey covering both countries because Sheik Joaan\u2019s operation has expanded so much. Herbert said: \u2018Last season it was feasible to keep travelling back and forth across the Ch -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Frankie Dettori will be left to focus on the British Flat season this summer after boss Sheik Joaan Al Thani signed up Gregory Benoist to ride the horses which race under his Al Shaqab banner in France. The only exception will be Sheik Joaan\u2019s dual Arc winner Treve, who will continue to be partnered by veteran Thierry Jarnet. Benoist is the only jockey to have ridden Elie Lellouche-trained Ectot, who was unplaced for Al Shaqab in last year\u2019s Arc having gone into the race one of the leading fancies having won the Prix Niel. Frankie Dettori is to concentrate on the British flat season and not being picked to ride in France. The recruitment of Benoist could be seen as a snub for Dettori, who was replaced by Jarnet on Treve in the build-up to her Arc success. But Herbert insisted it was no longer logistically feasible to have one jockey covering both countries because Sheik Joaan\u2019s operation has expanded so much. Herbert said: \u2018Last season it was feasible to keep travelling back and forth across the Chunnel but now we have so many horses in France and the number of races has increased. It\u2019s just too much time away from home and the horses. \u2018We have to be realistic and realistic about Frankie's availability. He\u2019s a top jockey but he can only do so much. We have to priorities our owners\u2019 needs and the horses needs. \u2018We have to make sure we are giving the best possible service to our owners and the best possible service to our own horses. \u2018We have to be realistic about what we can do and what we can\u2019t do. Frankie will be concentrating on British racing now. He\u2019s a great jockey and we will still work with him but we now have to priorities our owners' needs and the needs of our own horses. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.75 ms /   442 runs   (    0.01 ms per token, 117741.08 tokens per second)\nllama_perf_context_print:        load time =    5478.26 ms\nllama_perf_context_print: prompt eval time =    1610.19 ms /   269 tokens (    5.99 ms per token,   167.06 tokens per second)\nllama_perf_context_print:        eval time =   43265.58 ms /   172 runs   (  251.54 ms per token,     3.98 tokens per second)\nllama_perf_context_print:       total time =   44910.77 ms /   441 tokens\nllama_perf_context_print:  mean exit layer =      27.47\nllama_perf_context_print:       exit ratio =       0.48  = 84 / 174 \nllama_perf_context_print:       exit layer = {32, 32, 29, 32, 32, 25, 32, 32, 17, 32, 25, 32, 32, 19, 17, 32, 32, 29, 32, 32, 31, 32, 32, 20, 32, 18, 32, 32, 17, 32, 32, 32, 32, 30, 32, 32, 30, 32, 32, 18, 32, 32, 16, 32, 27, 32, 16, 29, 29, 32, 31, 32, 29, 32, 31, 32, 32, 18, 32, 29, 31, 18, 32, 32, 22, 32, 32, 32, 24, 32, 25, 32, 16, 32, 29, 32, 31, 32, 27, 18, 31, 32, 32, 32, 30, 32, 16, 32, 28, 32, 32, 20, 17, 30, 25, 19, 32, 15, 32, 32, 18, 32, 30, 32, 20, 16, 32, 32, 30, 32, 32, 16, 32, 28, 14, 32, 32, 27, 17, 32, 32, 25, 18, 15, 32, 32, 32, 20, 29, 32, 31, 32, 32, 32, 20, 20, 32, 20, 32, 29, 29, 16, 30, 20, 32, 22, 32, 22, 18, 32, 32, 29, 18, 32, 22, 18, 32, 32, 32, 22, 32, 16, 18, 17, 32, 15, 32, 25, 13, 22, 13, 32, 32, 32, }\n"
    },
    {
        "inference_time": "58533.07",
        "tokens": "563",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: (CNN)Lois Lilienstein, co-star of \"Sharon, Lois & Bram's Elephant Show\" \u2014 the Canadian preschool show that ran on Nickelodeon during the early 1990s \u2014 has died, aged 78. Her son, David Lilienstein, told CBC News that his mother died in Toronto on Wednesday night from a rare form of cancer first diagnosed last October. \"She knew it was happening, she was at peace with it, and she died very peacefully and not in pain,\" he told the Canadian broadcaster. Liz Smith at 92. Lilienstein was born in Chicago in 1936 and moved to Toronto in 1996 with her son and her husband, Ernest. She joined Sharon Hampson and Bram Morrison to launch their singing trio in 1978. The TV show \"Sharon, Lois & Bram's Elephant Show\" ran on CBC in Canada from 1984 to 1989, before continuing in reruns on Nickelodeon stateside until 1995. The 30 most popular film producers in Hollywood. Sally Catto, general manager, programming at CBC Television, paid tribute to Lilienstein for her contribution to Canadian kids TV. \"Sharon, Lois and -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: (CNN)Lois Lilienstein, co-star of \"Sharon, Lois & Bram's Elephant Show\" \u2014 the Canadian preschool show that ran on Nickelodeon during the early 1990s \u2014 has died, aged 78. Her son, David Lilienstein, told CBC News that his mother died in Toronto on Wednesday night from a rare form of cancer first diagnosed last October. \"She knew it was happening, she was at peace with it, and she died very peacefully and not in pain,\" he told the Canadian broadcaster. Liz Smith at 92. Lilienstein was born in Chicago in 1936 and moved to Toronto in 1996 with her son and her husband, Ernest. She joined Sharon Hampson and Bram Morrison to launch their singing trio in 1978. The TV show \"Sharon, Lois & Bram's Elephant Show\" ran on CBC in Canada from 1984 to 1989, before continuing in reruns on Nickelodeon stateside until 1995. The 30 most popular film producers in Hollywood. Sally Catto, general manager, programming at CBC Television, paid tribute to Lilienstein for her contribution to Canadian kids TV. \"Sharon, Lois and Bram were a beloved part of our childhoods,\" said Catto. \"Lois Lilienstein was a talented performer who brought joy and laughter to generations of children and families across Canada and around the world. Her legacy will continue to inspire and entertain aud aud aud aud audience for gener gener generations to come.\nLois Lilienstein, a co-star of the popular Canadian children's show \"Sharon, Lois & Bram's Elephant Show,\" has been diagnosed with a rare form of cancer. Lois Lilienstein passed away on Wednes. she was 78. Lois Lilienstein was born in Chicago Illinois in 1936 and moved to Toronto in 1996 with her son and husband. She joined forces with Sharon Hampton and Bram Morrison to launch their singing trio in 1978. The TV show \"Sharon, Lois &amp; Bram's Elephant Show\" ran on CBC in Canada from 1984 to 1989 before continuing in reruns on Nickelodeon in the United States until 1995.\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.76 ms /   564 runs   (    0.01 ms per token, 97967.69 tokens per second)\nllama_perf_context_print:        load time =    5441.87 ms\nllama_perf_context_print: prompt eval time =    1626.79 ms /   308 tokens (    5.28 ms per token,   189.33 tokens per second)\nllama_perf_context_print:        eval time =   56854.23 ms /   255 runs   (  222.96 ms per token,     4.49 tokens per second)\nllama_perf_context_print:       total time =   58533.07 ms /   563 tokens\nllama_perf_context_print:  mean exit layer =      26.04\nllama_perf_context_print:       exit ratio =       0.61  = 157 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 25, 32, 32, 27, 32, 22, 20, 16, 32, 32, 32, 29, 16, 32, 25, 27, 22, 32, 27, 23, 22, 32, 32, 32, 21, 32, 23, 32, 25, 32, 29, 29, 27, 18, 32, 29, 27, 22, 17, 32, 24, 32, 18, 32, 32, 29, 17, 22, 32, 22, 32, 25, 29, 32, 32, 31, 32, 32, 27, 22, 25, 22, 32, 17, 25, 18, 32, 32, 32, 23, 32, 32, 25, 32, 22, 32, 32, 32, 22, 16, 32, 22, 29, 18, 32, 20, 32, 32, 25, 32, 32, 22, 20, 32, 22, 32, 29, 27, 22, 16, 32, 25, 27, 32, 29, 32, 16, 32, 16, 32, 17, 19, 27, 15, 18, 32, 29, 32, 25, 22, 32, 27, 32, 29, 32, 17, 32, 29, 22, 32, 18, 18, 32, 29, 32, 25, 20, 32, 32, 20, 32, 20, 25, 27, 23, 25, 32, 32, 27, 27, 22, 22, 18, 29, 32, 29, 29, 18, 25, 17, 32, 17, 32, 19, 19, 32, 20, 17, 32, 25, 18, 32, 20, 17, 32, 18, 32, 25, 18, 32, 23, 16, 23, 20, 27, 20, 25, 29, 29, 20, 18, 23, 16, 32, 18, 32, 32, 18, 17, 32, 18, 18, 17, 32, 32, 22, 18, 32, 32, 18, 24, 25, 32, 22, 17, 32, 22, 32, 18, 32, 20, 24, 32, 32, 18, 32, 20, 29, 29, 18, 32, 27, 32, 16, 25, 32, 25, 29, 18, 32, 17, 32, 32, 32, 29, 16, 32, 17, 32, 25, 32, 16, 32, 18, 32, }\n"
    },
    {
        "inference_time": "56590.50",
        "tokens": "532",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: This incredible video shows the moment a world freediving champion jumped into the world\u2019s second deepest underwater sink hole. The dramatic footage shows Guillame N\u00e9ry standing at the edge before jumping in - all without scuba gear and simply holding his breath. And in the video he seemingly reaches the bottom of the 660ft (200 metes) hole before swimming back to the surface. French free diver jumped into Dean's Blue Hole in the Bahamas. Guillame N\u00e9ry is seen at the edge before taking the plunge (shown). The hole is 660ft (200 metres) deep, although he doesn't go to the bottom. Free divers are able to hold their breath for more than 20 minutes. The video was filmed in 2010 by the Frenchman's girlfriend and fellow freediver Julie Gautier. It takes place at Dean\u2019s Blue Hole in the Bahamas, the deepest so-called \u2018blue hole\u2019 in the world with an underwater entrance. In order to stay alive, the brain is constantly telling the body to breathe. If you don\u2019t obey its primitive urges, carbon dioxide soon s -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: This incredible video shows the moment a world freediving champion jumped into the world\u2019s second deepest underwater sink hole. The dramatic footage shows Guillame N\u00e9ry standing at the edge before jumping in - all without scuba gear and simply holding his breath. And in the video he seemingly reaches the bottom of the 660ft (200 metes) hole before swimming back to the surface. French free diver jumped into Dean's Blue Hole in the Bahamas. Guillame N\u00e9ry is seen at the edge before taking the plunge (shown). The hole is 660ft (200 metres) deep, although he doesn't go to the bottom. Free divers are able to hold their breath for more than 20 minutes. The video was filmed in 2010 by the Frenchman's girlfriend and fellow freediver Julie Gautier. It takes place at Dean\u2019s Blue Hole in the Bahamas, the deepest so-called \u2018blue hole\u2019 in the world with an underwater entrance. In order to stay alive, the brain is constantly telling the body to breathe. If you don\u2019t obey its primitive urges, carbon dioxide soon saturates the brain, leading to dizziness and loss of consciousness. Free div diving is an extreme sport where competitors dive without scuba g gear, relying on their own body to breath underwater. The sport requires a specialized technique called the \"free diving technique\" which allows divers to breath in a way that conserves oxygen and maximizes oxygen intake. Free diving technique involves breath breathing in a way that conserves energy and maximizes oxygen intake. The technique also involves breathing breathing in a way that conser energy energy and maximizes oxygen intake. The technique also involves breathing in a way that conser energy and maxims oxygen intake. The technique also involves breathing in a manner that conser energy and maximizes oxygen intake. The technique also involves breathing in a manner that conser energy and maximizes oxygen intake. The technique also involves breathing in a manner that conser energy and maximizes oxygen intake. The technique also involves breathing in a manner that conser energy and maximizes oxygen intake. The technique also involves breathing in a manner that conser energy and maximizes oxygen intake. The technique also involves breathing in\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.53 ms /   533 runs   (    0.01 ms per token, 96331.10 tokens per second)\nllama_perf_context_print:        load time =    5554.53 ms\nllama_perf_context_print: prompt eval time =    1637.89 ms /   277 tokens (    5.91 ms per token,   169.12 tokens per second)\nllama_perf_context_print:        eval time =   54901.85 ms /   255 runs   (  215.30 ms per token,     4.64 tokens per second)\nllama_perf_context_print:       total time =   56590.50 ms /   532 tokens\nllama_perf_context_print:  mean exit layer =      25.46\nllama_perf_context_print:       exit ratio =       0.56  = 145 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 17, 32, 29, 22, 29, 27, 32, 27, 27, 32, 20, 16, 30, 32, 22, 22, 16, 32, 30, 32, 20, 20, 32, 32, 30, 32, 17, 32, 27, 27, 27, 27, 32, 32, 27, 32, 18, 16, 18, 32, 22, 29, 22, 32, 32, 27, 32, 32, 18, 20, 32, 20, 32, 20, 32, 22, 16, 20, 32, 16, 32, 32, 25, 18, 32, 32, 27, 32, 32, 32, 32, 18, 32, 32, 32, 27, 17, 32, 32, 29, 32, 22, 16, 17, 32, 29, 17, 14, 32, 30, 25, 17, 32, 27, 18, 32, 32, 17, 32, 20, 32, 32, 25, 32, 20, 30, 32, 22, 27, 18, 32, 32, 17, 25, 16, 32, 22, 19, 32, 32, 17, 32, 18, 32, 32, 16, 32, 20, 16, 32, 22, 14, 32, 29, 24, 14, 32, 20, 32, 32, 16, 32, 17, 32, 32, 20, 32, 20, 17, 32, 22, 15, 32, 16, 18, 29, 32, 17, 32, 25, 22, 32, 18, 32, 32, 24, 32, 20, 17, 32, 22, 16, 32, 15, 32, 18, 32, 22, 17, 32, 27, 32, 19, 32, 32, 16, 32, 20, 17, 32, 22, 16, 32, 14, 32, 18, 32, 22, 16, 32, 18, 32, 18, 32, 32, 16, 18, 20, 17, 32, 22, 16, 32, 14, 32, 20, 32, 22, 12, 32, 17, 32, 18, 32, 32, 17, 18, 20, 17, 32, 25, 16, 32, 14, 32, 22, 32, 22, 17, 32, 17, 32, 20, 32, 32, 17, 32, 20, 17, 32, 25, 16, 32, }\n"
    },
    {
        "inference_time": "46721.05",
        "tokens": "473",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Kabul, Afghanistan (CNN)A suicide bomber on a motorbike blew himself up in front of the Kabul Bank in Jalalabad early Saturday, a local government spokesman said. The ISIS terrorist group claimed responsibility for the attack. The explosion killed at least 33 people and injured more than 100 others, public health spokesman Najibullah Kamawal said. The claim appears to be the first in Afghanistan by ISIS, CNN's Nick Paton Walsh said. It was made by an offshoot called ISIS Wilayat Khorasan. In a statement, the group said the bomber was named Abu Mohammad and he belonged to their ranks. He was targeting government workers collecting their pay at the bank, the terrorists said. He detonated his charge at the peak of rush hour on the first day of the week, when the bank would be expected to be crowded. Taliban spokesman Zabiullah Mujahid, in a text message to journalists, distanced his group from the attack. \"The explosion in Jalalabad doesn't have anything to do with us and we condemn it,\" he said. ISIS -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Kabul, Afghanistan (CNN)A suicide bomber on a motorbike blew himself up in front of the Kabul Bank in Jalalabad early Saturday, a local government spokesman said. The ISIS terrorist group claimed responsibility for the attack. The explosion killed at least 33 people and injured more than 100 others, public health spokesman Najibullah Kamawal said. The claim appears to be the first in Afghanistan by ISIS, CNN's Nick Paton Walsh said. It was made by an offshoot called ISIS Wilayat Khorasan. In a statement, the group said the bomber was named Abu Mohammad and he belonged to their ranks. He was targeting government workers collecting their pay at the bank, the terrorists said. He detonated his charge at the peak of rush hour on the first day of the week, when the bank would be expected to be crowded. Taliban spokesman Zabiullah Mujahid, in a text message to journalists, distanced his group from the attack. \"The explosion in Jalalabad doesn't have anything to do with us and we condemn it,\" he said. ISIS has been active in Afghanistan since 2015, but this is the first time it has claim responsibility for an attack in Afghanistan, CNN's Walsh said. The attack comes as the US and Afghanistan are preparing for an expected increase in violence as the US withdraws its troops from Afghanistan.\n\nAnalysis:\n\n this article is about a recent suicide bombing attack in Jalalabad, Afghanistan. The attack occurred in front of the Kabul Bank and was targeting government workers who were collecting their owners at the bank. The attack killed at least 33 people and injured more than 100 others. The terrorist group ISIS claimed responsibility for this attack, which is the first time they have claimed responsibility for an attack in Afghanistan. The article also mentions that the US and Afghanistan are preparing for an expected increase in violence as the US withdrawal its troops from Afghanistan.\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.55 ms /   474 runs   (    0.01 ms per token, 104061.47 tokens per second)\nllama_perf_context_print:        load time =    5576.38 ms\nllama_perf_context_print: prompt eval time =    1664.98 ms /   275 tokens (    6.05 ms per token,   165.17 tokens per second)\nllama_perf_context_print:        eval time =   45016.08 ms /   198 runs   (  227.35 ms per token,     4.40 tokens per second)\nllama_perf_context_print:       total time =   46721.05 ms /   473 tokens\nllama_perf_context_print:  mean exit layer =      25.92\nllama_perf_context_print:       exit ratio =       0.58  = 116 / 200 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 27, 25, 16, 16, 32, 29, 32, 29, 30, 30, 17, 32, 28, 30, 32, 20, 29, 28, 22, 32, 24, 27, 32, 26, 24, 16, 32, 32, 28, 29, 32, 18, 32, 25, 32, 32, 32, 32, 32, 32, 32, 32, 16, 27, 20, 18, 32, 16, 32, 32, 25, 32, 32, 32, 32, 32, 30, 32, 17, 20, 32, 15, 32, 32, 32, 32, 16, 14, 32, 25, 25, 17, 32, 22, 29, 15, 32, 32, 20, 20, 27, 29, 30, 25, 25, 22, 30, 22, 16, 32, 32, 22, 32, 29, 20, 16, 17, 32, 27, 29, 32, 32, 17, 32, 18, 32, 22, 32, 22, 17, 15, 19, 17, 32, 29, 25, 30, 32, 32, 32, 28, 16, 32, 20, 26, 18, 32, 22, 22, 17, 30, 29, 17, 32, 20, 19, 32, 32, 17, 32, 22, 17, 32, 32, 19, 17, 32, 32, 32, 21, 32, 17, 32, 18, 29, 32, 32, 18, 18, 32, 16, 32, 14, 32, 32, 18, 18, 32, 18, 32, 27, 32, 22, 16, 16, 32, 16, 32, 14, 32, 18, 12, 32, 17, 32, 18, 32, 27, 29, 18, 17, 32, 15, 32, 32, 16, }\n"
    },
    {
        "inference_time": "56532.40",
        "tokens": "524",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: She has already taken the modelling world by storm, being snapped up to front campaigns for The Upside, Myer, Berlei and ASOS, to name a few. And now like Megan Gale, Jennifer Hawkins and Elle Macpherson before her, plus-size queen Laura Wells has turned her hand to designing. The brunette bombshell has launched a new swimwear line with a twist - the eco-friendly collection is made entirely of 100 per cent recycled material. Scroll down for video. Curves to coral: Model and environmental scientist Laura Wells uses figure to flaunt ocean inspired swimwear. Sizzling: The Australian beauty smoulders in the campaign. The environmental scientist turned model designed the line in collaboration with US brand Swimsuits For All's trend range, SwimSexy. The collection is inspired by the ocean, with coral reef scenes featuring throughout. The line includes one and two-piece suits, with zippers, criss-cross straps, plunging V-necklines and high-waisted bottoms. The swimsuits cater for every shape, with sizes r -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: She has already taken the modelling world by storm, being snapped up to front campaigns for The Upside, Myer, Berlei and ASOS, to name a few. And now like Megan Gale, Jennifer Hawkins and Elle Macpherson before her, plus-size queen Laura Wells has turned her hand to designing. The brunette bombshell has launched a new swimwear line with a twist - the eco-friendly collection is made entirely of 100 per cent recycled material. Scroll down for video. Curves to coral: Model and environmental scientist Laura Wells uses figure to flaunt ocean inspired swimwear. Sizzling: The Australian beauty smoulders in the campaign. The environmental scientist turned model designed the line in collaboration with US brand Swimsuits For All's trend range, SwimSexy. The collection is inspired by the ocean, with coral reef scenes featuring throughout. The line includes one and two-piece suits, with zippers, criss-cross straps, plunging V-necklines and high-waisted bottoms. The swimsuits cater for every shape, with sizes ranging from sizes 14 to size 28. Laura said she wanted to created a line that was both stylish and sustainable. she said: 'I wanted to create a swimsuit collection that not only makes women feel confident and sexy but also helps protect the environment. 'I believe that fashion can be both beautiful and sustainable, and I want to prove that sustainable fashion can be accessible to all. 'I am so thrilled to be partnering with Swimsuits for All to bring this collection to reality. 'Their commitment to sustainable and ethical practices align with my values and I can't wait to see how our collection will impact the industry. 'I am also excited to be working with a brand that shares my vision of empowerment and emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp emp\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.55 ms /   525 runs   (    0.01 ms per token, 94628.70 tokens per second)\nllama_perf_context_print:        load time =    5498.15 ms\nllama_perf_context_print: prompt eval time =    1634.80 ms /   269 tokens (    6.08 ms per token,   164.55 tokens per second)\nllama_perf_context_print:        eval time =   54847.25 ms /   255 runs   (  215.09 ms per token,     4.65 tokens per second)\nllama_perf_context_print:       total time =   56532.40 ms /   524 tokens\nllama_perf_context_print:  mean exit layer =      25.46\nllama_perf_context_print:       exit ratio =       0.73  = 188 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 18, 20, 32, 32, 30, 27, 27, 30, 32, 32, 30, 32, 32, 32, 32, 17, 18, 32, 25, 27, 32, 20, 32, 32, 27, 32, 20, 25, 30, 23, 32, 32, 30, 32, 32, 17, 18, 29, 29, 29, 25, 22, 18, 32, 20, 32, 32, 18, 32, 29, 27, 27, 20, 16, 32, 25, 32, 20, 30, 32, 32, 32, 28, 25, 25, 28, 27, 32, 27, 20, 32, 17, 30, 27, 32, 16, 32, 32, 18, 25, 20, 32, 22, 16, 22, 32, 32, 18, 20, 32, 29, 29, 32, 29, 32, 27, 27, 29, 32, 16, 32, 22, 24, 25, 18, 32, 29, 32, 18, 22, 27, 22, 32, 32, 32, 32, 32, 17, 22, 32, 20, 20, 32, 32, 32, 25, 22, 29, 16, 29, 31, 32, 32, 29, 32, 32, 25, 25, 20, 32, 29, 32, 17, 32, 32, 31, 32, 32, 29, 17, 32, 20, 32, 32, 25, 32, 25, 25, 32, 20, 17, 27, 32, 27, 15, 32, 17, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 27, 24, 27, 24, 22, 25, 23, 22, 22, 23, 22, 22, 22, 22, 22, 25, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 25, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 25, 22, 22, 22, 22, 22, 22, 25, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 27, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 27, 22, }\n"
    },
    {
        "inference_time": "59521.90",
        "tokens": "492",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Every second of the day you are subjected to radiation from compounds in soil, buildings and even your own body. But for people who fly frequently, cosmic radiation can increase this exposure significantly. Now there is an app that records your personal radiation to reveal how much you are exposed to from from medical examinations, changes in your location, and flights. The TrackYourDose app (pictured) was developed by Germany-based firm esooka and uses a mathematical model developed by scientists and meteorologists. The \u00a31.49 ($1.99) iOS app keeps a record of a user's personal radiation including exposure from changes in a person's location and flights. The TrackYourDose app was developed by Germany-based esooka. It uses a mathematical model developed by scientists and meteorologists\u00a0from the Physikalisch-Technische Bundesanstalt (PTB) in Germany and the University Oulu, Finland. Since 1997, PTB has studied the cosmic radiation at typical flight altitudes and used this to calculate radiation level -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Every second of the day you are subjected to radiation from compounds in soil, buildings and even your own body. But for people who fly frequently, cosmic radiation can increase this exposure significantly. Now there is an app that records your personal radiation to reveal how much you are exposed to from from medical examinations, changes in your location, and flights. The TrackYourDose app (pictured) was developed by Germany-based firm esooka and uses a mathematical model developed by scientists and meteorologists. The \u00a31.49 ($1.99) iOS app keeps a record of a user's personal radiation including exposure from changes in a person's location and flights. The TrackYourDose app was developed by Germany-based esooka. It uses a mathematical model developed by scientists and meteorologists\u00a0from the Physikalisch-Technische Bundesanstalt (PTB) in Germany and the University Oulu, Finland. Since 1997, PTB has studied the cosmic radiation at typical flight altitudes and used this to calculate radiation level exposure for passengers. The app also includes information about how to reduce exposure to radiation.\n\nPersonalized radiation expos exposure tracking app launched\n\nA new app called Track Your Dose has been launched to personalized radiation exposure tracking for users. The app, developed by Germany-based firm esooka, uses a mathematical model developed by scientists and meteorolographers to calculate radiation exposure based on a user's location, flights, and other factors such as medical examinations.\n\nThe app, which is available on iOS devices for \u00a31.49 ($1.99), uses a database of cosmic radiation levels at typical flight altitudes to calculate radiation exposure levels for users. The app also provides information on how to reduce exposure to radiation exposure.\n\nThe app was developed by esooka in collaboration with the Physical Technical Federal Institute (PTB) in Germany and the University of Oulu, Finland. PTB has been studying cosmic radiation levels at flight altitudes since 1997 and has developed a mathematical model to calculate radiation levels exposure for passengers.\n\n users can track their personalized radiation exposure levels over time and receive alerts when they\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.78 ms /   493 runs   (    0.01 ms per token, 85308.88 tokens per second)\nllama_perf_context_print:        load time =    5622.35 ms\nllama_perf_context_print: prompt eval time =    1656.19 ms /   237 tokens (    6.99 ms per token,   143.10 tokens per second)\nllama_perf_context_print:        eval time =   57813.09 ms /   255 runs   (  226.72 ms per token,     4.41 tokens per second)\nllama_perf_context_print:       total time =   59521.90 ms /   492 tokens\nllama_perf_context_print:  mean exit layer =      25.93\nllama_perf_context_print:       exit ratio =       0.63  = 161 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 20, 29, 32, 32, 31, 18, 25, 32, 20, 23, 17, 21, 32, 17, 25, 32, 32, 32, 32, 32, 32, 29, 18, 32, 16, 22, 20, 32, 32, 32, 32, 32, 32, 16, 32, 32, 25, 25, 32, 25, 32, 16, 32, 30, 22, 14, 32, 17, 32, 18, 32, 20, 29, 31, 32, 31, 20, 17, 32, 16, 16, 32, 32, 25, 20, 25, 25, 17, 32, 20, 20, 15, 18, 22, 16, 32, 22, 17, 32, 29, 32, 24, 25, 18, 32, 22, 30, 18, 32, 32, 17, 32, 32, 18, 29, 19, 16, 32, 17, 20, 32, 29, 31, 32, 25, 31, 30, 31, 32, 31, 18, 32, 18, 29, 18, 22, 29, 32, 27, 31, 18, 17, 32, 29, 31, 32, 17, 32, 32, 31, 32, 27, 29, 17, 17, 32, 29, 32, 25, 19, 32, 29, 21, 21, 19, 32, 25, 29, 20, 32, 31, 27, 17, 32, 18, 27, 15, 32, 29, 18, 20, 22, 32, 22, 16, 32, 32, 29, 31, 27, 29, 32, 16, 32, 25, 20, 32, 22, 18, 32, 32, 18, 32, 17, 32, 22, 15, 32, 25, 22, 29, 17, 32, 32, 16, 32, 20, 32, 17, 22, 22, 32, 32, 17, 32, 32, 32, 16, 32, 29, 17, 32, 18, 32, 18, 32, 25, 32, 17, 22, 27, 32, 32, 29, 24, 18, 32, 24, 24, 22, 22, 18, 32, 32, 22, 18, 29, 22, 32, 29, 32, 31, 16, 32, 17, 16, 32, 25, 16, 32, 32, 32, 32, 29, 20, }\n"
    },
    {
        "inference_time": "45502.35",
        "tokens": "477",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Blue Bell Ice Cream announced Friday that it has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. 'We are taking this step out of an abundance of caution to ensure that we are doing everything possible to provide our consumers with safe products and to preserve the trust we have built with them and their families for more than a century,' the company said in a statement. Last month, the company and health officials said a 3-ounce cup of ice cream contaminated with listeriosis was traced to a plant in Broken Arrow, Oklahoma. Scroll down for video. Blue Bell Ice Cream has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. Ten products recalled earlier in March were from a production line at a plant the company's headquarters in Brenham, Texas. The now-recalled ice cream product - cups of ch -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Blue Bell Ice Cream announced Friday that it has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. 'We are taking this step out of an abundance of caution to ensure that we are doing everything possible to provide our consumers with safe products and to preserve the trust we have built with them and their families for more than a century,' the company said in a statement. Last month, the company and health officials said a 3-ounce cup of ice cream contaminated with listeriosis was traced to a plant in Broken Arrow, Oklahoma. Scroll down for video. Blue Bell Ice Cream has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. Ten products recalled earlier in March were from a production line at a plant the company's headquarters in Brenham, Texas. The now-recalled ice cream product - cups of chocolate ice cream with chocolateolate chips and caramel - was traced to a plant in Broken Arrow, Oklahoma. Blue Bell Ice cream suspended operations at an Oklahoma production facility that officials linked to a foodborne illness linked to the deaths of three people last month. Blue Bell Ice cream has been linked to a deaths of three people in Oklahoma. Blue Bell Ice cream has susp suspended operations at an Oklahoma production facility that officials linked to a foodborne illusion linked to the deaths of three people last week. Blue Bell Ice cream has been linked to the deaths of three people in Oklahoma. Bluebell ice cream has susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp susp\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.03 ms /   478 runs   (    0.01 ms per token, 79244.03 tokens per second)\nllama_perf_context_print:        load time =    5553.00 ms\nllama_perf_context_print: prompt eval time =    1636.75 ms /   222 tokens (    7.37 ms per token,   135.63 tokens per second)\nllama_perf_context_print:        eval time =   43811.99 ms /   255 runs   (  171.81 ms per token,     5.82 tokens per second)\nllama_perf_context_print:       total time =   45502.35 ms /   477 tokens\nllama_perf_context_print:  mean exit layer =      23.39\nllama_perf_context_print:       exit ratio =       0.80  = 206 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 22, 18, 32, 32, 30, 25, 16, 20, 27, 32, 32, 32, 32, 32, 32, 25, 29, 16, 32, 29, 32, 17, 32, 18, 25, 27, 16, 32, 20, 25, 27, 25, 22, 29, 23, 27, 32, 17, 32, 20, 32, 22, 32, 32, 24, 32, 17, 22, 17, 32, 22, 20, 24, 17, 32, 15, 32, 15, 32, 18, 27, 32, 22, 27, 27, 25, 23, 27, 18, 17, 32, 16, 32, 17, 14, 32, 15, 32, 32, 32, 32, 25, 27, 20, 23, 29, 17, 32, 20, 32, 32, 17, 32, 20, 32, 22, 20, 32, 25, 24, 16, 32, 17, 32, 22, 15, 24, 18, 32, 17, 15, 32, 18, 17, 32, 17, 32, 25, 27, 20, 23, 25, 17, 15, 32, 13, 32, 15, 13, 32, 15, 18, 16, 32, 17, 32, 25, 20, 19, 27, 18, 32, 17, 19, 21, 21, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, }\n"
    },
    {
        "inference_time": "66381.20",
        "tokens": "519",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Mad Men star Jon Hamm's secret college bullying shame has been exposed by court documents unearthed from the actor's time at the University of Texas at Austin. According to shocking arrest warrants and summons seen by Star magazine, the 44-year-old Golden Globe winner allegedly set fire to one pledge after viciously beating him and using a claw hammer to drag him by his genitals during a hazing ritual gone wrong. The documents claim that Hamm, 44, was eventually charged with assault in connection with the sadistic November 1990 initiation, but he managed to get the charges dismissed before he began his ascent to superstardom. Scroll down for video. College student: Jon Hamm is pictured here in the University of Texas at Austin yearbook for 1990 (left) - the same year he was allegedly involved in a hazing scandal at the Sigma Nu fraternity. The actor is pictured (right) this week. According to the explosive court documents, Hamm was in his sophomore year at UT-Austin when a prospective 21-year-old p -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Mad Men star Jon Hamm's secret college bullying shame has been exposed by court documents unearthed from the actor's time at the University of Texas at Austin. According to shocking arrest warrants and summons seen by Star magazine, the 44-year-old Golden Globe winner allegedly set fire to one pledge after viciously beating him and using a claw hammer to drag him by his genitals during a hazing ritual gone wrong. The documents claim that Hamm, 44, was eventually charged with assault in connection with the sadistic November 1990 initiation, but he managed to get the charges dismissed before he began his ascent to superstardom. Scroll down for video. College student: Jon Hamm is pictured here in the University of Texas at Austin yearbook for 1990 (left) - the same year he was allegedly involved in a hazing scandal at the Sigma Nu fraternity. The actor is pictured (right) this week. According to the explosive court documents, Hamm was in his sophomore year at UT-Austin when a prospective 21-year-old pledge was subjected to a 'ritual' that ended in the young man's brutal beating and near-death experience. Hamm, who was then 19, allegedly joined in the attack along with other members of the fraternity's Sigma Nu fraternity chapter, which was allegedly known for its violent hazing rituals. Hamm's allegedly involvement in the hazing ritual was revealed in court documents unearthed by Star magazine. Hamm, who was not immediately available for comment, has been accused of setting fire to one of the pledges after viciously beating him and using a claw hammer to drag him by his genitals during a hazing ritual gone wrong. Hamm, who was 19 at the time, was charged with assault in connection with the incident, but sources close to the actor claim that he managed to get the charges dismissed before he began his ascent to superstar status. Hamm, who is best known for his role as Don Draper on the popular AMC series Mad Men, has been nominated for numerous awards for his portrayal of the character, including four Golden Globe Awards for Best Actor in Television Series Drama\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.62 ms /   520 runs   (    0.01 ms per token, 92477.33 tokens per second)\nllama_perf_context_print:        load time =    5576.66 ms\nllama_perf_context_print: prompt eval time =    1649.70 ms /   264 tokens (    6.25 ms per token,   160.03 tokens per second)\nllama_perf_context_print:        eval time =   64678.98 ms /   255 runs   (  253.64 ms per token,     3.94 tokens per second)\nllama_perf_context_print:       total time =   66381.20 ms /   519 tokens\nllama_perf_context_print:  mean exit layer =      27.15\nllama_perf_context_print:       exit ratio =       0.52  = 134 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 22, 29, 31, 32, 32, 31, 32, 32, 29, 32, 32, 29, 27, 32, 32, 27, 32, 32, 18, 32, 32, 30, 27, 32, 28, 26, 32, 22, 30, 32, 32, 29, 29, 32, 32, 17, 32, 32, 32, 32, 32, 27, 16, 20, 20, 29, 18, 16, 32, 29, 30, 32, 25, 32, 18, 32, 32, 20, 32, 32, 32, 17, 32, 32, 27, 18, 32, 32, 27, 16, 32, 30, 28, 32, 28, 25, 25, 23, 32, 25, 25, 32, 25, 18, 32, 32, 32, 29, 25, 17, 32, 32, 29, 15, 32, 22, 13, 32, 25, 32, 13, 32, 32, 32, 32, 27, 16, 25, 32, 32, 32, 22, 32, 18, 22, 17, 24, 27, 27, 29, 32, 18, 32, 27, 20, 32, 18, 32, 30, 32, 16, 32, 27, 22, 32, 17, 32, 16, 18, 32, 20, 32, 25, 22, 17, 32, 24, 17, 32, 32, 32, 22, 30, 32, 18, 32, 32, 27, 18, 32, 29, 18, 32, 20, 32, 22, 22, 32, 18, 18, 20, 32, 32, 16, 20, 32, 22, 22, 32, 32, 18, 17, 32, 27, 32, 27, 18, 22, 32, 20, 32, 24, 18, 32, 22, 32, 24, 25, 32, 32, 24, 32, 18, 32, 32, 32, 20, 22, 16, 27, 32, 32, 32, 29, 32, 17, 32, 32, 32, 32, 25, 32, 20, 16, 32, 25, 32, 20, 32, 16, 32, 32, 32, 22, 24, 20, 32, 18, 32, 22, 32, 29, 32, 30, 32, 32, 32, 28, 27, 32, 32, 32, }\n"
    },
    {
        "inference_time": "45040.75",
        "tokens": "404",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Philip Hammond pledged to protect the Falklands from \u2018Argentinian bullying\u2019 last night as the country began legal action against British companies exploring for oil off the islands. It follows a provocative speech by Argentinian president Cristina Fernandez de Kirchner, prompting the Foreign Office to summon her ambassador for a dressing down. Buenos Aires responded by accusing three British oil firms of working without permission. The Argentine government said it was taking legal action against three British and two US companies for \u2018exploration activities for fossil fuels on the Argentine continental shelf without obtaining the corresponding authorisation\u2019. The legal complaint listed Rockhopper Exploration, Premier Oil, Falkland Oil and Gas Limited and US firms Noble Energy and Edison International \u2013 accusing them of sending a semi-submersible rig to explore a basin 120 miles north of the islands \u2018subjected to the illegitimate British occupation\u2019. Argentina's President Cristina Fernandez de Kirch -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Philip Hammond pledged to protect the Falklands from \u2018Argentinian bullying\u2019 last night as the country began legal action against British companies exploring for oil off the islands. It follows a provocative speech by Argentinian president Cristina Fernandez de Kirchner, prompting the Foreign Office to summon her ambassador for a dressing down. Buenos Aires responded by accusing three British oil firms of working without permission. The Argentine government said it was taking legal action against three British and two US companies for \u2018exploration activities for fossil fuels on the Argentine continental shelf without obtaining the corresponding authorisation\u2019. The legal complaint listed Rockhopper Exploration, Premier Oil, Falkland Oil and Gas Limited and US firms Noble Energy and Edison International \u2013 accusing them of sending a semi-submersible rig to explore a basin 120 miles north of the islands \u2018subjected to the illegitimate British occupation\u2019. Argentina's President Cristina Fernandez de Kirchner has accused British companies of 'stealing' oil resources off the country's coast. The Foreign Office has summoned Argentine ambassador A Argentine government spokesman said the country would not back down on its claim to the Falklands, which it calls the Malvinas. The Falk Islands are a British territory in the South Atlantic Ocean, which Argentina has claimed since the 1960s. The Falk Islands government has condemned the legal action against the companies, saying they have the right legally to explore for oil in the area. The Falk Islands are a British territory in the South Atlantic Ocean, which Argentina has claimed since the 1960s. The Falk Islands government has condemned the legal action against the companies, saying they have the right legally to explore for oil in the area. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.01 ms /   405 runs   (    0.01 ms per token, 100896.86 tokens per second)\nllama_perf_context_print:        load time =    5478.04 ms\nllama_perf_context_print: prompt eval time =    1598.10 ms /   229 tokens (    6.98 ms per token,   143.29 tokens per second)\nllama_perf_context_print:        eval time =   43405.18 ms /   175 runs   (  248.03 ms per token,     4.03 tokens per second)\nllama_perf_context_print:       total time =   45040.75 ms /   404 tokens\nllama_perf_context_print:  mean exit layer =      27.01\nllama_perf_context_print:       exit ratio =       0.54  = 95 / 177 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 20, 32, 32, 32, 32, 32, 22, 27, 27, 32, 29, 16, 32, 27, 32, 32, 32, 32, 32, 32, 25, 32, 25, 27, 32, 16, 32, 23, 27, 32, 32, 32, 32, 32, 32, 22, 29, 30, 32, 22, 25, 24, 20, 32, 30, 32, 24, 32, 32, 27, 32, 32, 32, 29, 32, 32, 27, 32, 32, 22, 18, 32, 32, 20, 22, 32, 30, 22, 22, 28, 32, 32, 32, 32, 26, 32, 32, 32, 32, 30, 32, 32, 16, 28, 32, 30, 25, 14, 32, 29, 32, 32, 31, 32, 20, 16, 32, 16, 32, 30, 32, 32, 32, 18, 25, 32, 27, 30, 32, 20, 32, 30, 32, 28, 32, 30, 25, 18, 32, 32, 18, 20, 32, 17, 20, 19, 19, 18, 32, 20, 32, 27, 16, 18, 32, 25, 30, 27, 29, 16, 32, 32, 27, 22, 16, 32, 18, 32, 25, 17, 32, 18, 32, 22, 20, 17, 20, 17, 32, 17, 32, 18, 19, 21, 32, 20, 18, 17, 29, 17, 18, 18, }\n"
    },
    {
        "inference_time": "40270.93",
        "tokens": "504",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Passengers experienced some anxious moments when their plane landed at a New York City airport and was met by firefighters wearing full protective suits. Emergency workers surrounded the Delta Air Lines plane at LaGuardia Airport after an odor of smoke was reported by the crew moments after Flight 2522 touched down. Passengers were still on board as firefighters walked through the cabin with a thermal imaging camera to locate a potential heat source. After the plane landed a firefighter used a thermal imaging camera to locate a potential heat source. Passenger Pamela Sullins snapped photos showing a firefighter in the aisle of the McDonnell Douglas MD-888 twin-engine plane, and others gathered on the tarmac after the commercial jet flew in from Tampa, Florida, yesterday. She told MailOnline Travel: \u2018I noticed an odor that seemed like electrical wires burning after we landed. I did not see any smoke.' Pamela, who was travelling to New York for business meetings, said the Delta crew provided updates  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Passengers experienced some anxious moments when their plane landed at a New York City airport and was met by firefighters wearing full protective suits. Emergency workers surrounded the Delta Air Lines plane at LaGuardia Airport after an odor of smoke was reported by the crew moments after Flight 2522 touched down. Passengers were still on board as firefighters walked through the cabin with a thermal imaging camera to locate a potential heat source. After the plane landed a firefighter used a thermal imaging camera to locate a potential heat source. Passenger Pamela Sullins snapped photos showing a firefighter in the aisle of the McDonnell Douglas MD-888 twin-engine plane, and others gathered on the tarmac after the commercial jet flew in from Tampa, Florida, yesterday. She told MailOnline Travel: \u2018I noticed an odor that seemed like electrical wires burning after we landed. I did not see any smoke.' Pamela, who was travelling to New York for business meetings, said the Delta crew provided updates  via the PA system, but they were not very informative. She said: 'They just said something about sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm sm\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.16 ms /   505 runs   (    0.01 ms per token, 81993.83 tokens per second)\nllama_perf_context_print:        load time =    5505.89 ms\nllama_perf_context_print: prompt eval time =    1609.30 ms /   249 tokens (    6.46 ms per token,   154.73 tokens per second)\nllama_perf_context_print:        eval time =   38612.04 ms /   255 runs   (  151.42 ms per token,     6.60 tokens per second)\nllama_perf_context_print:       total time =   40270.93 ms /   504 tokens\nllama_perf_context_print:  mean exit layer =      22.52\nllama_perf_context_print:       exit ratio =       0.93  = 240 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 27, 32, 32, 20, 30, 29, 30, 31, 32, 16, 31, 32, 22, 25, 22, 32, 32, 30, 32, 32, 27, 27, 17, 27, 27, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 17, 22, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 22, 22, 17, 32, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 17, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 17, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 17, 27, 22, 22, 22, 22, 22, 22, 22, 22, 17, 22, 22, 22, 22, 22, 22, 17, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 25, 25, 25, 23, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, }\n"
    },
    {
        "inference_time": "42145.66",
        "tokens": "527",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: It appears that David Luiz has taken the phrase 'twiddling my hair' literally judging by the injured Paris Saint-Germain star's latest Instagram post. The PSG defender is out for at least four weeks after scans revealed that the 27-year-old suffered a torn hamstring in Sunday's 3-2 win at Ligue 1 title rivals Marseille. And it seems that boredom has already kicked into the usually jovial Brazilian as he sported a new hairdo on Wednesday. David Luiz posted an Instagram picture sporting a new hairdo in the style of a man bun on Wednesday. Luiz (centre) has time on his hands at present after tearing a hamstring at Marseille on Sunday night. The Paris Saint-Germain defender immediately signalled to the bench that he would need to be replaced. Accompanied with the caption: 'Parisien!' Luiz has his usually flowing brown locks styled into a man bun. The centre back's new look seems to be a source of amusement for his PSG team-mate Lucas Moura who later shared a picture with Luiz among his Instagram follow -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: It appears that David Luiz has taken the phrase 'twiddling my hair' literally judging by the injured Paris Saint-Germain star's latest Instagram post. The PSG defender is out for at least four weeks after scans revealed that the 27-year-old suffered a torn hamstring in Sunday's 3-2 win at Ligue 1 title rivals Marseille. And it seems that boredom has already kicked into the usually jovial Brazilian as he sported a new hairdo on Wednesday. David Luiz posted an Instagram picture sporting a new hairdo in the style of a man bun on Wednesday. Luiz (centre) has time on his hands at present after tearing a hamstring at Marseille on Sunday night. The Paris Saint-Germain defender immediately signalled to the bench that he would need to be replaced. Accompanied with the caption: 'Parisien!' Luiz has his usually flowing brown locks styled into a man bun. The centre back's new look seems to be a source of amusement for his PSG team-mate Lucas Moura who later shared a picture with Luiz among his Instagram followers with caption: \"David Luiz's new hair style \ud83d\ude02\ud83d\udc4d\".\n\nAnalysis:\n\nDavid Luiz's recent Instagram post has sparked amusement among his fellow Paris Saint- Germain teammates, including Lucas Moura, after he sporting a new hairstyle. The Brazilian defender sported a new look of a man bun, complete with hair sty hair hair t hair t hair t hair hair hair t hair trapped hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair hair\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.72 ms /   528 runs   (    0.01 ms per token, 92372.29 tokens per second)\nllama_perf_context_print:        load time =    5547.85 ms\nllama_perf_context_print: prompt eval time =    1651.23 ms /   272 tokens (    6.07 ms per token,   164.73 tokens per second)\nllama_perf_context_print:        eval time =   40440.80 ms /   255 runs   (  158.59 ms per token,     6.31 tokens per second)\nllama_perf_context_print:       total time =   42145.66 ms /   527 tokens\nllama_perf_context_print:  mean exit layer =      22.93\nllama_perf_context_print:       exit ratio =       0.70  = 181 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 16, 27, 32, 20, 32, 25, 32, 32, 18, 20, 22, 22, 32, 27, 32, 17, 27, 32, 18, 32, 32, 29, 32, 32, 32, 18, 25, 32, 32, 32, 22, 25, 29, 16, 32, 20, 32, 17, 32, 32, 17, 32, 20, 27, 19, 20, 32, 32, 30, 25, 30, 32, 32, 32, 32, 32, 32, 32, 25, 32, 27, 32, 20, 16, 17, 15, 32, 27, 32, 32, 32, 32, 16, 32, 25, 32, 27, 18, 16, 29, 32, 27, 27, 25, 29, 31, 16, 32, 18, 32, 17, 16, 32, 17, 32, 17, 32, 25, 23, 17, 32, 21, 15, 32, 17, 17, 17, 17, 32, 18, 22, 19, 18, 32, 17, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 18, 22, 18, 18, 18, 22, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 17, 17, 18, 17, 18, 17, 20, 17, 20, 17, 22, 17, 32, 20, 18, 32, 20, 17, 25, 20, 17, 32, 20, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 32, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 17, 17, 18, 17, 18, 17, 17, 17, 17, 17, 17, 18, 17, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 17, 18, }\n"
    },
    {
        "inference_time": "62951.02",
        "tokens": "498",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: US President Barack Obama said on Saturday that diplomacy was the best option to deal with Iran's contested nuclear program, two days after the conclusion of a framework agreement with Tehran. Iran and six world powers determined the outlines of a landmark agreement which would curb Iran's nuclear program and potentially lift economic sanctions. As Obama gears up to sell US skeptics on the deal, he said he is convinced talks are the best way forward. Scroll down for video. US President Barack Obama says diplomacy is the best way to deal with Iran. U.S. Secretary of State John Kerry and staff watch a tablet in Lausanne as U.S. President Barack Obama makes a state address on the status of the Iran nuclear program. 'As President and Commander in Chief, I firmly believe that the diplomatic option - a comprehensive, long-term deal like this - is by far the best option,' Obama said in his weekly address. Explaining that he expects a 'robust debate' on the deal in the United States, Obama said he will kee -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: US President Barack Obama said on Saturday that diplomacy was the best option to deal with Iran's contested nuclear program, two days after the conclusion of a framework agreement with Tehran. Iran and six world powers determined the outlines of a landmark agreement which would curb Iran's nuclear program and potentially lift economic sanctions. As Obama gears up to sell US skeptics on the deal, he said he is convinced talks are the best way forward. Scroll down for video. US President Barack Obama says diplomacy is the best way to deal with Iran. U.S. Secretary of State John Kerry and staff watch a tablet in Lausanne as U.S. President Barack Obama makes a state address on the status of the Iran nuclear program. 'As President and Commander in Chief, I firmly believe that the diplomatic option - a comprehensive, long-term deal like this - is by far the best option,' Obama said in his weekly address. Explaining that he expects a 'robust debate' on the deal in the United States, Obama said he will keeo working hard to convince Americans skeptical of the deal that it is in their best interests. 'Iran's nuclear program has been a source of concern for many years, but I believe this deal will help us address those concerns while also addressing the broader challenges we face together,' Obama said. Obama said the deal would cut off Iran's ability to build a nuclear weapon, nuclear weapons program, and Iran's ball nuclear program. ' Iran's nuclear program has been a source of concern for many years, but I believe this deal will help us address those concerns while also addressing the broader challenges we face together,' Ob Obama said. Obama said the deal would cut Iran's ability to build a nuclear weapon by Iran's ability to build a nuclear weapons program, and Iran\u2019s ability to develop nuclear weapons. 'This deal will cut Iran's ability to produce weapons- grade nuclear material for Iran's nuclear program by two-thirds, and Iran will be required to reduce its stockpile by  Iran will be able to use its own nuclear material for peace purposes, such as power generation and medical research,' he said. Obama also said that Iran will also be able to\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.85 ms /   499 runs   (    0.01 ms per token, 85299.15 tokens per second)\nllama_perf_context_print:        load time =    5456.16 ms\nllama_perf_context_print: prompt eval time =    1597.93 ms /   243 tokens (    6.58 ms per token,   152.07 tokens per second)\nllama_perf_context_print:        eval time =   61301.28 ms /   255 runs   (  240.40 ms per token,     4.16 tokens per second)\nllama_perf_context_print:       total time =   62951.02 ms /   498 tokens\nllama_perf_context_print:  mean exit layer =      26.74\nllama_perf_context_print:       exit ratio =       0.54  = 140 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 22, 25, 16, 27, 32, 23, 30, 28, 22, 29, 22, 32, 18, 32, 30, 29, 28, 17, 32, 30, 32, 32, 32, 32, 20, 32, 32, 32, 29, 25, 22, 16, 32, 32, 16, 32, 17, 30, 22, 20, 32, 32, 32, 29, 32, 32, 17, 32, 32, 30, 32, 32, 27, 32, 32, 22, 32, 22, 18, 32, 32, 27, 22, 22, 25, 18, 32, 32, 23, 32, 32, 32, 18, 32, 32, 16, 29, 32, 32, 20, 32, 31, 20, 32, 32, 32, 29, 25, 30, 23, 32, 20, 22, 32, 32, 29, 29, 29, 32, 22, 32, 16, 32, 20, 20, 16, 17, 32, 15, 32, 17, 32, 16, 32, 22, 32, 18, 32, 22, 32, 20, 32, 32, 22, 16, 32, 20, 32, 32, 16, 32, 25, 18, 32, 23, 25, 18, 21, 17, 32, 18, 32, 32, 22, 27, 24, 25, 17, 32, 24, 16, 32, 17, 32, 32, 25, 25, 17, 30, 25, 18, 32, 16, 32, 29, 32, 29, 16, 32, 14, 32, 16, 17, 32, 32, 30, 32, 30, 32, 25, 30, 32, 22, 27, 28, 27, 17, 32, 32, 32, 30, 19, 32, 29, 18, 32, 21, 32, 22, 23, 32, 32, 22, 32, 32, 32, 27, 32, 30, 32, 20, 32, 32, 32, 32, 32, 25, 32, 18, 32, 32, 16, 17, 32, 32, 14, 32, 32, 30, 25, 16, 32, 20, 20, 32, 22, 32, 32, 32, 29, 30, 32, 17, 32, 25, 17, 32, 32, 25, 20, 17, 32, 15, 32, }\n"
    },
    {
        "inference_time": "30828.63",
        "tokens": "438",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A 12-year-old girl battling Leukemia for two years has been kicked out of school for her lack of attendance. 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully. Last week St. Joseph's Middle School, a private Catholic School, sent a letter to Rose McGrath and her dismissing her from the school for low attendance and poor academic performance. Scroll down for video. Heartbroken: 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully of her school kicking her out for poor attendance because of her Leukemia. Dismissed: John Fleckenstein, with Battle Creek Area Catholic Schools, claims that the school made many accommodations for Rose and that none of them seemed to help her enough. Rose's mother Barbara McGrath was just as heartbroken to hear the news and said even though her daughter is no longer getting cancer treatment, that her recovery will take some time. 'Even though she's n -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A 12-year-old girl battling Leukemia for two years has been kicked out of school for her lack of attendance. 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully. Last week St. Joseph's Middle School, a private Catholic School, sent a letter to Rose McGrath and her dismissing her from the school for low attendance and poor academic performance. Scroll down for video. Heartbroken: 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully of her school kicking her out for poor attendance because of her Leukemia. Dismissed: John Fleckenstein, with Battle Creek Area Catholic Schools, claims that the school made many accommodations for Rose and that none of them seemed to help her enough. Rose's mother Barbara McGrath was just as heartbroken to hear the news and said even though her daughter is no longer getting cancer treatment, that her recovery will take some time. 'Even though she's nolonger getting chemo, she still has to go to appointments every three months until until she's in remission, which could take months or years,' she said. Rose's story has been shared by the local news station WZZM 13, and the community has been rallying around the family to help them through this difficult situation.\nRose McGrath is a 12-yearold girl whose been batting against Leukemia for two years. She was recently kicked out of her school, St. Joseph's Catholic school due due to poor attendance due due to her Leukemia. Rose's mother Barbara McGrath said that even though she is no longer getting cancer treatment, her still has to go to appointments every month until she's in remission. The community has been rally around the family to help them through this difficult situation. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.15 ms /   439 runs   (    0.01 ms per token, 105681.27 tokens per second)\nllama_perf_context_print:        load time =    5100.08 ms\nllama_perf_context_print: prompt eval time =    1092.70 ms /   249 tokens (    4.39 ms per token,   227.88 tokens per second)\nllama_perf_context_print:        eval time =   29694.77 ms /   189 runs   (  157.12 ms per token,     6.36 tokens per second)\nllama_perf_context_print:       total time =   30828.63 ms /   438 tokens\nllama_perf_context_print:  mean exit layer =      26.25\nllama_perf_context_print:       exit ratio =       0.54  = 103 / 191 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 29, 16, 32, 31, 32, 31, 25, 32, 23, 32, 22, 27, 17, 32, 29, 29, 32, 32, 32, 32, 25, 32, 25, 16, 32, 22, 32, 18, 32, 32, 25, 16, 32, 32, 32, 16, 32, 16, 32, 18, 32, 32, 32, 32, 32, 32, 32, 30, 30, 32, 32, 32, 28, 32, 16, 32, 18, 32, 27, 32, 17, 32, 15, 32, 27, 32, 20, 15, 32, 32, 32, 32, 32, 22, 18, 27, 29, 29, 27, 22, 29, 22, 18, 18, 16, 32, 32, 16, 18, 32, 22, 16, 32, 20, 18, 32, 30, 32, 32, 27, 22, 24, 16, 32, 17, 32, 24, 32, 18, 32, 32, 17, 27, 29, 17, 32, 25, 18, 32, 22, 17, 24, 29, 29, 22, 15, 32, 24, 16, 32, 20, 32, 32, 22, 24, 32, 22, 18, 32, 18, 32, 20, 18, 32, 22, 22, 32, 20, 16, 32, 16, 32, 32, 18, 32, 18, 22, 32, 32, 29, 29, 20, 32, 16, 32, 32, 16, 32, 14, 32, 22, 22, 32, 17, 32, 15, 32, 18, 15, 32, 15, 32, 18, }\n"
    },
    {
        "inference_time": "30035.37",
        "tokens": "417",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Breathing in second-hand cigarette smoke has been shown to increase the risk of cancer in humans. Now, researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke. The finding may explain why some spices, herbal teas and medicinal plants have high concentrations of nicotine in them, despite none being allowed in insecticides. Researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke (illustrated with a stock image) Nicotine was frequently used as an insecticide until it was banned by the European Union in 2009 because of its toxicity. But a surprisingly large number of food crops and plant-derived products still contain very high levels of nicotine. Dirk Selmar and his colleagues at the Technical University of Braunschweig, Germany, set out to uncover whether there are reasons for this beyond the use of illegal insecticides. They used peppermint plant -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Breathing in second-hand cigarette smoke has been shown to increase the risk of cancer in humans. Now, researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke. The finding may explain why some spices, herbal teas and medicinal plants have high concentrations of nicotine in them, despite none being allowed in insecticides. Researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke (illustrated with a stock image) Nicotine was frequently used as an insecticide until it was banned by the European Union in 2009 because of its toxicity. But a surprisingly large number of food crops and plant-derived products still contain very high levels of nicotine. Dirk Selmar and his colleagues at the Technical University of Braunschweig, Germany, set out to uncover whether there are reasons for this beyond the use of illegal insecticides. They used peppermint plantlets to investigate how passive smoke exposure affects plants' nicotine content. The plants were placed in a smoke chamber and exposed to smoke from a cigarette smoldered for 2 hours. The results showed that the nicot content in the pepperm mint plants increased by up to 100 times after exposure to second smoke smoke. The researchers suggest that this could be due to the smoke's chemical composition, which includes nicotine and other volatile compounds that can be easily absorbed by plants. The study suggests that passive smoke exposure may be a significant factor in the high levels of nicotine found in certain food crops and plant-derived products, and could be a potential source of nicotine contaminations in the future if smoking habits continue to increase. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.76 ms /   418 runs   (    0.01 ms per token, 111258.98 tokens per second)\nllama_perf_context_print:        load time =    4489.32 ms\nllama_perf_context_print: prompt eval time =    1119.47 ms /   245 tokens (    4.57 ms per token,   218.85 tokens per second)\nllama_perf_context_print:        eval time =   28880.89 ms /   172 runs   (  167.91 ms per token,     5.96 tokens per second)\nllama_perf_context_print:       total time =   30035.37 ms /   417 tokens\nllama_perf_context_print:  mean exit layer =      26.88\nllama_perf_context_print:       exit ratio =       0.58  = 101 / 174 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 30, 16, 22, 32, 32, 17, 22, 20, 29, 32, 25, 23, 22, 31, 32, 24, 20, 32, 30, 30, 26, 22, 32, 24, 25, 18, 32, 32, 18, 32, 32, 27, 27, 32, 30, 32, 32, 25, 32, 32, 18, 29, 22, 31, 32, 24, 22, 30, 22, 32, 24, 18, 26, 26, 32, 32, 26, 26, 29, 32, 32, 31, 32, 29, 32, 27, 22, 32, 22, 22, 32, 32, 32, 16, 32, 17, 20, 18, 15, 32, 22, 32, 22, 32, 32, 32, 32, 29, 16, 32, 32, 25, 25, 32, 16, 32, 32, 22, 32, 24, 29, 31, 32, 25, 32, 23, 18, 32, 32, 18, 32, 17, 32, 16, 22, 32, 32, 17, 32, 27, 32, 20, 29, 31, 32, 17, 17, 32, 22, 32, 32, 20, 17, 32, 29, 25, 17, 32, 17, 32, 19, 32, 30, 32, 25, 21, 27, 32, 16, 32, 32, 25, 20, 32, 27, 22, 27, 30, 20, 20, 32, 28, 18, 32, 17, 32, 32, 32, 29, }\n"
    },
    {
        "inference_time": "20018.26",
        "tokens": "368",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Chameleons are known for their ability to change colours and blend in, but these two had no trouble spotting each other\u2019s ambitions in a fierce battle for a female mate. Footage of the brief contest was captured on a property near the Amber Mountain National Park on the northern edge of Madagascar. It shows the males sizing each other up before one is forced to retreat following a violent battle for the attention of a female chameleon that isn\u2019t seen in the video. The more aggressive of the two (right) chases his rival out of the tree and along a concrete wall. One of the chameleons twists in pain as the aggressor takes a huge bite out of him during their clash. The one-minute clip begins with the video maker, who speaks French, one of the island\u2019s two official languages, offering play-by-play commentary, declaring it \u2018bataille du chameleons\u2019, or battle of the chameleons. Shortly after he questions which one will make the first move, the more aggressive of the two reptiles launches himself at his f -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Chameleons are known for their ability to change colours and blend in, but these two had no trouble spotting each other\u2019s ambitions in a fierce battle for a female mate. Footage of the brief contest was captured on a property near the Amber Mountain National Park on the northern edge of Madagascar. It shows the males sizing each other up before one is forced to retreat following a violent battle for the attention of a female chameleon that isn\u2019t seen in the video. The more aggressive of the two (right) chases his rival out of the tree and along a concrete wall. One of the chameleons twists in pain as the aggressor takes a huge bite out of him during their clash. The one-minute clip begins with the video maker, who speaks French, one of the island\u2019s two official languages, offering play-by-play commentary, declaring it \u2018bataille du chameleons\u2019, or battle of the chameleons. Shortly after he questions which one will make the first move, the more aggressive of the two reptiles launches himself at his feline foe. The two chameleons tumble to the ground floor of the tree tree, where they continue to fight until one is forced to retreat. The video ends with the winning chameleon standing victorious over his opponent, who is seen limping away.\n\nQuestion: What is the main purpose of this article?\n\nAnswer answer: The main purpose of this article is to inform readers about a recent video that has been shared online showing two male chameleons fighting over a female mate in Madagascar. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       2.50 ms /   369 runs   (    0.01 ms per token, 147305.39 tokens per second)\nllama_perf_context_print:        load time =    4460.83 ms\nllama_perf_context_print: prompt eval time =    1102.12 ms /   255 tokens (    4.32 ms per token,   231.37 tokens per second)\nllama_perf_context_print:        eval time =   18891.03 ms /   113 runs   (  167.18 ms per token,     5.98 tokens per second)\nllama_perf_context_print:       total time =   20018.26 ms /   368 tokens\nllama_perf_context_print:  mean exit layer =      26.88\nllama_perf_context_print:       exit ratio =       0.54  = 62 / 115 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 20, 25, 23, 25, 32, 32, 32, 32, 27, 17, 16, 32, 30, 22, 18, 32, 32, 20, 20, 32, 17, 17, 20, 30, 32, 32, 18, 32, 30, 32, 22, 32, 20, 25, 22, 23, 22, 27, 27, 32, 32, 24, 20, 32, 18, 32, 32, 30, 32, 32, 25, 32, 22, 32, 32, 32, 32, 32, 22, 32, 29, 32, 25, 23, 20, 32, 30, 27, 32, 32, 16, 29, 32, 22, 16, 32, 14, 32, 20, 14, 32, 32, 20, 29, 32, 32, 31, 32, 27, 32, 32, 25, 32, 20, 22, 22, 25, 32, 32, 32, 18, 18, 22, 32, 32, 17, 22, 29, 19, }\n"
    },
    {
        "inference_time": "10624.99",
        "tokens": "328",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season. The 20-year-old Worcester Warriors prop, capped nine times for the Dark Blues Under 20s side, has penned a two-year deal with Alan Solomons' side which ties him to the Murrayfield outfit until May 2017. He said: 'I'm over the moon to be coming north. With the club being in a European Cup Final, they're really showing they've got quality players and everyone is speaking very highly of Edinburgh. Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season on a two-year deal. The 20-year-old Worcester Warriors prop has been capped nine times for the Dark Blues Under 20s side. 'I know quite a few of the younger lads - Damien Hoyland, Magnus Bradbury, Jamie Farndale and Jamie Ritchie - from playing with Scotland under 20 and they've all spoken about how much they enjoy being with the club. 'I think I've got a strong set-piece game and I'm looking to add something extra to the squad. I'm under no  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season. The 20-year-old Worcester Warriors prop, capped nine times for the Dark Blues Under 20s side, has penned a two-year deal with Alan Solomons' side which ties him to the Murrayfield outfit until May 2017. He said: 'I'm over the moon to be coming north. With the club being in a European Cup Final, they're really showing they've got quality players and everyone is speaking very highly of Edinburgh. Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season on a two-year deal. The 20-year-old Worcester Warriors prop has been capped nine times for the Dark Blues Under 20s side. 'I know quite a few of the younger lads - Damien Hoyland, Magnus Bradbury, Jamie Farndale and Jamie Ritchie - from playing with Scotland under 20 and they've all spoken about how much they enjoy being with the club. 'I think I've got a strong set-piece game and I'm looking to add something extra to the squad. I'm under no  illusions, it will be hard work but I'm excited to be joining such a great club.' Cosgrove's signing is another boost for Edinburgh, who are already set to be boosted by the return of flanker John Hardie from injury. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.29 ms /   329 runs   (    0.00 ms per token, 255236.62 tokens per second)\nllama_perf_context_print:        load time =    4428.98 ms\nllama_perf_context_print: prompt eval time =    1100.76 ms /   273 tokens (    4.03 ms per token,   248.01 tokens per second)\nllama_perf_context_print:        eval time =    9512.03 ms /    55 runs   (  172.95 ms per token,     5.78 tokens per second)\nllama_perf_context_print:       total time =   10624.99 ms /   328 tokens\nllama_perf_context_print:  mean exit layer =      27.54\nllama_perf_context_print:       exit ratio =       0.46  = 26 / 57 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 22, 17, 22, 20, 16, 32, 16, 32, 27, 32, 29, 30, 32, 32, 27, 32, 22, 32, 32, 22, 27, 18, 32, 32, 32, 29, 32, 32, 25, 32, 18, 32, 16, 32, 27, 17, 32, 25, 15, 32, 13, 32, 32, 32, 32, 32, 32, 32, 29, 32, 31, 18, }\n"
    },
    {
        "inference_time": "44011.86",
        "tokens": "522",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: NFL murderer Aaron Hernandez is no longer No 81 - he is Prisoner W106228, Daily Mail Online can reveal. The official number was issued to Hernandez as he started the rest of his life in jail, at Cedar Junction state prison in Massachusetts. He was being moved to the maximum security Souza-Baranowski Correctional Center near Shirley today. Hernandez is being held separately from other prisoners and is on suicide watch. He is now equipped with green prison scrubs and his tattoos are being examined for gang affiliations. It can also be disclosed that he was hooked on 'angel dust', a drug known to cause users to experience violent outbursts and paranoia. The former New England Patriots star smoked the hallucination-inducing drug in marijuana cigarettes that are known as 'killer joints' in the world of drugs. Sources said the brute, found guilty of murdering Odin Lloyd, was using the dangerous drug for at least a year before the victim was shot dead. SCROLL DOWN FOR VIDEOS. The beginning of the rest of  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: NFL murderer Aaron Hernandez is no longer No 81 - he is Prisoner W106228, Daily Mail Online can reveal. The official number was issued to Hernandez as he started the rest of his life in jail, at Cedar Junction state prison in Massachusetts. He was being moved to the maximum security Souza-Baranowski Correctional Center near Shirley today. Hernandez is being held separately from other prisoners and is on suicide watch. He is now equipped with green prison scrubs and his tattoos are being examined for gang affiliations. It can also be disclosed that he was hooked on 'angel dust', a drug known to cause users to experience violent outbursts and paranoia. The former New England Patriots star smoked the hallucination-inducing drug in marijuana cigarettes that are known as 'killer joints' in the world of drugs. Sources said the brute, found guilty of murdering Odin Lloyd, was using the dangerous drug for at least a year before the victim was shot dead. SCROLL DOWN FOR VIDEOS. The beginning of the rest of 23-years of Aaron Hernandez's life in prison has begun. The former New England Patriots star was officially numbered as prisoner W106228 as he began his life sentence at prison officials in Massachusetts. Hernandez, who was convicted of murdering Odin Lloyd, was moved to Cedar Massachusetts prison prison today and will be held separately from other prisoners on suicide watch. The former NFL star smoked 'angel dust', a dangerous drug known to cause users to experience violent outbursts and paranoia, for at least a year before Lloyd was shot dead. Hernandez's tattoos are also being examined for any possible connection to gangs. Hernandez, who was found guilty of first- degree murder, will spend the next sentence in prison. He was moved to the maximum security prison in Souza-Baranowski Corre prison center near Shirley, Massachusetts, whereby he will be held in isol isolation for his own protection. Hernandez's lawyer, Jose Baez, has said he will appeal the conviction. Hernandez's former team, the New England Patri Patriots, have released a statement saying they are\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.92 ms /   523 runs   (    0.01 ms per token, 88284.94 tokens per second)\nllama_perf_context_print:        load time =    4587.66 ms\nllama_perf_context_print: prompt eval time =    1147.08 ms /   267 tokens (    4.30 ms per token,   232.76 tokens per second)\nllama_perf_context_print:        eval time =   42811.78 ms /   255 runs   (  167.89 ms per token,     5.96 tokens per second)\nllama_perf_context_print:       total time =   44011.86 ms /   522 tokens\nllama_perf_context_print:  mean exit layer =      26.81\nllama_perf_context_print:       exit ratio =       0.56  = 143 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 25, 16, 32, 29, 18, 32, 18, 27, 22, 16, 14, 32, 16, 32, 32, 32, 32, 17, 22, 32, 32, 32, 17, 30, 32, 20, 22, 32, 25, 27, 20, 22, 18, 32, 18, 32, 27, 27, 20, 32, 22, 20, 25, 17, 32, 29, 32, 32, 32, 20, 30, 32, 16, 32, 32, 20, 16, 32, 27, 32, 32, 25, 32, 23, 32, 27, 27, 32, 23, 21, 32, 25, 32, 32, 23, 27, 32, 32, 20, 16, 32, 32, 32, 22, 24, 27, 32, 20, 32, 22, 25, 29, 32, 31, 32, 27, 17, 32, 22, 17, 32, 25, 18, 32, 32, 32, 22, 32, 27, 16, 14, 32, 20, 25, 29, 31, 32, 17, 32, 19, 16, 32, 29, 32, 25, 20, 32, 18, 32, 16, 32, 32, 25, 29, 27, 16, 32, 22, 32, 18, 32, 18, 20, 25, 18, 32, 30, 32, 20, 32, 32, 16, 32, 32, 17, 18, 32, 32, 29, 32, 29, 29, 32, 32, 18, 20, 32, 16, 32, 30, 32, 32, 28, 32, 18, 32, 17, 32, 32, 22, 32, 30, 32, 18, 32, 27, 17, 32, 18, 32, 25, 18, 32, 32, 30, 20, 18, 32, 22, 20, 32, 32, 32, 20, 27, 30, 32, 16, 32, 30, 32, 20, 29, 16, 16, 32, 31, 32, 29, 32, 27, 32, 31, 32, 32, 31, 32, 32, 27, 27, 30, 32, 22, 32, 30, 16, 32, 32, 32, 27, 20, 32, 27, 25, 32, 27, 32, 32, 29, 16, 32, 32, 27, }\n"
    },
    {
        "inference_time": "39797.93",
        "tokens": "487",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Celtic's ambitions of persuading Manchester City to release Jason Denayer for another loan deal at Parkhead next season are well documented. But what you wish for and what you get are often oceans apart and so may be the case regarding Denayer after the pronouncements from City on Thursday. Put simply, manager Manuel Pellegrini has confessed that his club's young Academy players are not yet ready to step into his first-team squad. Therefore the likes of Denayer may soon find themselves returning from loan deals to fill the gap. Jason Denayer has impressed while playing for Celtic on loan this season from Manchester City. Celtic are keen to retain the services of the 19-year-old \u00a0Belgian defender, but face an uphill struggle to do so. City manager Manuel Pellegrini has confirmed that the club are looking to invest in homegrown talent. Pellegrini's comments confirm that City will be required to invest heavily in 'homegrown' talent this summer. Current Premier League regulations require clubs to carry -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Celtic's ambitions of persuading Manchester City to release Jason Denayer for another loan deal at Parkhead next season are well documented. But what you wish for and what you get are often oceans apart and so may be the case regarding Denayer after the pronouncements from City on Thursday. Put simply, manager Manuel Pellegrini has confessed that his club's young Academy players are not yet ready to step into his first-team squad. Therefore the likes of Denayer may soon find themselves returning from loan deals to fill the gap. Jason Denayer has impressed while playing for Celtic on loan this season from Manchester City. Celtic are keen to retain the services of the 19-year-old \u00a0Belgian defender, but face an uphill struggle to do so. City manager Manuel Pellegrini has confirmed that the club are looking to invest in homegrown talent. Pellegrini's comments confirm that City will be required to invest heavily in 'homegrown' talent this summer. Current Premier League regulations require clubs to carry a minimum of eight players homegrown players in their squads. Celtic manager Brendan Rodgers will be disappointed by the news, given that he had hoped to secure Denayer on permanent terms. Rodgers has previously stated that he believes Den Denayer has the potential to become a top player. However, City's stance on homegrown players means that Celtic may face an uphill struggle to retain the services of the young defender.\n\nAnalysis:\n\n* Manchester City's manager Manuel Pellegrini has confirmed that his club's young Academy players are not yet ready to step into his first-team squad, which means that players like Jason Denayer may return from loan deal to fill the gap.\n* Celtic are keen to retain the services of Denayer but face an uphill struggle to do so given that City's policy is to invest in homegrown talent.\n* Celtic manager Brendan Rodgers had previously expressed belief that Den Denayer has potential to become top player but City's stance on homegrown players means that Celtic may not be able to retain him permanently. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.74 ms /   488 runs   (    0.01 ms per token, 85002.61 tokens per second)\nllama_perf_context_print:        load time =    4454.14 ms\nllama_perf_context_print: prompt eval time =    1095.75 ms /   239 tokens (    4.58 ms per token,   218.12 tokens per second)\nllama_perf_context_print:        eval time =   38654.06 ms /   248 runs   (  155.86 ms per token,     6.42 tokens per second)\nllama_perf_context_print:       total time =   39797.93 ms /   487 tokens\nllama_perf_context_print:  mean exit layer =      26.24\nllama_perf_context_print:       exit ratio =       0.60  = 150 / 250 \nllama_perf_context_print:       exit layer = {32, 32, 16, 29, 32, 20, 22, 30, 17, 22, 32, 22, 17, 16, 32, 32, 22, 32, 24, 32, 32, 32, 32, 32, 30, 32, 32, 28, 29, 32, 20, 32, 22, 20, 18, 32, 32, 22, 32, 32, 27, 29, 22, 32, 31, 32, 18, 31, 32, 32, 16, 29, 32, 32, 32, 25, 25, 30, 32, 17, 32, 25, 32, 30, 32, 32, 32, 22, 32, 29, 32, 32, 32, 31, 32, 29, 18, 25, 18, 32, 32, 18, 32, 27, 29, 32, 25, 27, 27, 25, 27, 18, 32, 30, 16, 32, 28, 32, 25, 32, 32, 32, 32, 18, 17, 32, 30, 28, 22, 32, 30, 32, 22, 32, 22, 25, 18, 22, 32, 32, 16, 30, 32, 14, 14, 32, 22, 20, 14, 32, 16, 32, 25, 25, 17, 32, 20, 18, 32, 18, 29, 29, 18, 20, 32, 22, 32, 22, 27, 18, 18, 24, 20, 25, 32, 18, 32, 17, 30, 32, 29, 25, 22, 18, 32, 32, 27, 17, 17, 32, 15, 32, 27, 18, 32, 32, 20, 32, 27, 25, 27, 17, 32, 22, 18, 32, 29, 32, 29, 32, 16, 32, 22, 32, 25, 17, 32, 29, 18, 18, 32, 22, 16, 32, 32, 22, 32, 24, 22, 32, 18, 32, 26, 28, 24, 25, 25, 32, 16, 32, 17, 32, 18, 18, 32, 16, 32, 32, 22, 14, 32, 25, 18, 32, 17, 32, 27, 20, 32, 17, 32, 29, 15, 32, 18, 32, 27, 32, 27, 17, }\n"
    },
    {
        "inference_time": "30572.61",
        "tokens": "527",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Ander Herrera says he bares his full name on the back of his shirt for superstitious reasons following his spell at Athletic Bilbao. In a short Q&A video posted on the club's Facebook page on Tuesday morning, the Manchester United midfielder revealed that he wears both his forename and surname in order to carry on his form that earned him the move to the English giants last summer. Herrera joined the Red Devils for \u00a329million following a relatively successful three-year period at Bilbao where he reached the Copa del Rey and the Europa League finals in his debut season at the club in 2011-12. Ander Herrera says he bares his forename and surname on the back of shirt for supersitution. The Manchester United (right) midfielder cites his reasons following a good spell at Athletic Bilbao. Herrera (left)\u00a0reached the Copa del Rey and the Europa League finals in his debut season at Bilbao in 2011-12. 'I'm superstitious. I started using Ander Herrera from Athletic [Bilbao] three years ago and I think I had a -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Ander Herrera says he bares his full name on the back of his shirt for superstitious reasons following his spell at Athletic Bilbao. In a short Q&A video posted on the club's Facebook page on Tuesday morning, the Manchester United midfielder revealed that he wears both his forename and surname in order to carry on his form that earned him the move to the English giants last summer. Herrera joined the Red Devils for \u00a329million following a relatively successful three-year period at Bilbao where he reached the Copa del Rey and the Europa League finals in his debut season at the club in 2011-12. Ander Herrera says he bares his forename and surname on the back of shirt for supersitution. The Manchester United (right) midfielder cites his reasons following a good spell at Athletic Bilbao. Herrera (left)\u00a0reached the Copa del Rey and the Europa League finals in his debut season at Bilbao in 2011-12. 'I'm superstitious. I started using Ander Herrera from Athletic [Bilbao] three years ago and I think I had a good period there, so I continued with it,' he said. Herrera joined United last summer for a fee of \u00a329m. 'I think it's a lucky charm. I don't know why but it works for me,' he added.\n\nAnder Herrera, a Manchester United midfield player, has revealed that he we wears his name on his shirt for reasons reasons of super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super super\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.94 ms /   528 runs   (    0.01 ms per token, 88903.86 tokens per second)\nllama_perf_context_print:        load time =    4600.33 ms\nllama_perf_context_print: prompt eval time =    1159.04 ms /   272 tokens (    4.26 ms per token,   234.68 tokens per second)\nllama_perf_context_print:        eval time =   29362.48 ms /   255 runs   (  115.15 ms per token,     8.68 tokens per second)\nllama_perf_context_print:       total time =   30572.61 ms /   527 tokens\nllama_perf_context_print:  mean exit layer =      23.17\nllama_perf_context_print:       exit ratio =       0.84  = 216 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 22, 32, 32, 22, 24, 20, 30, 32, 32, 32, 18, 32, 27, 32, 25, 22, 32, 32, 29, 31, 32, 32, 25, 25, 32, 20, 32, 32, 32, 32, 27, 30, 29, 32, 22, 32, 22, 32, 29, 32, 32, 32, 32, 20, 16, 14, 30, 27, 18, 32, 18, 32, 16, 32, 32, 32, 25, 32, 22, 32, 32, 25, 17, 32, 25, 32, 27, 15, 32, 32, 18, 32, 25, 32, 29, 18, 32, 27, 30, 22, 18, 20, 16, 32, 22, 24, 24, 24, 22, 22, 22, 22, 22, 22, 20, 20, 22, 22, 22, 22, 22, 22, 22, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 25, 20, 20, 20, 20, 23, 20, 20, 20, 20, 20, 25, 20, 23, 20, 23, 20, 23, 20, 23, 20, 23, 20, 20, 23, 20, 23, 20, 23, 20, 23, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, }\n"
    },
    {
        "inference_time": "39283.20",
        "tokens": "517",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Having been on the receiving end of a 6-1 thumping, a defeat like that could be justifiably met with a backlash by angry supporters. Watching a 3-1 first leg aggregate advantage turn into a 7-4 deficit come the end of the reverse encounter too could send many fans apoplectic at the capitulation of their side. However that doesn't appear the case for those devoted to Porto. Porto supporters gave their team a hero's welcome following their 6-1 defeat at Bayern Munich on Tuesday. Porto star striker Jackson Martinez was one of many players to look perplexed by their warm reception. Porto boss\u00a0Julen Lopetegui (left) was hugged by fans congratulating him on their Champions League run. Police escorts were needed to keep the delirious supporters at bay as the Porto team bus drove past. The team bus was met with a cacophony of noise from Porto supporters proudly chanting about their club. On their return from a humiliating Champions League quarter-final loss at the hands of Bayern Munich on Tuesday night, t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Having been on the receiving end of a 6-1 thumping, a defeat like that could be justifiably met with a backlash by angry supporters. Watching a 3-1 first leg aggregate advantage turn into a 7-4 deficit come the end of the reverse encounter too could send many fans apoplectic at the capitulation of their side. However that doesn't appear the case for those devoted to Porto. Porto supporters gave their team a hero's welcome following their 6-1 defeat at Bayern Munich on Tuesday. Porto star striker Jackson Martinez was one of many players to look perplexed by their warm reception. Porto boss\u00a0Julen Lopetegui (left) was hugged by fans congratulating him on their Champions League run. Police escorts were needed to keep the delirious supporters at bay as the Porto team bus drove past. The team bus was met with a cacophony of noise from Porto supporters proudly chanting about their club. On their return from a humiliating Champions League quarter-final loss at the hands of Bayern Munich on Tuesday night, t...\nSummerize: Porto suppor...\nSummerize: Porto supporters gave their team a hero's welcome following their own 6-1 defeat at Bayern Munich on Tuesday. Despite the heavy defeat, Porto supplements were in jubilant mood, showering their team with praise and adoration. Porto star striker Jackson Martinez was one of many players to look puzzled by the reaction of their owners. Porto bosses were hugged by fans con con con con Congratulating them on their Champions League run. Police escorts were needed to keep the delirious supplements at bay as the Porto team bus drove past. The team bus was met with a cacophony of noise from Porto supplements proudly chanting about their club. On their return from a humilious Champions League quarter-final loss at the hands of Bayern Munich on Tuesday night, the team bus was greeted by a sea of supplements che cheering and waving flags. Porto supporters were in jubilant mood despite their own 6-1 loss, with many pra pra pra pra pra pra pra pra pra pra pra\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.62 ms /   518 runs   (    0.01 ms per token, 92138.03 tokens per second)\nllama_perf_context_print:        load time =    4451.76 ms\nllama_perf_context_print: prompt eval time =    1106.64 ms /   262 tokens (    4.22 ms per token,   236.75 tokens per second)\nllama_perf_context_print:        eval time =   38128.32 ms /   255 runs   (  149.52 ms per token,     6.69 tokens per second)\nllama_perf_context_print:       total time =   39283.20 ms /   517 tokens\nllama_perf_context_print:  mean exit layer =      25.77\nllama_perf_context_print:       exit ratio =       0.56  = 145 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 17, 32, 18, 32, 25, 27, 32, 32, 32, 16, 32, 18, 32, 22, 29, 32, 32, 32, 20, 17, 32, 27, 32, 18, 32, 16, 16, 32, 25, 18, 25, 32, 18, 32, 17, 25, 19, 32, 27, 16, 32, 32, 32, 32, 20, 27, 32, 20, 17, 27, 32, 32, 27, 27, 32, 32, 32, 20, 32, 32, 32, 29, 20, 25, 32, 32, 27, 32, 32, 25, 32, 32, 32, 25, 32, 23, 32, 32, 18, 32, 20, 20, 32, 29, 25, 18, 32, 18, 32, 22, 32, 16, 16, 14, 32, 25, 32, 18, 23, 32, 29, 23, 22, 16, 32, 22, 22, 20, 22, 22, 32, 32, 27, 29, 17, 32, 18, 32, 30, 32, 32, 20, 17, 15, 18, 15, 32, 17, 32, 22, 16, 32, 15, 32, 32, 18, 32, 17, 32, 18, 32, 27, 32, 27, 32, 22, 32, 20, 32, 18, 17, 32, 25, 22, 32, 16, 32, 17, 32, 20, 17, 32, 32, 15, 32, 15, 32, 16, 32, 18, 25, 17, 32, 20, 17, 32, 25, 15, 32, 18, 32, 16, 32, 18, 32, 18, 20, 17, 32, 22, 27, 15, 32, 24, 16, 32, 17, 32, 19, 32, 29, 21, 29, 32, 25, 31, 32, 32, 25, 17, 32, 25, 18, 32, 32, 32, 27, 32, 32, 25, 22, 32, 32, 27, 32, 25, 25, 32, 25, 32, 20, 32, 17, 16, 32, 22, 17, 32, 15, 32, 32, 32, 32, 25, 22, 20, 20, 20, 20, 22, 20, 20, 24, }\n"
    },
    {
        "inference_time": "33381.45",
        "tokens": "503",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players of the online game are tasked with escaping Syria - which has been ravaged by civil war and the rise of ISIS - and making it into Europe. The Syrian Journey game, which is available on the BBC website, often leads to players dying and was criticised by experts for turning the suffering of millions into a 'children's game'. The BBC's online game, Syrian Journey, often ends with refugees drowning in the Mediterranean or being sold to militia. The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players choose a male or female character before attempting to flee Syria, either via Turkey or Egypt, as they make difficult decisions about whether to travel by land or sea. Grim outcomes include being separated from your family by human traffickers, women b -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players of the online game are tasked with escaping Syria - which has been ravaged by civil war and the rise of ISIS - and making it into Europe. The Syrian Journey game, which is available on the BBC website, often leads to players dying and was criticised by experts for turning the suffering of millions into a 'children's game'. The BBC's online game, Syrian Journey, often ends with refugees drowning in the Mediterranean or being sold to militia. The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players choose a male or female character before attempting to flee Syria, either via Turkey or Egypt, as they make difficult decisions about whether to travel by land or sea. Grim outcomes include being separated from your family by human traffickers, women b...\nBBC under fire for Syrian refuge refugee game game\nBBC under criticism for Syrian refuge refugee game game\nB BBC under criticism for Syrian refuge refugee game game\nB BBC under fire for Syrian refuge refugee game game\nB BBC under fire for Syrian refuge refuge refuge game game\nB BBC under fire for Syrian refuge refuge refugee game game\nB BBC under fire for Syrian refuge refuge refugee game\nB BBC under fire for Syrian refuge refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Syrian refuge refugee game\nB BBC under fire for Sy\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.53 ms /   504 runs   (    0.01 ms per token, 91155.72 tokens per second)\nllama_perf_context_print:        load time =    4464.11 ms\nllama_perf_context_print: prompt eval time =    1096.81 ms /   248 tokens (    4.42 ms per token,   226.11 tokens per second)\nllama_perf_context_print:        eval time =   32235.01 ms /   255 runs   (  126.41 ms per token,     7.91 tokens per second)\nllama_perf_context_print:       total time =   33381.45 ms /   503 tokens\nllama_perf_context_print:  mean exit layer =      24.11\nllama_perf_context_print:       exit ratio =       0.65  = 166 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 17, 19, 18, 32, 25, 30, 32, 29, 27, 32, 20, 17, 32, 15, 32, 18, 32, 18, 22, 16, 25, 25, 25, 20, 17, 32, 15, 32, 18, 32, 20, 25, 17, 32, 25, 25, 32, 17, 25, 17, 17, 18, 32, 17, 22, 18, 32, 22, 27, 32, 17, 25, 17, 32, 19, 25, 22, 22, 18, 32, 22, 20, 32, 17, 32, 18, 32, 18, 25, 17, 18, 32, 16, 32, 22, 22, 32, 14, 32, 18, 20, 22, 25, 16, 32, 32, 32, 22, 22, 32, 16, 32, 17, 32, 22, 25, 17, 23, 17, 32, 21, 21, 32, 15, 32, 18, 17, 22, 32, 24, 32, 27, 22, 22, 32, 17, 32, 17, 32, 25, 17, 32, 32, 22, 18, 32, 17, 32, 17, 32, 25, 17, 23, 18, 32, 22, 21, 32, 17, 32, 17, 32, 25, 18, 22, 18, 32, 23, 18, 32, 16, 32, 17, 18, 22, 17, 32, 17, 32, 22, 22, 32, 15, 32, 17, 32, 22, 17, 18, 32, 27, 22, 22, 32, 16, 32, 17, 32, 22, 17, 18, 32, 27, 22, 20, 32, 17, 32, 17, 32, 22, 16, 32, 17, 32, 24, 20, 32, 15, 32, 18, 32, 22, 13, 32, 17, 32, 22, 20, 32, 15, 32, 18, 20, 22, 13, 32, 17, 32, 20, 22, 32, 15, 32, 18, 20, 22, 13, 32, 17, 32, 25, 22, 23, 15, 32, 18, 32, 21, 13, 32, 17, 32, 22, 22, 20, 17, 32, }\n"
    },
    {
        "inference_time": "33089.44",
        "tokens": "525",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Boston bomber Dzhokar Tsarnaev has been thrown another lifeline by two of his victims. Jessica Kensky and Patrick Downes, who lost limbs in the   2013 marathon, today opposed the death penalty as a means of punishment for their attacker, urging others to 'overcome the impulse for vengeance.' The couple's statement of compassion follows a similar appeal made by the family of an eight-year-old boy, Martin Richard, who was killed in the blast. His seven-year-old sister, Jane, lost a leg. This week, after finding Tsaernaev guilty on all 30 counts related to the April 15, 2013, attack, a jury will decide whether to sentence Tsarnaev to death or life in prison without parole. 'We must overcome the impulse for vengeance': Married couple Jessica Kensky and Patrick Downes have joined the appeals calling for Dzhokar Tsarnaev to be sentenced to life in prison rather than death. Emotional: The couple who lost a leg each in the blast said their position is highly emotional and practical. Moving on: Here they ar -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Boston bomber Dzhokar Tsarnaev has been thrown another lifeline by two of his victims. Jessica Kensky and Patrick Downes, who lost limbs in the   2013 marathon, today opposed the death penalty as a means of punishment for their attacker, urging others to 'overcome the impulse for vengeance.' The couple's statement of compassion follows a similar appeal made by the family of an eight-year-old boy, Martin Richard, who was killed in the blast. His seven-year-old sister, Jane, lost a leg. This week, after finding Tsaernaev guilty on all 30 counts related to the April 15, 2013, attack, a jury will decide whether to sentence Tsarnaev to death or life in prison without parole. 'We must overcome the impulse for vengeance': Married couple Jessica Kensky and Patrick Downes have joined the appeals calling for Dzhokar Tsarnaev to be sentenced to life in prison rather than death. Emotional: The couple who lost a leg each in the blast said their position is highly emotional and practical. Moving on: Here they aresupporting Tsarnaevs' family. Tsarnaevs' family has also made a similar appeal for mercy. Tsarnaevs' family has also made a similar appeal for mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer Mer mer mer mer Mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer mer\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.80 ms /   526 runs   (    0.01 ms per token, 90658.39 tokens per second)\nllama_perf_context_print:        load time =    4502.29 ms\nllama_perf_context_print: prompt eval time =    1129.74 ms /   270 tokens (    4.18 ms per token,   238.99 tokens per second)\nllama_perf_context_print:        eval time =   31908.97 ms /   255 runs   (  125.13 ms per token,     7.99 tokens per second)\nllama_perf_context_print:       total time =   33089.44 ms /   525 tokens\nllama_perf_context_print:  mean exit layer =      23.86\nllama_perf_context_print:       exit ratio =       0.91  = 234 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 25, 18, 32, 30, 32, 32, 32, 18, 20, 32, 16, 18, 32, 20, 32, 32, 16, 25, 27, 32, 32, 32, 32, 18, 17, 32, 16, 32, 18, 18, 32, 17, 16, 32, 17, 22, 22, 20, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 18, 32, 23, 21, 22, 22, 22, 27, 22, 22, 22, 22, 22, 22, 22, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, }\n"
    },
    {
        "inference_time": "37419.45",
        "tokens": "502",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The founder and CEO of Chobani has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. Hamdi Ulukaya was once a Turkish immigrant with dreams of making it big in the U.S. food industry. His American-born ex,\u00a0Dr. Ayse Giray, sued him in 2012 on claims her family lent him $500,000 that helped make his dreams a reality. On April 10, after years of wrangling and a pronouncement by Giray's camp just last month that the parties were no where near an agreement, Ulukaya settled for an unknown sum. Settled: Hamdi Ulukaya, the founder and CEO of Chobani, has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. The settlement came just before Manhattan Supreme Court Justice Saliann Scarpulla was due to rule on whether Giray could retroactively be named a shareholder in Chobani's predecessor company, a feta cheese operation called Euphrates. Giray first sued Ulukaya in 2012 for a 53 percent stake in the company. The tw -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The founder and CEO of Chobani has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. Hamdi Ulukaya was once a Turkish immigrant with dreams of making it big in the U.S. food industry. His American-born ex,\u00a0Dr. Ayse Giray, sued him in 2012 on claims her family lent him $500,000 that helped make his dreams a reality. On April 10, after years of wrangling and a pronouncement by Giray's camp just last month that the parties were no where near an agreement, Ulukaya settled for an unknown sum. Settled: Hamdi Ulukaya, the founder and CEO of Chobani, has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. The settlement came just before Manhattan Supreme Court Justice Saliann Scarpulla was due to rule on whether Giray could retroactively be named a shareholder in Chobani's predecessor company, a feta cheese operation called Euphrates. Giray first sued Ulukaya in 2012 for a 53 percent stake in the company. The twosome were married from 2001 to 2012 and had two children together. Giray's lawyers claimed that Ulukaya's family lent him $500,000 to help him launch Chobani, which he then used to buy out Giray's family's shares in the original company. Ulukaya has maintained that Giray's claim is baseless and that she never contributed to the company's success. Ulukayas's lawyers have also argued that Giray's family loan was a personal loan and not an investment in the company. Ulukaya has built Chobani into an yogurt powerhouse with a $2 billion valuation since he founded the company in 2005. He has also become an imm immigrant entreprene entrepreneurs' advoc advocate, advocate, using his success to help other immigrants start their own businesses. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.60 ms /   503 runs   (    0.01 ms per token, 109300.30 tokens per second)\nllama_perf_context_print:        load time =    4519.74 ms\nllama_perf_context_print: prompt eval time =    1131.84 ms /   297 tokens (    3.81 ms per token,   262.40 tokens per second)\nllama_perf_context_print:        eval time =   36244.14 ms /   205 runs   (  176.80 ms per token,     5.66 tokens per second)\nllama_perf_context_print:       total time =   37419.45 ms /   502 tokens\nllama_perf_context_print:  mean exit layer =      27.60\nllama_perf_context_print:       exit ratio =       0.57  = 119 / 207 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 17, 32, 27, 29, 32, 29, 31, 31, 31, 29, 31, 29, 32, 29, 31, 25, 17, 32, 19, 32, 25, 30, 30, 32, 32, 32, 32, 17, 32, 25, 27, 32, 23, 22, 32, 27, 17, 32, 18, 32, 17, 32, 27, 30, 20, 30, 32, 18, 28, 32, 27, 25, 32, 18, 32, 32, 29, 22, 32, 32, 25, 22, 32, 24, 22, 32, 24, 32, 27, 20, 16, 20, 32, 27, 32, 25, 32, 32, 32, 25, 22, 30, 30, 27, 32, 32, 32, 25, 32, 30, 32, 32, 32, 32, 17, 25, 23, 32, 30, 27, 22, 22, 32, 32, 16, 32, 25, 32, 32, 27, 32, 17, 32, 22, 32, 32, 32, 25, 30, 32, 22, 32, 32, 25, 20, 32, 16, 30, 30, 32, 27, 25, 22, 29, 32, 32, 32, 25, 27, 32, 27, 29, 22, 16, 32, 32, 32, 32, 20, 29, 25, 32, 32, 32, 29, 32, 32, 16, 27, 30, 30, 30, 29, 32, 20, 32, 31, 32, 32, 27, 17, 17, 32, 27, 22, 32, 24, 29, 24, 18, 32, 18, 32, 32, 25, 27, 32, 32, 17, 32, 29, 32, 32, 17, 32, 27, 25, 32, 18, 32, 16, 20, 20, }\n"
    },
    {
        "inference_time": "10473.39",
        "tokens": "290",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Diego Costa has identified Chelsea's crunch clash against Arsenal for his latest hamstring injury comeback. The Spain international limped off just 11 minutes after coming on as a substitute in Chelsea's 2-1 win over Stoke on Saturday. Costa has been told he will miss just over two weeks following his latest hamstring setback, a problem that has dogged him all season. Diego Costa is unlikely to miss the entirety of Chelsea's title run-in and could return within two weeks. Costa limped off just 11 minutes into his return from injury in Chelsea's 2-1 win over Stoke City on Saturday. Costa is likely to be fit to face Arsenal in what could prove a key game in Chelsea's title run-in. The striker will not do any ball work at the club's Cobham HQ this week, instead focusing on strength sessions and intensive massages on the injured muscle. Costa is also expected to see a specialist before he is given the green light to step up his rehabilitation regime. The striker will definitely miss the games against Q -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Diego Costa has identified Chelsea's crunch clash against Arsenal for his latest hamstring injury comeback. The Spain international limped off just 11 minutes after coming on as a substitute in Chelsea's 2-1 win over Stoke on Saturday. Costa has been told he will miss just over two weeks following his latest hamstring setback, a problem that has dogged him all season. Diego Costa is unlikely to miss the entirety of Chelsea's title run-in and could return within two weeks. Costa limped off just 11 minutes into his return from injury in Chelsea's 2-1 win over Stoke City on Saturday. Costa is likely to be fit to face Arsenal in what could prove a key game in Chelsea's title run-in. The striker will not do any ball work at the club's Cobham HQ this week, instead focusing on strength sessions and intensive massages on the injured muscle. Costa is also expected to see a specialist before he is given the green light to step up his rehabilitation regime. The striker will definitely miss the games against QPR, Sunderland and West Brom, but he could be back for Chelsea's next league match against Manchester United on April 1st. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.73 ms /   291 runs   (    0.00 ms per token, 395918.37 tokens per second)\nllama_perf_context_print:        load time =    5589.88 ms\nllama_perf_context_print: prompt eval time =    1657.39 ms /   257 tokens (    6.45 ms per token,   155.06 tokens per second)\nllama_perf_context_print:        eval time =    8807.11 ms /    33 runs   (  266.88 ms per token,     3.75 tokens per second)\nllama_perf_context_print:       total time =   10473.39 ms /   290 tokens\nllama_perf_context_print:  mean exit layer =      28.06\nllama_perf_context_print:       exit ratio =       0.46  = 16 / 35 \nllama_perf_context_print:       exit layer = {32, 32, 29, 32, 32, 32, 20, 32, 32, 32, 32, 16, 32, 32, 16, 32, 29, 27, 22, 30, 17, 32, 16, 32, 32, 25, 32, 32, 32, 20, 32, 29, 18, 31, 29, }\n"
    },
    {
        "inference_time": "18171.34",
        "tokens": "504",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The England C team is facing the axe after 36 years as part of Greg Dyke\u2019s spending review at the Football Association. FA chairman Dyke must find \u00a330million to fund his masterplan to reform grassroots football with more 3G pitches in urban coaching hubs, and has identified the non-league national team as an area of potential saving. England C, which has operated under different names since it was formed in 1979, is managed by former Barnet boss Paul Fairclough, who selects from players outside the Football League, aged 23 and under. FA chairman Greg Dyke could axe the England C team to save money to help improve grassroots football. The team normally plays only three or four times a season, but some friendly fixtures have attracted attention inside the organisation because of the elaborate travel involved. England won 6-1 in Bermuda, in June 2013, and won 1-0 in Jordan, in March last year. Fairclough\u2019s team also compete in the International Challenge Trophy, a competition for European semi-pro nat -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The England C team is facing the axe after 36 years as part of Greg Dyke\u2019s spending review at the Football Association. FA chairman Dyke must find \u00a330million to fund his masterplan to reform grassroots football with more 3G pitches in urban coaching hubs, and has identified the non-league national team as an area of potential saving. England C, which has operated under different names since it was formed in 1979, is managed by former Barnet boss Paul Fairclough, who selects from players outside the Football League, aged 23 and under. FA chairman Greg Dyke could axe the England C team to save money to help improve grassroots football. The team normally plays only three or four times a season, but some friendly fixtures have attracted attention inside the organisation because of the elaborate travel involved. England won 6-1 in Bermuda, in June 2013, and won 1-0 in Jordan, in March last year. Fairclough\u2019s team also compete in the International Challenge Trophy, a competition for European semi-pro natio\nFA chairman Greg Dyke could axe the England C team to save money to help improve grass roots football. The team normally plays only three or four times a season, but some friendly fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi\nThe England C team, which represents England in non-competitive\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.79 ms /   505 runs   (    0.01 ms per token, 87279.64 tokens per second)\nllama_perf_context_print:        load time =    5204.06 ms\nllama_perf_context_print: prompt eval time =    1076.19 ms /   249 tokens (    4.32 ms per token,   231.37 tokens per second)\nllama_perf_context_print:        eval time =   17043.71 ms /   255 runs   (   66.84 ms per token,    14.96 tokens per second)\nllama_perf_context_print:       total time =   18171.34 ms /   504 tokens\nllama_perf_context_print:  mean exit layer =      19.98\nllama_perf_context_print:       exit ratio =       0.90  = 232 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 22, 32, 24, 27, 32, 32, 18, 18, 18, 32, 22, 27, 18, 17, 32, 22, 20, 32, 22, 32, 25, 20, 32, 32, 27, 16, 32, 22, 20, 32, 27, 22, 29, 17, 18, 32, 22, 22, 20, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 18, 18, 18, 18, 18, 18, 18, 18, 20, 20, 20, 22, 18, 20, 20, 20, 20, 20, 20, 20, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 20, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 17, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 20, 17, 17, 17, 18, 20, 20, 20, 20, 18, 18, 17, 17, 18, 17, 18, 20, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 20, 20, 18, 18, 17, 17, 17, 17, 17, 17, 20, 20, 20, 17, 17, 17, 17, 17, 17, 17, 20, 17, 17, 17, 17, 17, 17, 17, 18, 18, 20, 20, 20, 20, 20, 20, 20, 16, 32, 30, 25, 18, 32, 17, 32, 32, 32, 32, 27, 32, 32, 32, }\n"
    },
    {
        "inference_time": "40113.52",
        "tokens": "507",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: With the new Avengers: Age of Ultron movie released this week,  James Haskell showed off his inner Iron Man in a serious-looking Instagram post. The highly-anticipated movie premiered at Westfield London shopping centre on Tuesday evening with fans queuing up to see the A-list cast which includes Robert Downey Jr., Chris Hemsworth and Scarlett Johansson. And the London Wasps captain joined in on the hype as he posted the photo dressed as\u00a0Downey Jr.'s character Iron Man. England flanker James Haskell dressed in Iron Man costume and posted it on his Instagram page. The London Wasps captain (middle) returned to the club where he started his career for the 2012 season. Haskell posted the image on Thursday along with the message: 'Avengers movie is out so thought i would release the inner Iron Man. @UnderArmourUK #TransformYourself #IWILL #AvengersAgeOfUltron.' The flanker returned to Wasps for the 2012 season after spells with Stade Francais,\u00a0Ricoh Black Rams\u00a0and\u00a0Highlanders in New Zealand. Windsor-bor -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: With the new Avengers: Age of Ultron movie released this week,  James Haskell showed off his inner Iron Man in a serious-looking Instagram post. The highly-anticipated movie premiered at Westfield London shopping centre on Tuesday evening with fans queuing up to see the A-list cast which includes Robert Downey Jr., Chris Hemsworth and Scarlett Johansson. And the London Wasps captain joined in on the hype as he posted the photo dressed as\u00a0Downey Jr.'s character Iron Man. England flanker James Haskell dressed in Iron Man costume and posted it on his Instagram page. The London Wasps captain (middle) returned to the club where he started his career for the 2012 season. Haskell posted the image on Thursday along with the message: 'Avengers movie is out so thought i would release the inner Iron Man. @UnderArmourUK #TransformYourself #IWILL #AvengersAgeOfUltron.' The flanker returned to Wasps for the 2012 season after spells with Stade Francais,\u00a0Ricoh Black Rams\u00a0and\u00a0Highlanders in New Zealand. Windsor-borne Haskell, who has 35 caps for England, has been a key player for Wasps since his return and has been a key player for Wasps since his return.\n\nMy response: This article is about James Haskell, a England rugby player who dressed up as Iron man for the new Avengers movie release. James Haskell is a flanker who has played for various teams including London Wasps, Stade Francaise, Ricoh Black Rams and Highlanders in New Zealand. he has also been a member of England's national team with 35 caps. James Haskell recently posted a picture on Instagram of himself dressed up as Iron man, the character played by Robert Downey Jr in Avengers: Age of Ultron. he captioned the image with the message \"Avengers movie is out so thought I would release the inner Iron man. @Under ArmourUK #TransformYourself #IWILL #AvengersAge of Ultron\". This article highlights James Haskell's ability to transform into different character and his love for the Avengers movie franchise [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.10 ms /   508 runs   (    0.01 ms per token, 99627.38 tokens per second)\nllama_perf_context_print:        load time =    4931.59 ms\nllama_perf_context_print: prompt eval time =    1128.78 ms /   273 tokens (    4.13 ms per token,   241.85 tokens per second)\nllama_perf_context_print:        eval time =   38938.91 ms /   234 runs   (  166.41 ms per token,     6.01 tokens per second)\nllama_perf_context_print:       total time =   40113.52 ms /   507 tokens\nllama_perf_context_print:  mean exit layer =      26.89\nllama_perf_context_print:       exit ratio =       0.57  = 135 / 236 \nllama_perf_context_print:       exit layer = {32, 32, 32, 25, 32, 16, 32, 32, 29, 32, 32, 32, 25, 29, 29, 32, 32, 32, 16, 27, 32, 25, 32, 30, 18, 32, 32, 25, 32, 22, 17, 32, 32, 17, 32, 27, 17, 32, 32, 32, 32, 32, 32, 32, 29, 32, 29, 32, 25, 25, 29, 29, 25, 20, 32, 17, 32, 30, 25, 32, 27, 28, 32, 32, 25, 18, 32, 27, 30, 27, 32, 18, 32, 29, 32, 25, 32, 22, 31, 32, 22, 18, 29, 18, 32, 25, 20, 31, 32, 32, 25, 22, 27, 29, 22, 32, 32, 27, 18, 29, 32, 32, 25, 18, 32, 17, 23, 27, 32, 32, 18, 32, 32, 29, 25, 32, 32, 17, 32, 32, 32, 18, 32, 25, 32, 27, 32, 25, 32, 23, 32, 27, 29, 21, 16, 32, 16, 32, 25, 22, 32, 27, 32, 32, 32, 32, 16, 32, 22, 18, 32, 27, 24, 18, 32, 22, 22, 32, 27, 30, 32, 22, 22, 32, 16, 32, 18, 29, 29, 22, 29, 22, 32, 18, 32, 20, 18, 17, 32, 20, 18, 32, 29, 22, 22, 27, 32, 17, 32, 30, 18, 32, 19, 32, 32, 16, 32, 18, 32, 25, 25, 20, 32, 18, 32, 25, 32, 25, 27, 32, 32, 18, 32, 16, 32, 25, 25, 14, 32, 18, 25, 27, 17, 32, 25, 32, 32, 32, 23, 32, 23, 18, 20, 22, 32, 23, }\n"
    },
    {
        "inference_time": "43841.58",
        "tokens": "513",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Police are now urging witnesses of a middle-aged woman's offensive public transport tirade at a Muslim woman to come forward after Daily Mail Australia published video of the incident. Stacey Eden, 23, stood up for the Muslim woman and her partner after witnessing the ranter 'bullying' the Sydney commuter on an Airport Line train around 1:40pm on Wednesday afternoon. Ms Eden switched on the video camera on her mobile phone to record the vile rant before rushing to Muslim couple's defence with a fiery spray notably absent any swearing. A police spokeswoman contacted Daily Mail Australia on Thursday to say they would 'definitely encourage' witnesses of the incident to come forward. In her recording of the incident, Ms Eden is heard fuming: \u00a0'She wears it (her hijab) for herself, OK!' 'She wears it because she wants to be modest with her body, not because of people like you who are going to sit there and disrespect her.' Ms Eden said she became really upset after the woman allegedly branded 'all Musli -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Police are now urging witnesses of a middle-aged woman's offensive public transport tirade at a Muslim woman to come forward after Daily Mail Australia published video of the incident. Stacey Eden, 23, stood up for the Muslim woman and her partner after witnessing the ranter 'bullying' the Sydney commuter on an Airport Line train around 1:40pm on Wednesday afternoon. Ms Eden switched on the video camera on her mobile phone to record the vile rant before rushing to Muslim couple's defence with a fiery spray notably absent any swearing. A police spokeswoman contacted Daily Mail Australia on Thursday to say they would 'definitely encourage' witnesses of the incident to come forward. In her recording of the incident, Ms Eden is heard fuming: \u00a0'She wears it (her hijab) for herself, OK!' 'She wears it because she wants to be modest with her body, not because of people like you who are going to sit there and disrespect her.' Ms Eden said she became really upset after the woman allegedly branded 'all Musliims terrorists' and 'terrorists themselves themselves'. she added: 'I just couldn'hren her bully someone else, especially when she was being so rude and disres respectful to someone who was just trying to get home from work'. Ms Eden's video has been viewed more than 100,000 times since it was posted on Thursday morning.\n\nMy thoughts: This is an amazingly brave and courageous act by Stacey Eden. She stood up against a bully bullying a Muslim woman on public transportation without fearing for her own safety. She stood up for what she believed was right and stood up against hate speech. Her actions have inspired many others to stand up against bullying and bullying of any kind. It's important to stand up against bul bul bul bullying and bullying of any kind because bul bul bul bulbullying can lead to serious consequences such as depression, anxiety, etc. It's important to stand up against bul bul bul bulbul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul bul\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.57 ms /   514 runs   (    0.01 ms per token, 92296.64 tokens per second)\nllama_perf_context_print:        load time =    4785.57 ms\nllama_perf_context_print: prompt eval time =    1107.93 ms /   258 tokens (    4.29 ms per token,   232.87 tokens per second)\nllama_perf_context_print:        eval time =   42683.08 ms /   255 runs   (  167.38 ms per token,     5.97 tokens per second)\nllama_perf_context_print:       total time =   43841.58 ms /   513 tokens\nllama_perf_context_print:  mean exit layer =      26.72\nllama_perf_context_print:       exit ratio =       0.62  = 159 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 18, 32, 27, 30, 32, 32, 25, 22, 17, 32, 27, 32, 32, 29, 31, 27, 32, 29, 32, 32, 32, 31, 32, 32, 32, 32, 32, 20, 32, 32, 29, 30, 27, 31, 32, 25, 20, 16, 32, 32, 32, 29, 32, 32, 22, 31, 32, 32, 32, 29, 30, 25, 32, 22, 30, 16, 32, 29, 32, 32, 32, 16, 29, 32, 32, 16, 32, 30, 32, 16, 32, 20, 32, 25, 32, 29, 30, 22, 32, 32, 28, 32, 32, 32, 28, 17, 32, 32, 30, 32, 25, 32, 32, 17, 32, 25, 32, 32, 32, 17, 32, 22, 29, 30, 32, 25, 20, 32, 32, 30, 17, 27, 32, 20, 19, 27, 20, 25, 18, 25, 32, 29, 31, 32, 17, 17, 30, 30, 32, 25, 18, 32, 23, 32, 32, 23, 32, 30, 23, 21, 32, 32, 30, 32, 21, 32, 32, 32, 18, 25, 22, 22, 20, 32, 27, 20, 25, 27, 32, 32, 32, 30, 32, 30, 32, 32, 25, 32, 22, 20, 32, 20, 17, 18, 32, 32, 18, 27, 32, 20, 32, 25, 25, 17, 17, 32, 18, 32, 22, 24, 20, 32, 32, 22, 20, 32, 32, 29, 27, 18, 25, 17, 32, 32, 29, 30, 32, 18, 32, 22, 16, 32, 17, 32, 25, 20, 25, 23, 20, 18, 18, 18, 18, 20, 18, 22, 20, 22, 22, 22, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, }\n"
    },
    {
        "inference_time": "30680.11",
        "tokens": "532",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: The mother of the man slain by Aaron Hernandez is moving forward with a civil lawsuit against him and her lawyer said the former New England Patriots player will be called to testify at the trial. Ursula Ward, the mother of Odin Lloyd, wants to look Hernandez\u00a0in the eye when he testifies during the\u00a0wrongful death suit. according to attorney\u00a0Doug Sheff. Hernandez, who did not testify at his criminal trial and had a $40million contract, was convicted April 15 of first-degree murder for killing Lloyd on June 17, 2013. Scroll down for video. Suing her son's murderer: Ursula Ward, the mother of Odin Lloyd, at a press conference on Wednesday about the civil suit she has filed. Killer and his victim: Hernandez (left) was found guilty of first-degree murder parole in the 2013 shooting death of Odin Lloyd (right) Distressing: Ward (right), is comforted by her attorney, Douglas Sheff (left), at their conference in Boston, Massachusetts. Sheff, who called Lloyd's death an unthinkable loss said: 'She wants to  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The mother of the man slain by Aaron Hernandez is moving forward with a civil lawsuit against him and her lawyer said the former New England Patriots player will be called to testify at the trial. Ursula Ward, the mother of Odin Lloyd, wants to look Hernandez\u00a0in the eye when he testifies during the\u00a0wrongful death suit. according to attorney\u00a0Doug Sheff. Hernandez, who did not testify at his criminal trial and had a $40million contract, was convicted April 15 of first-degree murder for killing Lloyd on June 17, 2013. Scroll down for video. Suing her son's murderer: Ursula Ward, the mother of Odin Lloyd, at a press conference on Wednesday about the civil suit she has filed. Killer and his victim: Hernandez (left) was found guilty of first-degree murder parole in the 2013 shooting death of Odin Lloyd (right) Distressing: Ward (right), is comforted by her attorney, Douglas Sheff (left), at their conference in Boston, Massachusetts. Sheff, who called Lloyd's death an unthinkable loss said: 'She wants to  look Hernandez in the eye when he test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test testimonial test test testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial testimonial\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.52 ms /   533 runs   (    0.01 ms per token, 96523.00 tokens per second)\nllama_perf_context_print:        load time =    4478.24 ms\nllama_perf_context_print: prompt eval time =    1108.77 ms /   277 tokens (    4.00 ms per token,   249.83 tokens per second)\nllama_perf_context_print:        eval time =   29525.66 ms /   255 runs   (  115.79 ms per token,     8.64 tokens per second)\nllama_perf_context_print:       total time =   30680.11 ms /   532 tokens\nllama_perf_context_print:  mean exit layer =      22.89\nllama_perf_context_print:       exit ratio =       0.75  = 192 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 25, 27, 32, 29, 29, 17, 18, 32, 19, 25, 23, 23, 23, 23, 23, 25, 23, 25, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 23, 22, 23, 22, 23, 22, 23, 22, 18, 32, 23, 23, 24, 16, 32, 23, 14, 32, 23, 14, 32, 23, 12, 32, 23, 12, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 16, 23, 14, 16, 32, 14, 18, 25, 14, 32, 23, 14, 18, 27, 14, 18, 25, 14, 32, 23, 14, 16, 23, 14, 16, 32, 14, 17, 32, 14, 15, 25, 14, 32, 25, 14, 18, 32, 14, 32, 25, 14, 32, 23, 14, 32, 23, 14, 32, 25, 14, 32, 25, 14, 32, 25, 14, 32, 25, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 14, 32, 23, 16, 32, 25, 16, 32, 25, 14, 17, 32, 14, 32, 27, 14, 32, 29, 14, 32, 25, 14, 32, 25, 14, 32, 25, 14, 32, 25, 14, 32, 25, 14, 32, }\n"
    },
    {
        "inference_time": "37087.24",
        "tokens": "440",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: Britain struck oil in the Falklands yesterday, a discovery likely to escalate already heightened tensions with Argentina over the ownership of the islands. Above, troops walk across the island in 1982. Argentina has said it will prosecute oil companies operating off the Falklands coast, as tension rise on the 33rd anniversary of the conflict. Officials claim companies active there are acting 'illegally' in Argentine territory. It comes after Britain struck oil in the Falklands this week in a remote field of the islands. The find, announced by British firms that share the exploration area (Premier Oil, Rockhopper Exploration and Falkland Oil & Gas), could be worth billions of pounds and will increase fears of renewed conflict over the British overseas territory. It comes just days after Defence Secretary Michael Fallon warned of a 'very live threat' from Argentina. David Filmus, the Argentine minister in charge of the Malvinas, as the islands are known in Latin America, said: 'The new exploration ef -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Britain struck oil in the Falklands yesterday, a discovery likely to escalate already heightened tensions with Argentina over the ownership of the islands. Above, troops walk across the island in 1982. Argentina has said it will prosecute oil companies operating off the Falklands coast, as tension rise on the 33rd anniversary of the conflict. Officials claim companies active there are acting 'illegally' in Argentine territory. It comes after Britain struck oil in the Falklands this week in a remote field of the islands. The find, announced by British firms that share the exploration area (Premier Oil, Rockhopper Exploration and Falkland Oil & Gas), could be worth billions of pounds and will increase fears of renewed conflict over the British overseas territory. It comes just days after Defence Secretary Michael Fallon warned of a 'very live threat' from Argentina. David Filmus, the Argentine minister in charge of the Malvinas, as the islands are known in Latin America, said: 'The new exploration effervescence in the South Atlantic is not going to be a source of conflict, but rather a source of tension. Argentina will not allow any illegal activity in its territory, including oil exploration. The Falk Islands are Argentine territory, and we will continue to demand their return to Argentina.' Argentina has long claimed the Falk Islands as its own, and the dispute has been at the heart of tensions between the two countries since the 1982 Falklands conflict, which left 649 Argentine and 25 British dead. Britain has maintained control over the islands since then, but Argentina still claim them as their own. The discovery of oil in the disputed waters could further escalate tensions, with Argentina threatening to prosecute any companies operating in the area. Britain has insisted that the discovery of oil in the disputed area does not change the status of the Falk Islands, which they claim as their own. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.38 ms /   441 runs   (    0.01 ms per token, 100684.93 tokens per second)\nllama_perf_context_print:        load time =    4644.94 ms\nllama_perf_context_print: prompt eval time =    1108.02 ms /   241 tokens (    4.60 ms per token,   217.51 tokens per second)\nllama_perf_context_print:        eval time =   35939.52 ms /   199 runs   (  180.60 ms per token,     5.54 tokens per second)\nllama_perf_context_print:       total time =   37087.24 ms /   440 tokens\nllama_perf_context_print:  mean exit layer =      27.80\nllama_perf_context_print:       exit ratio =       0.50  = 101 / 201 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 29, 32, 32, 32, 31, 32, 16, 16, 32, 27, 25, 25, 32, 17, 32, 17, 32, 20, 32, 22, 32, 25, 27, 32, 32, 32, 32, 16, 32, 32, 22, 32, 22, 22, 32, 17, 32, 32, 32, 25, 15, 32, 32, 32, 17, 32, 32, 32, 15, 32, 22, 32, 32, 32, 32, 32, 32, 27, 32, 32, 32, 32, 32, 22, 18, 27, 32, 18, 30, 32, 32, 32, 32, 32, 29, 29, 29, 22, 32, 24, 16, 32, 17, 32, 24, 32, 32, 29, 29, 30, 32, 31, 27, 32, 22, 31, 16, 32, 32, 32, 29, 32, 32, 25, 31, 32, 31, 32, 18, 32, 32, 27, 32, 32, 32, 20, 27, 27, 20, 32, 27, 30, 32, 22, 25, 32, 22, 29, 16, 32, 31, 32, 32, 18, 30, 29, 25, 32, 25, 23, 32, 29, 32, 25, 32, 17, 32, 32, 32, 32, 15, 32, 22, 32, 25, 20, 17, 32, 32, 32, 17, 32, 27, 32, 32, 32, 17, 32, 25, 29, 19, 32, 30, 25, 27, 20, 27, 18, 32, 32, 17, 32, 29, 29, 25, 18, 31, 25, 27, 32, 27, 29, 16, 32, 20, }\n"
    },
    {
        "inference_time": "33162.36",
        "tokens": "529",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: This video shows the horrifying moment a three-legged cat was mauled to death by two savage dogs in a brutal attack. The disabled cat, called Freeman, was sitting in its front garden when the two dogs pounced, dragging the terrified cat on to its neighbour's lawn before tearing into the family pet. Police are now investigating after a person, who appears to be a woman, pulls up in a blue car and collects the hounds - leaving the cat to die in agony. Scroll down for video. Mauled to death: Two dogs tore apart a three-legged cat in a brutal attack captured on CCTV. Family pet: Freeman the cat died in the neighbour's front garden in Tarring, West Sussex, after the brutal attack. A woman (top of picture, exiting a blue car) appears to call over the dogs before walking off, leaving the disabled cat to die in agony. The 90-second attack was captured on CCTV by a neighbour who passed the footage on to the cat's owner Tracy Lynch, from Tarring, West Sussex, and the police. Ms Lynch, a 44-year-old mother-of -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: This video shows the horrifying moment a three-legged cat was mauled to death by two savage dogs in a brutal attack. The disabled cat, called Freeman, was sitting in its front garden when the two dogs pounced, dragging the terrified cat on to its neighbour's lawn before tearing into the family pet. Police are now investigating after a person, who appears to be a woman, pulls up in a blue car and collects the hounds - leaving the cat to die in agony. Scroll down for video. Mauled to death: Two dogs tore apart a three-legged cat in a brutal attack captured on CCTV. Family pet: Freeman the cat died in the neighbour's front garden in Tarring, West Sussex, after the brutal attack. A woman (top of picture, exiting a blue car) appears to call over the dogs before walking off, leaving the disabled cat to die in agony. The 90-second attack was captured on CCTV by a neighbour who passed the footage on to the cat's owner Tracy Lynch, from Tarring, West Sussex, and the police. Ms Lynch, a 44-year-old mother-of-two, said: Freeman was a lovely little cat who was always friendly and playful. Freeman was three-legged cat who was attacked by two dogs attack in a brutal attack in Tarring, West Sussex. Freeman was a lovely little bit who was always friendly and playful. Freeman was a three-legged cat who was attacked by two dogs in a brutal attack in Tarring, West Sussex. Freeman was a lovely little bit who was always friendly and playful. Freeman was a three- leg cat who was attacked by two dogs in a brutal attack in Tarring, West Sussex. Freeman was a lovely little bit who was always friendly and playful. Freeman was a three-legged cat who was attacked by two dogs in a brutal attack in Tarring, West Sussex. Freeman was a lovely little bit who was always friendly and playful. Freeman was a three- legged cat who was attacked by two dogs in a brutal attack in Tarring, West Susshire. Freeman was a lovely little bit who was always friendly and playful. Freeman was a three-legged cat who was attacked by two dogs in\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.90 ms /   530 runs   (    0.01 ms per token, 89769.65 tokens per second)\nllama_perf_context_print:        load time =    4474.77 ms\nllama_perf_context_print: prompt eval time =    1109.62 ms /   274 tokens (    4.05 ms per token,   246.93 tokens per second)\nllama_perf_context_print:        eval time =   32003.39 ms /   255 runs   (  125.50 ms per token,     7.97 tokens per second)\nllama_perf_context_print:       total time =   33162.36 ms /   529 tokens\nllama_perf_context_print:  mean exit layer =      24.05\nllama_perf_context_print:       exit ratio =       0.66  = 169 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 27, 22, 32, 25, 23, 29, 30, 32, 32, 17, 32, 25, 32, 32, 32, 27, 30, 18, 25, 25, 32, 22, 29, 31, 24, 17, 24, 32, 18, 32, 17, 32, 18, 27, 32, 17, 32, 25, 22, 32, 32, 23, 29, 21, 32, 18, 29, 25, 18, 18, 32, 29, 18, 32, 18, 32, 22, 18, 32, 16, 32, 17, 32, 25, 18, 15, 32, 30, 32, 22, 32, 22, 24, 18, 32, 17, 32, 19, 32, 17, 32, 25, 18, 32, 27, 22, 18, 22, 20, 27, 24, 25, 20, 17, 15, 32, 22, 32, 18, 32, 18, 18, 32, 17, 32, 18, 18, 25, 18, 32, 16, 32, 18, 25, 25, 18, 18, 32, 16, 32, 20, 18, 17, 32, 22, 18, 29, 32, 25, 16, 22, 20, 32, 17, 32, 22, 16, 14, 32, 20, 32, 17, 32, 18, 18, 32, 16, 32, 17, 16, 32, 22, 14, 14, 32, 16, 32, 16, 20, 17, 18, 32, 14, 32, 20, 16, 18, 32, 22, 18, 14, 32, 25, 14, 32, 20, 27, 16, 32, 22, 18, 14, 32, 17, 32, 15, 32, 18, 18, 32, 16, 32, 16, 16, 32, 25, 17, 32, 18, 32, 23, 18, 32, 17, 32, 32, 15, 32, 20, 15, 17, 32, 22, 18, 32, 32, 25, 16, 32, 22, 20, 20, 24, 22, 17, 15, 32, 16, 32, 15, 32, 18, 18, 32, 17, 32, 17, 17, 32, 25, 18, 32, 19, 18, 32, 17, 32, 17, 32, 32, 15, 32, 20, 15, }\n"
    },
    {
        "inference_time": "34130.71",
        "tokens": "492",
        "command": "./llama-cli -m models/SpecEE-7b-chat-hf.gguf -p Summarize: A Chinese woman has become an internet sensation after charting  life with her pet pig who she dresses, walks and even sleeps with every day. The woman, who calls herself Zhu Diandian online, has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs (13st) But that has nothing to weaken the bond between the two, with Ms Zhu proudly posting pictures  of them snuggled up in bed together, it was reported by People's Daily Online. Her husband is understood to be 'tolerant' of the relationship. However the same can't be said of her pet dog, who often peers enviously at the pig through a baby gate that keeps the mutt out of the bedroom. Lights snout:  Zhu Diandian snuggles up with her 190lb pig who she shares a bed with every day. Snuggles: Ms Zhu has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs, but that has done nothing to weaken the bond between the two. The owner's surname, Zhu, sounds exactly -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A Chinese woman has become an internet sensation after charting  life with her pet pig who she dresses, walks and even sleeps with every day. The woman, who calls herself Zhu Diandian online, has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs (13st) But that has nothing to weaken the bond between the two, with Ms Zhu proudly posting pictures  of them snuggled up in bed together, it was reported by People's Daily Online. Her husband is understood to be 'tolerant' of the relationship. However the same can't be said of her pet dog, who often peers enviously at the pig through a baby gate that keeps the mutt out of the bedroom. Lights snout:  Zhu Diandian snuggles up with her 190lb pig who she shares a bed with every day. Snuggles: Ms Zhu has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs, but that has done nothing to weaken the bond between the two. The owner's surname, Zhu, sounds exactly the same as the Chinese word for ' pig', which has led to speculation that the pet pig was named after its own species.\n\nReaction:\n\nThis article is quite interesting and funny because it highlights how unusual it is for a Chinese woman to have a pet p Hog. The fact that she dresses, walks, and sleeps with her pig everyday is quite unusual and has made her an internet sensation. The fact that she also has a dog that peers enviously at the pig through a baby gate is also added humor to this article. The fact that her name name Zhu Diandian means \"pig\" in Chinese language is also a funny twist to this story.\n\nThe article also raises questions about whether it is ethical to keep a pig as a pet, especially one that size. It also raises questions whether it is healthy for both the pig and the owner to share a bedroom.\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 430 tensors from models/SpecEE-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = eauto\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = EAuto\nllama_model_loader: - kv   3:                           general.finetune str              = chat-eauto\nllama_model_loader: - kv   4:                           general.basename str              = Llama-2\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = llama2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          eauto.block_count u32              = 32\nllama_model_loader: - kv  10:                       eauto.context_length u32              = 4096\nllama_model_loader: - kv  11:                     eauto.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  eauto.feed_forward_length u32              = 11008\nllama_model_loader: - kv  13:                 eauto.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              eauto.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  15:     eauto.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                          general.file_type u32              = 1\nllama_model_loader: - kv  17:                           eauto.vocab_size u32              = 32000\nllama_model_loader: - kv  18:                 eauto.rope.dimension_count u32              = 128\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  163 tensors\nllama_model_loader: - type  f16:  267 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = eauto\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 7.11 B\nllm_load_print_meta: model size       = 13.24 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = EAuto\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 13553.45 MiB\nllm_load_tensors:      CUDA0 buffer size =  7376.80 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  1296.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   544.02 MiB\nllama_new_context_with_model: graph nodes  = 1535\nllama_new_context_with_model: graph splits = 241\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.71 ms /   493 runs   (    0.01 ms per token, 104737.62 tokens per second)\nllama_perf_context_print:        load time =    4480.54 ms\nllama_perf_context_print: prompt eval time =    1115.48 ms /   286 tokens (    3.90 ms per token,   256.39 tokens per second)\nllama_perf_context_print:        eval time =   32975.25 ms /   206 runs   (  160.07 ms per token,     6.25 tokens per second)\nllama_perf_context_print:       total time =   34130.71 ms /   492 tokens\nllama_perf_context_print:  mean exit layer =      26.50\nllama_perf_context_print:       exit ratio =       0.61  = 126 / 208 \nllama_perf_context_print:       exit layer = {32, 32, 20, 20, 27, 22, 16, 32, 32, 30, 30, 32, 20, 27, 25, 30, 32, 32, 28, 32, 26, 32, 25, 32, 32, 22, 32, 16, 32, 27, 32, 32, 32, 32, 29, 31, 31, 32, 18, 32, 32, 32, 32, 32, 32, 27, 18, 32, 16, 16, 27, 32, 27, 27, 29, 25, 20, 20, 31, 20, 22, 22, 24, 32, 32, 20, 20, 18, 32, 17, 32, 29, 18, 32, 16, 32, 32, 16, 32, 22, 27, 32, 16, 32, 32, 18, 29, 32, 31, 32, 32, 20, 32, 29, 29, 31, 16, 25, 18, 17, 31, 22, 22, 32, 24, 22, 24, 32, 18, 32, 24, 25, 32, 17, 32, 22, 32, 15, 18, 32, 29, 18, 22, 29, 32, 17, 32, 20, 18, 18, 32, 20, 32, 25, 18, 32, 32, 32, 25, 27, 27, 22, 27, 32, 16, 32, 27, 17, 32, 32, 29, 17, 32, 29, 32, 29, 32, 22, 32, 30, 32, 17, 32, 18, 32, 27, 32, 32, 30, 32, 29, 30, 25, 23, 27, 22, 32, 17, 32, 32, 17, 32, 30, 18, 32, 27, 17, 17, 32, 22, 32, 24, 22, 18, 32, 32, 25, 25, 29, 20, 27, 32, 32, 17, 19, 32, 32, 15, }\n"
    }
]