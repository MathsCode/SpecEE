[
    {
        "Model": "llama",
        "Inference time": "3760466.29 ms",
        "Tokens": "36657",
        "Tokens per second": "9.75"
    },
    {
        "inference_time": "17095.49",
        "tokens": "355",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Hillary Clinton\u2019s security detail arrived at a suburban Des Moines, Iowa fruit processing company on Tuesday with an added vehicle \u2013 a second Scooby. After her signature oversize black Chevy conversion van dropped her off at Capitol Fruit Company in Norwalk, Iowa, a visually identical GMC van drove up to the building with a nearly identical Secret Service escort vehicle. Both armored vehicles have raised roofs, deep-tinted windows and New York license plates. But while the original van \u2013 the one nicknamed 'Scooby' after the Scooby-Doo cartoon show \u2013 sports a mustard-yellow New York tag, the second has blue and white plates of a different design. Scroll down for video. WHY BUY ONE WHEN YOU CAN HAVE TWO AT TWICE THE PRICE? The first picture of both of Hillary Clinton's Scooby mobiles. One is a GMC and the other is a Chevrolet, but they are mechanically identical. CONVOY: Scooby-one and Scooby-two took up positions in Hillary's motorcade on a freeway near Des Moines. BACK SEAT DRIVER? Hillary Clinton  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Hillary Clinton\u2019s security detail arrived at a suburban Des Moines, Iowa fruit processing company on Tuesday with an added vehicle \u2013 a second Scooby. After her signature oversize black Chevy conversion van dropped her off at Capitol Fruit Company in Norwalk, Iowa, a visually identical GMC van drove up to the building with a nearly identical Secret Service escort vehicle. Both armored vehicles have raised roofs, deep-tinted windows and New York license plates. But while the original van \u2013 the one nicknamed 'Scooby' after the Scooby-Doo cartoon show \u2013 sports a mustard-yellow New York tag, the second has blue and white plates of a different design. Scroll down for video. WHY BUY ONE WHEN YOU CAN HAVE TWO AT TWICE THE PRICE? The first picture of both of Hillary Clinton's Scooby mobiles. One is a GMC and the other is a Chevrolet, but they are mechanically identical. CONVOY: Scooby-one and Scooby-two took up positions in Hillary's motorcade on a freeway near Des Moines. BACK SEAT DRIVER? Hillary Clinton  was seen in the back seat of Scooby-one, the original van, as it drove through the Iowa countryside. The second Scooby, Scooby-two, was seen in the background of this picture, parked nearby. The two vans are mechanically identical, but the second one has different license plates.\n\n\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.64 ms /   356 runs   (    0.00 ms per token, 216676.81 tokens per second)\nllama_perf_context_print:        load time =    5572.54 ms\nllama_perf_context_print: prompt eval time =    1007.65 ms /   280 tokens (    3.60 ms per token,   277.88 tokens per second)\nllama_perf_context_print:        eval time =   16072.24 ms /    75 runs   (  214.30 ms per token,     4.67 tokens per second)\nllama_perf_context_print:       total time =   17095.49 ms /   355 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 77 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55952.32",
        "tokens": "506",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: After a fire took a family's home of four years leaving them without anything, a Georgia mother of six is making a desperate plea for help. Kalimah Dixon said she was awakened by a neighbor yelling there was a fire early on Monday morning, and was lucky to get her children, aged one to 14, out of the Decatur condo safely. However, all that was left of her rented condo was a pile of rubble, and in her efforts to get her family back on their feet, she is asking for assistance. 'My children are homeless. They have nothing. They lost everything,' Dixon told Channel 2. Scroll down for video. A mother of six said her family has nothing after fire reduced their Georgia condo to a pile of rubble (above Kalimah Dixon with her five of her six children) The Dekalb County mother said the fire happened early on Monday and that she managed to get all of her children safely out (remains of the condo following the fire above) While Dixon was renting the Condo in the last four years, she had quit working to enroll  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: After a fire took a family's home of four years leaving them without anything, a Georgia mother of six is making a desperate plea for help. Kalimah Dixon said she was awakened by a neighbor yelling there was a fire early on Monday morning, and was lucky to get her children, aged one to 14, out of the Decatur condo safely. However, all that was left of her rented condo was a pile of rubble, and in her efforts to get her family back on their feet, she is asking for assistance. 'My children are homeless. They have nothing. They lost everything,' Dixon told Channel 2. Scroll down for video. A mother of six said her family has nothing after fire reduced their Georgia condo to a pile of rubble (above Kalimah Dixon with her five of her six children) The Dekalb County mother said the fire happened early on Monday and that she managed to get all of her children safely out (remains of the condo following the fire above) While Dixon was renting the Condo in the last four years, she had quit working to enroll 13-year-old daughter in a special needs school. 'I was a stay-at-home mom, and I was taking care of my daughter, who has special needs,' she said. 'I had to quit my job to take care of her, and now I have nothing.' Dixon said she has no insurance and no savings to help her family get back on their feet. 'I'm just trying to get my kids back to normalcy, and I don't know how I'm going to do it,' she said. 'I'm just trying to get them back in school, and get them back to their routine, and get them back to their lives.' Dixon said she is grateful for any help she can get, and that she is hoping to raise enough money to rent a new place to live. 'I'm just trying to get us back on our feet, and get us back to where we were before the fire happened,' she said. 'I'm just trying to get us back to normalcy.' Dixon said she is also hoping to raise awareness about the importance of having insurance and savings to help protect families from unexpected disasters. 'I never thought this\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.48 ms /   507 runs   (    0.01 ms per token, 92568.92 tokens per second)\nllama_perf_context_print:        load time =    3916.26 ms\nllama_perf_context_print: prompt eval time =    1001.02 ms /   251 tokens (    3.99 ms per token,   250.74 tokens per second)\nllama_perf_context_print:        eval time =   54901.74 ms /   255 runs   (  215.30 ms per token,     4.64 tokens per second)\nllama_perf_context_print:       total time =   55952.32 ms /   506 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56647.38",
        "tokens": "465",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows. Documents reveal the shocking details of the case that saw a couple return to Australia with a baby girl while leaving her twin brother behind. Australian government officials had full knowledge of the startling ordeal, according to documents obtained\u00a0the ABC. An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows (stock image) The FOI documents reportedly show staff at the Australian High Commission in India and Department of Foreign Affairs and Trade were aware the couple was from New South Wales, where it is illegal to enter into international surrogacy arrangements. Conversations between the Australian High Commission in New Delhi and Government officials in Canberra detail how the couple travelled to India in late 2012 and seeked citizenship for a baby -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows. Documents reveal the shocking details of the case that saw a couple return to Australia with a baby girl while leaving her twin brother behind. Australian government officials had full knowledge of the startling ordeal, according to documents obtained\u00a0the ABC. An Australian couple who abandoned a boy born in a surrogacy deal in India were repeatedly told the child would be left stateless, new evidence shows (stock image) The FOI documents reportedly show staff at the Australian High Commission in India and Department of Foreign Affairs and Trade were aware the couple was from New South Wales, where it is illegal to enter into international surrogacy arrangements. Conversations between the Australian High Commission in New Delhi and Government officials in Canberra detail how the couple travelled to India in late 2012 and seeked citizenship for a baby boy born through surrogacy. The documents reveal the couple was informed the baby would be stateless, but they continued with the process and returned to Australia with a baby girl in 2013. The couple's whereabouts are currently unknown, but the documents suggest they may have abandoned the boy in India. The case has sparked outrage in Australia, with many calling for the couple to be held accountable for their actions. The Australian government has been urged to take action to ensure the boy is protected and given the right to citizenship. The case has also raised questions about the need for better regulation of surrogacy arrangements in India and the need for greater transparency and accountability in the process. The Australian government has been contacted for comment.\n\nThe Australian couple who abandoned a baby boy born through surrogacy in India were repeatedly told the child would be left stateless, new evidence has revealed. Documents obtained by the ABC show that officials at the Australian High Commission in India and the Department of Foreign Affairs and Trade were aware of the couple's actions. The couple, who were from New South Wales, where it is illegal to enter into international surrogacy arrangements, travelled to India\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.58 ms /   466 runs   (    0.01 ms per token, 83482.62 tokens per second)\nllama_perf_context_print:        load time =    3868.28 ms\nllama_perf_context_print: prompt eval time =     988.02 ms /   210 tokens (    4.70 ms per token,   212.55 tokens per second)\nllama_perf_context_print:        eval time =   55609.26 ms /   255 runs   (  218.08 ms per token,     4.59 tokens per second)\nllama_perf_context_print:       total time =   56647.38 ms /   465 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "4362.25",
        "tokens": "284",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: We all need a helping hand sometimes, and nobody more so than Kecil the baby orangutan, who had a tough start to life after his natural mother and another surrogate mother rejected him. But the one-year-old ape found love at the third time of asking with second surrogate mother Maggie at Chicago's Brookfield Zoo, and has today been pictured cuddling up to the experienced mother ahead of his first public outing this weekend. Maggie, 53, who has already raised one surrogate child, has proved herself yet again by helping to raise Kecil (pronounced Ka-cheel) for the past 10 months, and the two now have an inseparable bond. Scroll down for video. Taking care: Maggie, 53, wraps a protective arm around Kecil the baby orangutan who she has been raising for the past ten months after his mother and a surrogate mother both rejected him. Inseparable: Zookeepers were nervous that Maggie might reject Kecil after raising four of her own children and another surrogate which was given to her in 2002, but she has pr -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: We all need a helping hand sometimes, and nobody more so than Kecil the baby orangutan, who had a tough start to life after his natural mother and another surrogate mother rejected him. But the one-year-old ape found love at the third time of asking with second surrogate mother Maggie at Chicago's Brookfield Zoo, and has today been pictured cuddling up to the experienced mother ahead of his first public outing this weekend. Maggie, 53, who has already raised one surrogate child, has proved herself yet again by helping to raise Kecil (pronounced Ka-cheel) for the past 10 months, and the two now have an inseparable bond. Scroll down for video. Taking care: Maggie, 53, wraps a protective arm around Kecil the baby orangutan who she has been raising for the past ten months after his mother and a surrogate mother both rejected him. Inseparable: Zookeepers were nervous that Maggie might reject Kecil after raising four of her own children and another surrogate which was given to her in 2002, but she has proudbly proved herself once again.\n\n\n\n\n\n\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.35 ms /   285 runs   (    0.00 ms per token, 805084.75 tokens per second)\nllama_perf_context_print:        load time =    3879.83 ms\nllama_perf_context_print: prompt eval time =    1106.13 ms /   269 tokens (    4.11 ms per token,   243.19 tokens per second)\nllama_perf_context_print:        eval time =    3251.26 ms /    15 runs   (  216.75 ms per token,     4.61 tokens per second)\nllama_perf_context_print:       total time =    4362.25 ms /   284 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 17 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "38496.62",
        "tokens": "439",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Paris Saint-Germain star Javier Pastore has dismissed Eric Cantona\u2019s suggestion that he is the greatest footballer on the planet. Cantona, the former Manchester United forward, claimed earlier this month that Pastore is \u2018the best player in the world\u2019. But speaking ahead of his side\u2019s second-leg tie against Barcelona in the Nou Camp, Pastore believes Lionel Messi and Cristiano Ronaldo are a class above him. Javier Pastore addresses the world's media ahead of Paris Saint-Germain's European clash in Barcelona. Pastore and his Paris Saint-Germain team-mates are pictured at the Nou Camp on Monday ahead of their tie. \u2018Cantona has his opinion and I thank him for his praise. However, I don't share his view,\u2019 said Castore. \u2018I don't compare myself with the best. Cantona likes good football and goals, but I think Messi and Cristiano Ronaldo are incredible players.\u2019 Pastore scored a brace in PSG\u2019s 3-1 win at Nice on Saturday as the Parisians reclaimed top spot in Ligue 1. Speaking at the Laureus Sports Awards, -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Paris Saint-Germain star Javier Pastore has dismissed Eric Cantona\u2019s suggestion that he is the greatest footballer on the planet. Cantona, the former Manchester United forward, claimed earlier this month that Pastore is \u2018the best player in the world\u2019. But speaking ahead of his side\u2019s second-leg tie against Barcelona in the Nou Camp, Pastore believes Lionel Messi and Cristiano Ronaldo are a class above him. Javier Pastore addresses the world's media ahead of Paris Saint-Germain's European clash in Barcelona. Pastore and his Paris Saint-Germain team-mates are pictured at the Nou Camp on Monday ahead of their tie. \u2018Cantona has his opinion and I thank him for his praise. However, I don't share his view,\u2019 said Castore. \u2018I don't compare myself with the best. Cantona likes good football and goals, but I think Messi and Cristiano Ronaldo are incredible players.\u2019 Pastore scored a brace in PSG\u2019s 3-1 win at Nice on Saturday as the Parisians reclaimed top spot in Ligue 1. Speaking at the Laureus Sports Awards, Cantona said: 'Pastore is the best player in the world. He has everything. He has speed, power, technique, vision, and he scores goals. He is the complete player.' But Pastore, who has scored 13 goals in 24 appearances for PSG this season, insists he is not in the same league as Messi and Ronaldo. 'I don't compare myself with the best. Messi and Ronaldo are incredible players. They have won everything and have been at the top for many years. I have a lot of respect for them and I am happy to play in the same league as them.' PSG face Barcelona in the second leg of their Champions League last-16 tie on Tuesday night, with the tie finely poised at 1-1. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.76 ms /   440 runs   (    0.01 ms per token, 117114.72 tokens per second)\nllama_perf_context_print:        load time =    3925.07 ms\nllama_perf_context_print: prompt eval time =    1037.43 ms /   266 tokens (    3.90 ms per token,   256.40 tokens per second)\nllama_perf_context_print:        eval time =   37424.32 ms /   173 runs   (  216.33 ms per token,     4.62 tokens per second)\nllama_perf_context_print:       total time =   38496.62 ms /   439 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 175 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "15953.26",
        "tokens": "336",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Campaign:\u00a0Amanda Burleigh has won her battle. A midwife has won a decade-long battle to give mothers more time attached to their newborns before the umbilical cord is cut. Amanda Burleigh was convinced that clamping the cord within seconds of delivery \u2013 shutting off the blood supply from the placenta \u2013 was wrong. Since the 1950s, when doctors and midwives started giving women a hormonal injection to reduce the risk of haemorrhage, they have clamped the cord within seconds of birth. They feared the drug \u2013 since replaced with a safer substitute \u2013 could harm the baby. But Ms Burleigh, a midwife at Leeds Teaching Hospitals NHS Trust, believed it was unnatural to clamp the cord when it was still visibly pulsating with blood. She said: \u2018One day I had a brainwave and thought, \u201cWe all clamp the cord within seconds of the baby being born, but we are doing it without any evidence about whether it\u2019s good or bad for the baby.\u201d' She contacted other medics, including obstetrician Dr David Hutchon, and together t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Campaign:\u00a0Amanda Burleigh has won her battle. A midwife has won a decade-long battle to give mothers more time attached to their newborns before the umbilical cord is cut. Amanda Burleigh was convinced that clamping the cord within seconds of delivery \u2013 shutting off the blood supply from the placenta \u2013 was wrong. Since the 1950s, when doctors and midwives started giving women a hormonal injection to reduce the risk of haemorrhage, they have clamped the cord within seconds of birth. They feared the drug \u2013 since replaced with a safer substitute \u2013 could harm the baby. But Ms Burleigh, a midwife at Leeds Teaching Hospitals NHS Trust, believed it was unnatural to clamp the cord when it was still visibly pulsating with blood. She said: \u2018One day I had a brainwave and thought, \u201cWe all clamp the cord within seconds of the baby being born, but we are doing it without any evidence about whether it\u2019s good or bad for the baby.\u201d' She contacted other medics, including obstetrician Dr David Hutchon, and together t...\n\nQuestion: What is the main issue that Amanda Burleigh is trying to address?\n\nAnswer: The main issue that Amanda Burleigh is trying to address is the practice of clamping the umbilical cord within seconds of birth, without any evidence to support its safety or effectiveness for the baby. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.51 ms /   337 runs   (    0.00 ms per token, 223031.11 tokens per second)\nllama_perf_context_print:        load time =    3863.23 ms\nllama_perf_context_print: prompt eval time =    1012.41 ms /   267 tokens (    3.79 ms per token,   263.73 tokens per second)\nllama_perf_context_print:        eval time =   14925.66 ms /    69 runs   (  216.31 ms per token,     4.62 tokens per second)\nllama_perf_context_print:       total time =   15953.26 ms /   336 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 71 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "42822.77",
        "tokens": "421",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Francois Hollande has vowed to 'show no mercy' if French peacekeepers are found guilty of sexual assaulting starving children in the Central African Republic. Fourteen soldiers dispatched to restore order after a 2013 coup have been accused of abusing children as young as nine as they begged for something to eat, according\u00a0to a French judicial source. The investigation has been underway since last year but was only made public yesterday. The French president today said: 'If some soldiers have behaved badly, I will show no mercy.' President Francois Hollande has vowed to 'show no mercy' if French peacekeepers in Central African Republic are found guilty of raping hungry children in exchange for food. The French defence ministry has vehemently denied covering up the scandal following revelations it was made aware of the allegations in July last year when it received a leaked UN report on the subject. The ministry said it immediately launched a probe into the case when it received the news, sending in -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Francois Hollande has vowed to 'show no mercy' if French peacekeepers are found guilty of sexual assaulting starving children in the Central African Republic. Fourteen soldiers dispatched to restore order after a 2013 coup have been accused of abusing children as young as nine as they begged for something to eat, according\u00a0to a French judicial source. The investigation has been underway since last year but was only made public yesterday. The French president today said: 'If some soldiers have behaved badly, I will show no mercy.' President Francois Hollande has vowed to 'show no mercy' if French peacekeepers in Central African Republic are found guilty of raping hungry children in exchange for food. The French defence ministry has vehemently denied covering up the scandal following revelations it was made aware of the allegations in July last year when it received a leaked UN report on the subject. The ministry said it immediately launched a probe into the case when it received the news, sending in a team of investigators to the country to gather evidence. The investigation is ongoing and no charges have been filed yet.\n\nFrancois Hollande has vowed to show no mercy if French peacekeepers are found guilty of sexually assaulting starving children in the Central African Republic. The French president made the statement after an investigation was launched into allegations that 14 French soldiers deployed to the country in 2013 to restore order after a coup had sexually assaulted children as young as nine in exchange for food. The investigation was only made public yesterday, but Hollande has made it clear that he will not tolerate any wrongdoing by French peacekeepers. The French defense ministry has denied covering up the scandal and has stated that it immediately launched an investigation into the allegations when it received the news last year. The investigation is ongoing, and no charges have been filed yet. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.18 ms /   422 runs   (    0.01 ms per token, 100981.10 tokens per second)\nllama_perf_context_print:        load time =    3862.89 ms\nllama_perf_context_print: prompt eval time =    1004.21 ms /   226 tokens (    4.44 ms per token,   225.05 tokens per second)\nllama_perf_context_print:        eval time =   41780.90 ms /   195 runs   (  214.26 ms per token,     4.67 tokens per second)\nllama_perf_context_print:       total time =   42822.77 ms /   421 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 197 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57049.89",
        "tokens": "511",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A shocking 'resurrection ceremony' for a  two-year-old dead boy at a Texas church has been caught on camera. In the clip capturing the attempted resurrection, the boy identified by a witness as Benjamin, is being held in the arms of Texas pastor's wife Aracely Meza as others, including a man who appears to be her husband Pastor Daniel Meza, are gathered around. The witness, who said she knew Benjamin's mother since she was pregnant with him, said the child was possessed by demons, according to pastors. A shocking 'resurrection ceremony' for a two-year-old dead boy at a Texas church has been caught on camera. Aracely Meza, who is reportedly not the child's mother, was arrested on Monday and has been charged with injury to a child by omission, said police (above Meza in the video) In the video, Aracely Meza is seen next to a man who appears to be her husband Pastor Daniel Meza, as she uses oils and prays while trying to bring the boy, identified by a witness as Benjamin, back to life (above scenes fr -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A shocking 'resurrection ceremony' for a  two-year-old dead boy at a Texas church has been caught on camera. In the clip capturing the attempted resurrection, the boy identified by a witness as Benjamin, is being held in the arms of Texas pastor's wife Aracely Meza as others, including a man who appears to be her husband Pastor Daniel Meza, are gathered around. The witness, who said she knew Benjamin's mother since she was pregnant with him, said the child was possessed by demons, according to pastors. A shocking 'resurrection ceremony' for a two-year-old dead boy at a Texas church has been caught on camera. Aracely Meza, who is reportedly not the child's mother, was arrested on Monday and has been charged with injury to a child by omission, said police (above Meza in the video) In the video, Aracely Meza is seen next to a man who appears to be her husband Pastor Daniel Meza, as she uses oils and prays while trying to bring the boy, identified by a witness as Benjamin, back to life (above scenes fr om the church) The witness, who wishes to remain anonymous, told DailyMail.com that Benjamin's mother had been a member of the church for several years and had been struggling with drug addiction. The witness said that Benjamin was born prematurely and had several health problems, including a heart condition. The witness said that Benjamin died in 2019, but his mother continued to attend church and believed that he was still alive. The witness said that the pastors at the church believed that Benjamin was possessed by demons and that they were trying to perform a resurrection ceremony to save him. The witness said that the ceremony involved Aracely Meza using oils and praying while holding Benjamin. The witness said that the ceremony lasted for several hours and that Benjamin never showed any signs of coming back to life. The witness said that after the ceremony, Benjamin's body was taken away in a black bag. The witness said that she was shocked and saddened by the incident and that she did not believe that the pastors had any right to try to bring Benjamin back to life. The witness said that she had known Benjamin's mother since she was pregnant with him and that she had seen him several times at\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.48 ms /   512 runs   (    0.01 ms per token, 93362.51 tokens per second)\nllama_perf_context_print:        load time =    3895.58 ms\nllama_perf_context_print: prompt eval time =     992.56 ms /   256 tokens (    3.88 ms per token,   257.92 tokens per second)\nllama_perf_context_print:        eval time =   56008.38 ms /   255 runs   (  219.64 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   57049.89 ms /   511 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "46940.24",
        "tokens": "482",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Crucible thoroughbreds John Higgins and Graeme Dott led the Scottish charge on Sunday at the Betfred World Championship. Four-time winner Higgins impressed with a 10-5 first-round victory over Robert Milkins, and looked to be approaching the form that last saw him take the title four years ago. The 39-year-old fired breaks of 77, 75, 69 and a clinching 106 in the second session as he coasted home against Gloucester potter Milkins, who took the match beyond its evening interval but was thoroughly outplayed. John Higgins plays a shot during his match against Robert Milkins during the Betfred World Championships. Milkins said of his conqueror: 'It's going to take a good man to beat him on that form. 'He's playing really good stuff. I think I did well to get five frames.' Dott, unseeded this year but a proven Crucible class act and champion in 2006, was a 10-8 winner earlier as he ousted Ricky Walden. Higgins has sprung back to competitive form level this year, notably in taking the Welsh Open title in -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Crucible thoroughbreds John Higgins and Graeme Dott led the Scottish charge on Sunday at the Betfred World Championship. Four-time winner Higgins impressed with a 10-5 first-round victory over Robert Milkins, and looked to be approaching the form that last saw him take the title four years ago. The 39-year-old fired breaks of 77, 75, 69 and a clinching 106 in the second session as he coasted home against Gloucester potter Milkins, who took the match beyond its evening interval but was thoroughly outplayed. John Higgins plays a shot during his match against Robert Milkins during the Betfred World Championships. Milkins said of his conqueror: 'It's going to take a good man to beat him on that form. 'He's playing really good stuff. I think I did well to get five frames.' Dott, unseeded this year but a proven Crucible class act and champion in 2006, was a 10-8 winner earlier as he ousted Ricky Walden. Higgins has sprung back to competitive form level this year, notably in taking the Welsh Open title in March, and he is now on a 12-match unbeaten run at the Crucible. He said: 'I'm playing well, I'm happy with my game and I'm enjoying myself. 'I've got a good chance of going far in this tournament, but I know it's not going to be easy.' Higgins has been in this position before, having won the World Championship in 1998, 2001, 2007 and 2009, and he is determined to make the most of his current form. 'I've been in this position before and I know what it takes to win it,' he said. 'I've got to keep my focus and my concentration, and I've got to keep playing the way I am.' Higgins will face either Mark Allen or Stephen Maguire in the last 16, while Dott will take on Shaun Murphy or Barry Hawkins. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.56 ms /   483 runs   (    0.01 ms per token, 105921.05 tokens per second)\nllama_perf_context_print:        load time =    3891.50 ms\nllama_perf_context_print: prompt eval time =    1005.94 ms /   271 tokens (    3.71 ms per token,   269.40 tokens per second)\nllama_perf_context_print:        eval time =   45893.32 ms /   211 runs   (  217.50 ms per token,     4.60 tokens per second)\nllama_perf_context_print:       total time =   46940.24 ms /   482 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 213 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "43636.31",
        "tokens": "464",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Barcelona stars Neymar, Dani Alves and Adriano looked delighted following their vital win over Celta Vigo in La Liga - but their choice of matching denim was somewhat questionable. The flamboyant Brazilians all played their part in helping the Catalonian side return home with all three points, following Jeremy Mathieu's winning goal. Neymar took to Instagram after his side returned to the summit of La Liga: 'We got home...important victory...Congratulations guys!!! Vamo to top.' Dani Alves (from left), Adriano and Neymar were delighted with their hard-fought victory over Celta Vigo. Dani Alves, who's being linked with a move away from the Nou Camp, also posted a similar picture of the trio. He said: 'A great day at work, a little trairagem in the picture lol Return to home with my brothas @neymarjr @adriano21c. Joy, joy , one is that every day.' Game winner Mathieu admits his side were poor and puts it down to the international break. The three Brazilians all started for Barcelona and played a cruc -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Barcelona stars Neymar, Dani Alves and Adriano looked delighted following their vital win over Celta Vigo in La Liga - but their choice of matching denim was somewhat questionable. The flamboyant Brazilians all played their part in helping the Catalonian side return home with all three points, following Jeremy Mathieu's winning goal. Neymar took to Instagram after his side returned to the summit of La Liga: 'We got home...important victory...Congratulations guys!!! Vamo to top.' Dani Alves (from left), Adriano and Neymar were delighted with their hard-fought victory over Celta Vigo. Dani Alves, who's being linked with a move away from the Nou Camp, also posted a similar picture of the trio. He said: 'A great day at work, a little trairagem in the picture lol Return to home with my brothas @neymarjr @adriano21c. Joy, joy , one is that every day.' Game winner Mathieu admits his side were poor and puts it down to the international break. The three Brazilians all started for Barcelona and played a crucila role in the win.\n\nReaction: \n\nThe article is about Barcelona's victory over Celta Vigo in La Liga, where the team's three Brazilian players, Neymar, Dani Alves, and Adriano, looked delighted with the win. However, their choice of matching denim outfits was met with some criticism. The article highlights Neymar's Instagram post, where he congratulates his teammates and expresses their joy at returning home with all three points. Dani Alves also posted a similar picture of the trio, with a similar message. Mathieu, who scored the winning goal, admits that Barcelona were poor in the game and attributes it to the international break. Overall, the article focuses on the team's victory and the performances of their Brazilian players, while also mentioning Dani Alves' potential move away from Barcelona. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.43 ms /   465 runs   (    0.01 ms per token, 104989.84 tokens per second)\nllama_perf_context_print:        load time =    3904.36 ms\nllama_perf_context_print: prompt eval time =    1004.48 ms /   267 tokens (    3.76 ms per token,   265.81 tokens per second)\nllama_perf_context_print:        eval time =   42592.63 ms /   197 runs   (  216.21 ms per token,     4.63 tokens per second)\nllama_perf_context_print:       total time =   43636.31 ms /   464 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 199 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55630.72",
        "tokens": "521",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Chelsea boss Jose Mourinho is planning a summer swoop for Atletico Madrid midfielder Koke. The club's technical director Michael Emenalo travelled to Spain on Saturday to watch the 23-year-old during Atletico's 2-1 victory against Deportivo, according to Spanish newspaper AS. The highly-rated Spaniard signed a long-term deal with Diego Simeone's side only last year, but Mourinho is reportedly planning to offer second-choice left-back Filipe Luis to his former club as a makeweight for the deal. Chelsea boss Jose Mourinho could be planning a summer swoop for Atletico Madrid midfielder Koke (above) Spanish newspaper AS report that Mourinho could offer Filipe Luis in a part-exchange deal this summer. Luis has struggled to make an impact at Stamford Bridge this season after failing to displace Spanish defender Cesar Azpilicueta from the left-back spot. AS also report that Koke's release clause is around \u00a350million which Chelsea could be willing to shell out as Mourinho looks to add to a midfield that ha -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Chelsea boss Jose Mourinho is planning a summer swoop for Atletico Madrid midfielder Koke. The club's technical director Michael Emenalo travelled to Spain on Saturday to watch the 23-year-old during Atletico's 2-1 victory against Deportivo, according to Spanish newspaper AS. The highly-rated Spaniard signed a long-term deal with Diego Simeone's side only last year, but Mourinho is reportedly planning to offer second-choice left-back Filipe Luis to his former club as a makeweight for the deal. Chelsea boss Jose Mourinho could be planning a summer swoop for Atletico Madrid midfielder Koke (above) Spanish newspaper AS report that Mourinho could offer Filipe Luis in a part-exchange deal this summer. Luis has struggled to make an impact at Stamford Bridge this season after failing to displace Spanish defender Cesar Azpilicueta from the left-back spot. AS also report that Koke's release clause is around \u00a350million which Chelsea could be willing to shell out as Mourinho looks to add to a midfield that ha already seen the likes of Nemanja Matic and Cesc Fabregas arrive at the club in recent months.\n\nJose Mourinho is planning to make a summer move for Atletico Madrid midfielder Koke, according to reports in Spain.\n\nThe Chelsea boss sent his technical director, Michael Emenalo, to watch the 23-year-old play for Atletico in their 2-1 win over Deportivo on Saturday.\n\nEmenalo was reportedly impressed by Koke's performance, and Mourinho is believed to be planning to offer second-choice left-back Filipe Luis to Atletico as part of a deal to sign the midfielder.\n\nKoke signed a long-term deal with Atletico last year, but Mourinho is willing to pay his release clause of around \u00a350million to bring him to Stamford Bridge.\n\nMourinho has already made several signings in the midfield department this season, including Nemanja Matic and Cesc Fabregas, and Koke would be the latest addition to the area.\n\nIt remains to be seen if Atletico will\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.48 ms /   522 runs   (    0.01 ms per token, 95290.25 tokens per second)\nllama_perf_context_print:        load time =    3930.81 ms\nllama_perf_context_print: prompt eval time =    1014.32 ms /   266 tokens (    3.81 ms per token,   262.24 tokens per second)\nllama_perf_context_print:        eval time =   54564.94 ms /   255 runs   (  213.98 ms per token,     4.67 tokens per second)\nllama_perf_context_print:       total time =   55630.72 ms /   521 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55924.44",
        "tokens": "535",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Tom Ballard plans to climb K2, 20 years after his mother died descending from its peak. A climber has set his sights on the summit of K2 - the second highest mountain in the world - 20 years after his mother died attempting the same feat. Alison Hargreaves, 32, was swept to her death in 260mph winds on her descent from the Himalayan mountain in 1995. Now her son Tom Ballard, one of the world's most accomplished climbers, plans to conquer K2 himself. Mr Ballard, 26, told The Times\u00a0that he saw it as his destiny to follow in his mother's footsteps. He said: 'I think it must be. At one stage I thought I would go and climb mountains for her. Then I realised that was actually a bit stupid because she had already climbed them herself. I realised I was climbing for myself. That's how I've gone on.' Last month Mr Ballard became the first to solo climb all six of the great north faces of the Alps in the winter season - and now he is training for K2. The climber was just six years old when his mother died whi -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Tom Ballard plans to climb K2, 20 years after his mother died descending from its peak. A climber has set his sights on the summit of K2 - the second highest mountain in the world - 20 years after his mother died attempting the same feat. Alison Hargreaves, 32, was swept to her death in 260mph winds on her descent from the Himalayan mountain in 1995. Now her son Tom Ballard, one of the world's most accomplished climbers, plans to conquer K2 himself. Mr Ballard, 26, told The Times\u00a0that he saw it as his destiny to follow in his mother's footsteps. He said: 'I think it must be. At one stage I thought I would go and climb mountains for her. Then I realised that was actually a bit stupid because she had already climbed them herself. I realised I was climbing for myself. That's how I've gone on.' Last month Mr Ballard became the first to solo climb all six of the great north faces of the Alps in the winter season - and now he is training for K2. The climber was just six years old when his mother died whi...\nTom Ballard, a renowned climber, plans to climb K2, the second highest mountain in the world, 20 years after his mother, Alison Hargreaves, died attempting to climb it. Ballard, who was just six years old when his mother died, has been training for the climb and has already accomplished several notable feats in the climbing world. Despite the tragic loss of his mother, Ballard sees his climb of K2 as a way to honor her memory and fulfill his own destiny as a climber.\nBallard's mother, Alison Hargreaves, was a renowned climber in her own right, and her death on K2 in 1995 was a tragic loss for the climbing community. Ballard has been inspired by his mother's legacy and sees his climb of K2 as a way to carry on her spirit. Despite the challenges and dangers of the climb, Ballard is determined to reach the summit and fulfill his mother's dream.\nThe climb of K2 is notoriously difficult and has claimed many lives over the years. Ballard is well aware of the\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   536 runs   (    0.01 ms per token, 98276.49 tokens per second)\nllama_perf_context_print:        load time =    3887.73 ms\nllama_perf_context_print: prompt eval time =    1018.82 ms /   280 tokens (    3.64 ms per token,   274.83 tokens per second)\nllama_perf_context_print:        eval time =   54856.15 ms /   255 runs   (  215.12 ms per token,     4.65 tokens per second)\nllama_perf_context_print:       total time =   55924.44 ms /   535 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "54927.14",
        "tokens": "524",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Trillions of pounds worth of oil \u2013 as much as the entire North Sea fields \u2013 lies beneath an area of England dubbed \u2018Britain\u2019s Dallas\u2019, it was claimed yesterday. Analysis suggests there is up to 100billion barrels of the fossil fuel under the Home Counties. The potential goldmine in the Weald Basin, across Surrey, Sussex, Hampshire and Kent, could meet up to a third of Britain\u2019s oil demand within 15 years, according to the consortium exploring the area. Scroll down for video. UK Oil & Gas Investments say they have discovered 100billion barrels worth of oil reserves in the Weald Basin, near Gatwick airport (pictured, the well head at Horse Hill) A map showing the location of the oil and gas deposits within the Weald Basin, which are thought to lie at between 2,500ft and 3,000ft below the ground. This could reduce reliance on oil from the Middle East and Russia and could be a major boost for British industry at a time when the North Sea supply is dwindling. But industry experts remained sceptical of t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Trillions of pounds worth of oil \u2013 as much as the entire North Sea fields \u2013 lies beneath an area of England dubbed \u2018Britain\u2019s Dallas\u2019, it was claimed yesterday. Analysis suggests there is up to 100billion barrels of the fossil fuel under the Home Counties. The potential goldmine in the Weald Basin, across Surrey, Sussex, Hampshire and Kent, could meet up to a third of Britain\u2019s oil demand within 15 years, according to the consortium exploring the area. Scroll down for video. UK Oil & Gas Investments say they have discovered 100billion barrels worth of oil reserves in the Weald Basin, near Gatwick airport (pictured, the well head at Horse Hill) A map showing the location of the oil and gas deposits within the Weald Basin, which are thought to lie at between 2,500ft and 3,000ft below the ground. This could reduce reliance on oil from the Middle East and Russia and could be a major boost for British industry at a time when the North Sea supply is dwindling. But industry experts remained sceptical of t\nSummary: A significant amount of oil, estimated to be worth trillions of pounds, has been discovered beneath an area in England known as \"Britain's Dallas.\" Analysis suggests that there are up to 100 billion barrels of fossil fuel under the Home Counties, which could meet up to a third of Britain's oil demand within 15 years. The discovery is made by UK Oil & Gas Investments, who have found the reserves in the Weald Basin, near Gatwick airport. The oil and gas deposits are believed to be located between 2,500ft and 3,000ft below the ground. This find could reduce Britain's reliance on oil from the Middle East and Russia and provide a major boost for the country's industry at a time when the North Sea supply is dwindling. However, industry experts remain skeptical of the find, citing the difficulty of extracting the oil and the potential environmental impact.\n\nKeywords: oil, fossil fuel, Weald Basin, Home Counties, Gatwick airport, North Sea supply, industry experts, environmental impact. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.39 ms /   525 runs   (    0.01 ms per token, 97402.60 tokens per second)\nllama_perf_context_print:        load time =    3901.94 ms\nllama_perf_context_print: prompt eval time =    1003.13 ms /   272 tokens (    3.69 ms per token,   271.15 tokens per second)\nllama_perf_context_print:        eval time =   53875.25 ms /   252 runs   (  213.79 ms per token,     4.68 tokens per second)\nllama_perf_context_print:       total time =   54927.14 ms /   524 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 254 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56743.58",
        "tokens": "502",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The British Veterinary Association has withdrawn its support for the shooting of free-running badgers in controversial pilot culls. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. It concluded that the first two years of culling had not demonstrated conclusively that \u2018controlled shooting\u2019 of free-running badgers could be effective and humane. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. The BVA said it still supported badger culling to tackle tuberculosis in cattle. Animal welfare campaigners welcomed the news but said the BVA\u2019s ongoing support for cage-trapping and shooting was \u2018regrettable\u2019. It comes after the Environment Secretary promised to roll out more culls across the country if the Tories win the election. Addressing the National Farmers Union conference in February, Liz Truss sai -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The British Veterinary Association has withdrawn its support for the shooting of free-running badgers in controversial pilot culls. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. It concluded that the first two years of culling had not demonstrated conclusively that \u2018controlled shooting\u2019 of free-running badgers could be effective and humane. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA. The BVA said it still supported badger culling to tackle tuberculosis in cattle. Animal welfare campaigners welcomed the news but said the BVA\u2019s ongoing support for cage-trapping and shooting was \u2018regrettable\u2019. It comes after the Environment Secretary promised to roll out more culls across the country if the Tories win the election. Addressing the National Farmers Union conference in February, Liz Truss sai...\nThe British Veterinary Association (BVA) has withdrawn its support for the shooting of free-running badgers in controversial pilot culls. The two four-year pilot culls in Somerset and Gloucestershire should be completed by trapping the badgers in cages and then shooting them, says the BVA.\nThe BVA concluded that the first two years of culling had not demonstrated conclusively that \u2018controlled shooting\u2019 of free-running badgers could be effective and humane. The BVA said it still supported badger culling to tackle tuberculosis in cattle, but added that it could not support the shooting of free-running badgers in the current form.\nAnimal welfare campaigners welcomed the news but said the BVA\u2019s ongoing support for cage-trapping and shooting was \u2018regrettable\u2019. It comes after the Environment Secretary promised to roll out more culls across the country if the Tories win the election.\nAddressing the National Farmers Union conference in February, Liz Truss said: \u201cWe will continue to work with farmers and the badger community to find a solution to bov\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.49 ms /   503 runs   (    0.01 ms per token, 91604.44 tokens per second)\nllama_perf_context_print:        load time =    3879.57 ms\nllama_perf_context_print: prompt eval time =     987.15 ms /   247 tokens (    4.00 ms per token,   250.22 tokens per second)\nllama_perf_context_print:        eval time =   55707.80 ms /   255 runs   (  218.46 ms per token,     4.58 tokens per second)\nllama_perf_context_print:       total time =   56743.58 ms /   502 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56824.87",
        "tokens": "499",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Modern technology is supposed to be making our lives easier, but often all it does is create more pitfalls. 21st century additions like a Facebook page, an Instagram profile and a smart phone often end up doing more damage to a budding relationship than good. These days we are more connected than ever, but that regularly pushes a potential partner further away. Smart phones and social media often cause nightmare for men and women on dates (picture posed by models) On a date in the 1990s, the biggest fails a person could be responsible for were a lack of conversation, food in the teeth or a man asking\u00a0the\u00a0woman to split the bill. Nowadays, the man or woman is also likely to try and\u00a0photograph\u00a0their food or keep\u00a0checking\u00a0their phone at\u00a0the\u00a0table. Girls are often caught out spending too much time in the bathroom, while they compare notes on their evening with friends as their date is sat back at the table. And after the evening is over both parties can fall foul of the temptation to get in touch too q -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Modern technology is supposed to be making our lives easier, but often all it does is create more pitfalls. 21st century additions like a Facebook page, an Instagram profile and a smart phone often end up doing more damage to a budding relationship than good. These days we are more connected than ever, but that regularly pushes a potential partner further away. Smart phones and social media often cause nightmare for men and women on dates (picture posed by models) On a date in the 1990s, the biggest fails a person could be responsible for were a lack of conversation, food in the teeth or a man asking\u00a0the\u00a0woman to split the bill. Nowadays, the man or woman is also likely to try and\u00a0photograph\u00a0their food or keep\u00a0checking\u00a0their phone at\u00a0the\u00a0table. Girls are often caught out spending too much time in the bathroom, while they compare notes on their evening with friends as their date is sat back at the table. And after the evening is over both parties can fall foul of the temptation to get in touch too q...\n\nQuestion: What are some of the ways in which modern technology is creating problems in relationships?\n\nAnswer: According to the article, modern technology is creating problems in relationships in several ways:\n\n1. Distraction: Modern technology, such as smartphones and social media, can be a major distraction during dates, causing individuals to spend more time on their phones than interacting with their partner.\n2. Over-sharing: Social media can lead to over-sharing, which can create unrealistic expectations and pressure in relationships.\n3. Comparison: Constantly comparing oneself to others on social media can lead to feelings of inadequacy and low self-esteem in relationships.\n4. Lack of face-to-face interaction: The overuse of technology can lead to a lack of face-to-face interaction, which is essential for building strong and healthy relationships.\n5. Miscommunication: Technology can also lead to miscommunication and misunderstandings in relationships, as messages can be misinterpreted or taken out of context.\n6. Jealousy: Social media can also fuel jealousy and insecurity in relationships, as individuals may feel threatened by\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.47 ms /   500 runs   (    0.01 ms per token, 91390.97 tokens per second)\nllama_perf_context_print:        load time =    3897.04 ms\nllama_perf_context_print: prompt eval time =     991.04 ms /   244 tokens (    4.06 ms per token,   246.21 tokens per second)\nllama_perf_context_print:        eval time =   55784.08 ms /   255 runs   (  218.76 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   56824.87 ms /   499 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56550.31",
        "tokens": "529",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Ahead of this weekend's Premier League action, Sportsmail will be providing you with all you need to know about every fixture, with team news, provisional squads, betting odds and Opta stats. Here is all the information you need for Tottenham's home clash with Aston Villa... Tottenham Hotspur vs Aston Villa (White Hart Lane) Team news. Tottenham. Kyle Walker and Hugo Lloris are absent for Tottenham, while Jan Vertonghen is a doubt for this weekend's match against Aston Villa. England right-back Walker is unavailable after sustaining a foot injury in last week's draw at Burnley - a match goalkeeper Lloris missed through a knee injury which continues to keep him sidelined. Kyle Walker (left) was injured for Tottenham after clashing with Burnley's Kieran Tripper last Sunday. Vertonghen (virus) is one of several doubts for the Premier League encounter at White Hart Lane, where Harry Kane again wears the captain's armband. Provisional squad: Vorm, Friedel, Yedlin, Chiriches, Kaboul, Dier, Vertonghen, Ro -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Ahead of this weekend's Premier League action, Sportsmail will be providing you with all you need to know about every fixture, with team news, provisional squads, betting odds and Opta stats. Here is all the information you need for Tottenham's home clash with Aston Villa... Tottenham Hotspur vs Aston Villa (White Hart Lane) Team news. Tottenham. Kyle Walker and Hugo Lloris are absent for Tottenham, while Jan Vertonghen is a doubt for this weekend's match against Aston Villa. England right-back Walker is unavailable after sustaining a foot injury in last week's draw at Burnley - a match goalkeeper Lloris missed through a knee injury which continues to keep him sidelined. Kyle Walker (left) was injured for Tottenham after clashing with Burnley's Kieran Tripper last Sunday. Vertonghen (virus) is one of several doubts for the Premier League encounter at White Hart Lane, where Harry Kane again wears the captain's armband. Provisional squad: Vorm, Friedel, Yedlin, Chiriches, Kaboul, Dier, Vertonghen, Rooney, Eriksen, Alli, Kane, Lamela, Janssen. Aston Villa. Villa boss Steve Bruce has no fresh injury concerns ahead of the trip to Tottenham, with Jordan Ayew (hamstring) and Ritchie De Laet (knee) the only absentees. Provisional squad: Guzan, Johnstone, Hutton, Chester, Elmohamady, Veretout, Gueye, Ayew, Agbonlahor, Grealish, Bacuna, Gestede, Davis. Betting odds. Tottenham are 11/10 to win the match, with Aston Villa priced at 11/4 to take all three points. The draw is available at 12/5. Opta stats. Tottenham have won their last three Premier League matches against Aston Villa, scoring at least two goals in each of those games. The Villans have won just one of their last 13 away league games against Spurs (D4 L8). Harry Kane has scored 10 goals in his last 10 Premier League appearances against Aston Villa, including a hat-trick in the\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.50 ms /   530 runs   (    0.01 ms per token, 96398.69 tokens per second)\nllama_perf_context_print:        load time =    3946.52 ms\nllama_perf_context_print: prompt eval time =    1004.20 ms /   274 tokens (    3.66 ms per token,   272.85 tokens per second)\nllama_perf_context_print:        eval time =   55497.90 ms /   255 runs   (  217.64 ms per token,     4.59 tokens per second)\nllama_perf_context_print:       total time =   56550.31 ms /   529 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "40755.08",
        "tokens": "443",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A mother who made the heartbreaking decision to give birth to her sick baby at just 20 weeks had her wedding dress redesigned so her tiny daughter could be buried in it. Jaye Cox and her husband, Matthew, decided to deliver their daughter Annabelle prematurely on May 5 last year after scans showed she had developed a serious form of Spina Bifida. Annabelle was born weighing just 290 grams at Canberra Hospital and her distraught parents were able to cradle her in the short five minutes she survived. Jaye and Matthew Cox delivered their daughter Annabelle\u00a0at just 20 weeks in Canberra Hospital after scans showed she had developed a serious form of Spina Bifida. 'We were planning a life for her and you think you're going to see her laugh, walk, talk, go to school, get married,' Ms Cox told Daily Mail Australia. 'But we were faced with this decision - do we let her go and free her of the suffering, or do we go ahead and bring her into the world and watch her suffer. 'My husband and I had to make the dec -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A mother who made the heartbreaking decision to give birth to her sick baby at just 20 weeks had her wedding dress redesigned so her tiny daughter could be buried in it. Jaye Cox and her husband, Matthew, decided to deliver their daughter Annabelle prematurely on May 5 last year after scans showed she had developed a serious form of Spina Bifida. Annabelle was born weighing just 290 grams at Canberra Hospital and her distraught parents were able to cradle her in the short five minutes she survived. Jaye and Matthew Cox delivered their daughter Annabelle\u00a0at just 20 weeks in Canberra Hospital after scans showed she had developed a serious form of Spina Bifida. 'We were planning a life for her and you think you're going to see her laugh, walk, talk, go to school, get married,' Ms Cox told Daily Mail Australia. 'But we were faced with this decision - do we let her go and free her of the suffering, or do we go ahead and bring her into the world and watch her suffer. 'My husband and I had to make the decisions that were best for her, and that was to let her go.'\n\nIn this article, a mother named Jaye Cox made the heartbreaking decision to give birth to her sick baby at just 20 weeks. Despite her grief, she had her wedding dress redesigned so that her tiny daughter, Annabelle, could be buried in it. The baby was born weighing just 290 grams and survived for only five minutes. Jaye and her husband, Matthew, had to make the difficult decision to let their daughter go, but they were able to cradle her in their arms before she passed away. This article highlights the emotional and difficult decisions that parents must make when faced with a sick or premature baby, and the love and devotion they show towards their child, even in the face of tragedy. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.91 ms /   444 runs   (    0.01 ms per token, 113525.95 tokens per second)\nllama_perf_context_print:        load time =    3890.63 ms\nllama_perf_context_print: prompt eval time =    1084.94 ms /   260 tokens (    4.17 ms per token,   239.64 tokens per second)\nllama_perf_context_print:        eval time =   39634.04 ms /   183 runs   (  216.58 ms per token,     4.62 tokens per second)\nllama_perf_context_print:       total time =   40755.08 ms /   443 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 185 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "1001.08",
        "tokens": "255",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: PC Ronnie Lungu has won a race discrimination case against Wiltshire Police. A black police constable has won a race discrimination case against his force after they did not promote him due to the colour of his skin. Ronnie Lungu was singled out as 'a marked man' by Wiltshire Police solely due to his race, the employment tribunal ruled. It found the 40-year-old was passed over for promotion in favour of white colleagues after his internal assessments were secretly downgraded to make him appear unworthy. It also heard evidence that derogatory comments were made about Mr Lungu based on his colour violating his dignity. When the PC raised concerns with his superiors they were not taken seriously and his complaints were brushed off as trivial. The force - which has only 16 ethnic minority officers out of a total of just over 1,000 - had denied the claims. But the tribunal ruled that the Zimbabwe-born officer was unlawfully discriminated against on grounds of race. Mr Lungu said: 'When I realised I was  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: PC Ronnie Lungu has won a race discrimination case against Wiltshire Police. A black police constable has won a race discrimination case against his force after they did not promote him due to the colour of his skin. Ronnie Lungu was singled out as 'a marked man' by Wiltshire Police solely due to his race, the employment tribunal ruled. It found the 40-year-old was passed over for promotion in favour of white colleagues after his internal assessments were secretly downgraded to make him appear unworthy. It also heard evidence that derogatory comments were made about Mr Lungu based on his colour violating his dignity. When the PC raised concerns with his superiors they were not taken seriously and his complaints were brushed off as trivial. The force - which has only 16 ethnic minority officers out of a total of just over 1,000 - had denied the claims. But the tribunal ruled that the Zimbabwe-born officer was unlawfully discriminated against on grounds of race. Mr Lungu said: 'When I realised I was  [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.04 ms /   255 runs   (    0.00 ms per token, 7285714.29 tokens per second)\nllama_perf_context_print:        load time =    3918.79 ms\nllama_perf_context_print: prompt eval time =     998.96 ms /   254 tokens (    3.93 ms per token,   254.27 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    1001.08 ms /   255 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 2 \nllama_perf_context_print:       exit layer = {32, 32, }\n"
    },
    {
        "inference_time": "19674.01",
        "tokens": "329",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Usually parents get to work teaching children their ABCs. But one parent decided to teach their baby an altogether less practical trick. American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Footage shows him gnawing on the side of a drinks container before stopping and prompting his child to follow. The infant then obediently performs the same stunt with a water bottle of her own. The duo then sit together enjoying some bonding time with their simple props,. To date the clip has been watched more than 190,000 times, with many viewers deeming it 'cute' and 'adorable'. 'That's so awesome. I'm amazed at how she pays attention to you and copies. It must feel great to have a daughter like her,' one commenter wrote. Repeat after me: American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Home tutoring: Footage shows him gnawing on the side of a drinks container before sto -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Usually parents get to work teaching children their ABCs. But one parent decided to teach their baby an altogether less practical trick. American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Footage shows him gnawing on the side of a drinks container before stopping and prompting his child to follow. The infant then obediently performs the same stunt with a water bottle of her own. The duo then sit together enjoying some bonding time with their simple props,. To date the clip has been watched more than 190,000 times, with many viewers deeming it 'cute' and 'adorable'. 'That's so awesome. I'm amazed at how she pays attention to you and copies. It must feel great to have a daughter like her,' one commenter wrote. Repeat after me: American YouTube user KentuckyFriedIdiot filmed himself teaching his newborn daughter how to suck a plastic water bottle. Home tutoring: Footage shows him gnawing on the side of a drinks container before stooping and prompting his child to follow. The infant then obediently performs the same stunt with a water bottle of her own. The duo then sit together enjoying some bonding time with their simple props. The video has been viewed more than 190,000 times with many viewers deeming it 'cute' and 'adorable'. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.78 ms /   330 runs   (    0.01 ms per token, 184977.58 tokens per second)\nllama_perf_context_print:        load time =    4010.36 ms\nllama_perf_context_print: prompt eval time =    1066.36 ms /   248 tokens (    4.30 ms per token,   232.57 tokens per second)\nllama_perf_context_print:        eval time =   18589.34 ms /    81 runs   (  229.50 ms per token,     4.36 tokens per second)\nllama_perf_context_print:       total time =   19674.01 ms /   329 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 83 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55405.47",
        "tokens": "511",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Taken at the dawn of photography, these unique pictures are some of the first ever taken of the city of Bristol, nearly 100 years before much of its historic centre was destroyed by Nazi bombing. The images, captured between 1850 and 1855 by photographer Hugh Owen, have surfaced after being put up for auction - and could fetch up to \u00a330,000. Everything from the waterfront, city centre and St Mary Redcliffe church can be seen in the shots, as well as\u00a0The Fourteen Stars Tavern building which was pulled down in 1857. It became part of the site for The Bristol Brewery Georges before that was closed in 1999 and later turned into a block of flats. Many of the buildings pictured have changed dramatically between the 19th century and today, particularly the area of High Street. Other shots are almost identical, including the one of St John's Gate at the Church of St John the Baptist, which shows that little is different in certain areas that escaped the heavy bombing during the Second World War. The albume -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Taken at the dawn of photography, these unique pictures are some of the first ever taken of the city of Bristol, nearly 100 years before much of its historic centre was destroyed by Nazi bombing. The images, captured between 1850 and 1855 by photographer Hugh Owen, have surfaced after being put up for auction - and could fetch up to \u00a330,000. Everything from the waterfront, city centre and St Mary Redcliffe church can be seen in the shots, as well as\u00a0The Fourteen Stars Tavern building which was pulled down in 1857. It became part of the site for The Bristol Brewery Georges before that was closed in 1999 and later turned into a block of flats. Many of the buildings pictured have changed dramatically between the 19th century and today, particularly the area of High Street. Other shots are almost identical, including the one of St John's Gate at the Church of St John the Baptist, which shows that little is different in certain areas that escaped the heavy bombing during the Second World War. The albume are expected to fetch between \u00a320,000 and \u00a330,000 when they go under the hammer at auctioneers, Christie's, in London on June 23.\n\nAnalysis:\n\n* The photographs are from the early days of photography and show Bristol in the mid-19th century.\n* The images were taken by Hugh Owen between 1850 and 1855 and have recently surfaced and are up for auction.\n* The photographs show various parts of Bristol, including the waterfront, city center, and St Mary Redcliffe church.\n* Some of the buildings pictured have changed significantly since the 19th century, while others remain largely unchanged.\n* The photographs are expected to fetch between \u00a320,000 and \u00a330,000 at auction.\n\nEvaluation:\n\n* The photographs provide a unique glimpse into Bristol's past and are of historical significance.\n* The images show the city's architecture and landscape in the mid-19th century, which is an important period in Bristol\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.48 ms /   512 runs   (    0.01 ms per token, 93430.66 tokens per second)\nllama_perf_context_print:        load time =    3904.89 ms\nllama_perf_context_print: prompt eval time =     996.26 ms /   256 tokens (    3.89 ms per token,   256.96 tokens per second)\nllama_perf_context_print:        eval time =   54359.80 ms /   255 runs   (  213.18 ms per token,     4.69 tokens per second)\nllama_perf_context_print:       total time =   55405.47 ms /   511 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56560.15",
        "tokens": "538",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A pro-Ukip businessman who forced Liam Fox to resign as Defence Secretary is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tycoon Harvey Boulter has already given \u00a330,000 to the Ukip candidate fighting Fox in his Somerset seat in a continued vendetta against him. Boulter is also spending another \u00a330,000 to try to defeat Tom Tugendhat, who is standing for the Tories in Tonbridge and West Malling, Kent. Scroll down for video. Pro-Ukip businessman Harvey Boulter (above), who forced Liam Fox to resign as Defence Secretary, is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tugendhat\u2019s father, Sir Michael Tugendhat, is the High Court judge who awarded Fox a six-figure sum in damages after he sued Boulter for libellous remarks made on Sky TV. Fox resigned from the Cabinet in 2011 after Boulter revealed details of how Fox\u2019s close friend, businessman Adam Werritty, accompanied him on overseas trips as Defence Secretary. Mr Boulter said: \u2018This may look like naked  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A pro-Ukip businessman who forced Liam Fox to resign as Defence Secretary is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tycoon Harvey Boulter has already given \u00a330,000 to the Ukip candidate fighting Fox in his Somerset seat in a continued vendetta against him. Boulter is also spending another \u00a330,000 to try to defeat Tom Tugendhat, who is standing for the Tories in Tonbridge and West Malling, Kent. Scroll down for video. Pro-Ukip businessman Harvey Boulter (above), who forced Liam Fox to resign as Defence Secretary, is behind an extraordinary bid to defeat a \u2018golden boy\u2019 Tory candidate. Tugendhat\u2019s father, Sir Michael Tugendhat, is the High Court judge who awarded Fox a six-figure sum in damages after he sued Boulter for libellous remarks made on Sky TV. Fox resigned from the Cabinet in 2011 after Boulter revealed details of how Fox\u2019s close friend, businessman Adam Werritty, accompanied him on overseas trips as Defence Secretary. Mr Boulter said: \u2018This may look like naked \nambition but I am determined to stop these two Tory candidates who are a threat to the country\u2019s future\u2019.\n\nAnalysis:\n\n* Harvey Boulter is a pro-Ukip businessman who has a vendetta against Liam Fox, who he forced to resign as Defence Secretary in 2011.\n* Boulter is now spending \u00a360,000 to try to defeat two Tory candidates, Tom Tugendhat and Fox, in their respective seats in Somerset and Kent.\n* Boulter's actions are seen as an attempt to disrupt the Tory party's chances of winning the general election.\n* The video below shows Boulter discussing his motivations for supporting the Ukip candidate in Fox's seat.\n\nQuestion: What is the reason behind Harvey Boulter's actions?\n\nAnswer: According to the article, Boulter is motivated by a vendetta against Liam Fox, who he forced to resign as Defence Secretary in 2011. Boulter is determined to stop what he sees as a threat to the country's future by defeating two\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   539 runs   (    0.01 ms per token, 98935.39 tokens per second)\nllama_perf_context_print:        load time =    3946.22 ms\nllama_perf_context_print: prompt eval time =    1034.48 ms /   283 tokens (    3.66 ms per token,   273.57 tokens per second)\nllama_perf_context_print:        eval time =   55475.60 ms /   255 runs   (  217.55 ms per token,     4.60 tokens per second)\nllama_perf_context_print:       total time =   56560.15 ms /   538 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56700.90",
        "tokens": "508",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Aston Villa boss Tim Sherwood has offered encouragement to a six-year-old fan he beat to the job. Charlie Pye, from Leamington Spa, applied for the Villa hotseat after Paul Lambert was sacked in February. The Villa fan wanted a chocolate bar as a win bonus and would have agreed terms for a signed claret and blue football, a WWE wrestling figure and some sweets. Sherwood wrote Charlie a letter congratulating him on his credentials, despite missing out on the job. Aston Villa boss Tim Sherwood (centre) showed his softer side by replying to 6-year-old Charlie Pye. He wrote: 'From your letter it is clear to see that you would indeed be an ideal candidate for the job due to your knowledge of the game - and your very capable backroom staff, of course! 'Hopefully you're not too disappointed at missing out and I'm informed that I just managed to pip you to the post so you were very close to being offered the job. 'Our chief executive Tom Fox has informed me that the club would like your permission to keep  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Aston Villa boss Tim Sherwood has offered encouragement to a six-year-old fan he beat to the job. Charlie Pye, from Leamington Spa, applied for the Villa hotseat after Paul Lambert was sacked in February. The Villa fan wanted a chocolate bar as a win bonus and would have agreed terms for a signed claret and blue football, a WWE wrestling figure and some sweets. Sherwood wrote Charlie a letter congratulating him on his credentials, despite missing out on the job. Aston Villa boss Tim Sherwood (centre) showed his softer side by replying to 6-year-old Charlie Pye. He wrote: 'From your letter it is clear to see that you would indeed be an ideal candidate for the job due to your knowledge of the game - and your very capable backroom staff, of course! 'Hopefully you're not too disappointed at missing out and I'm informed that I just managed to pip you to the post so you were very close to being offered the job. 'Our chief executive Tom Fox has informed me that the club would like your permission to keep 50 per cent of your wages for the next 10 years, in exchange for the signed claret and blue football. 'I hope you're not too disappointed and I wish you all the best in your future endeavours.' Charlie's mum, Sarah Pye, said: 'Charlie was absolutely over the moon when he received the letter. He's been talking about it non-stop since he got it. 'He's really proud of himself and can't believe that the manager of Aston Villa wrote to him.'\n\nAnalysis: Tim Sherwood, the manager of Aston Villa, has shown his softer side by replying to a six-year-old fan who applied for the job. Charlie Pye, from Leamington Spa, wrote a letter to Sherwood after Paul Lambert was sacked in February, expressing his interest in the position. Despite missing out on the job, Sherwood wrote a letter to Charlie congratulating him on his credentials and offering a signed claret and blue football as a consolation prize. The letter also included a joke about Charlie's backroom staff, which is a reference to the fact that Charlie is only six years old\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.51 ms /   509 runs   (    0.01 ms per token, 92360.73 tokens per second)\nllama_perf_context_print:        load time =    3933.24 ms\nllama_perf_context_print: prompt eval time =    1017.22 ms /   253 tokens (    4.02 ms per token,   248.72 tokens per second)\nllama_perf_context_print:        eval time =   55634.31 ms /   255 runs   (  218.17 ms per token,     4.58 tokens per second)\nllama_perf_context_print:       total time =   56700.90 ms /   508 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "16526.25",
        "tokens": "334",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Atletico Madrid turned up the heat on Valencia in the battle for La Liga's third automatic Champions League berth with club top scorer Antoine Griezmann setting them on the way to a 2-0 stroll at bottom side Cordoba on Saturday. The victory for the champions at Cordoba's El Arcangel stadium in balmy Andalucia lifted Atletico above Valencia and onto 62 points from 29 matches. Valencia have 60 points in fourth and can reclaim third spot with a win at home to sixth-placed Villarreal on Sunday. France forward Griezmann struck his 15th league goal of the campaign in the fifth minute after stealing the ball, surging forward from midfield and cracking a precise low shot into the bottom corner. Atletico Madrid forward Antoine Griezmann scored his 15th goal of the season by netting opener. Saul Niguez doubled Atletico's lead in the 39th minute to ensure his side leapfrogged Valencia. Valencia are currently two points behind Atletico Madrid but do have a game in hand over their top four rivals. Midfielder Sa -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Atletico Madrid turned up the heat on Valencia in the battle for La Liga's third automatic Champions League berth with club top scorer Antoine Griezmann setting them on the way to a 2-0 stroll at bottom side Cordoba on Saturday. The victory for the champions at Cordoba's El Arcangel stadium in balmy Andalucia lifted Atletico above Valencia and onto 62 points from 29 matches. Valencia have 60 points in fourth and can reclaim third spot with a win at home to sixth-placed Villarreal on Sunday. France forward Griezmann struck his 15th league goal of the campaign in the fifth minute after stealing the ball, surging forward from midfield and cracking a precise low shot into the bottom corner. Atletico Madrid forward Antoine Griezmann scored his 15th goal of the season by netting opener. Saul Niguez doubled Atletico's lead in the 39th minute to ensure his side leapfrogged Valencia. Valencia are currently two points behind Atletico Madrid but do have a game in hand over their top four rivals. Midfielder Saul Niguez scored his second goal of the season to seal the win for Atletico Madrid.\n\nKeywords: Atletico Madrid, Valencia, La Liga, Champions League, Antoine Griezmann, Saul Niguez, Cordoba, El Arcangel stadium, Andalucia, France, Villarreal. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.58 ms /   335 runs   (    0.00 ms per token, 211623.50 tokens per second)\nllama_perf_context_print:        load time =    3955.08 ms\nllama_perf_context_print: prompt eval time =    1036.32 ms /   263 tokens (    3.94 ms per token,   253.78 tokens per second)\nllama_perf_context_print:        eval time =   15474.31 ms /    71 runs   (  217.95 ms per token,     4.59 tokens per second)\nllama_perf_context_print:       total time =   16526.25 ms /   334 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 73 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "19799.67",
        "tokens": "363",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: An 18-year-old mother-of-one had two heart attacks before dying from multiple organ failure after taking M-Cat at a house party, her family have said. Abby Swinfield, from\u00a0Castle Donington, in Derbyshire, was partying with friends when she was rushed to hospital in the early hours of March 30. Miss Swinfield, who is believed to have a son, three, was taken to Good Hope Hospital in Sutton Coldfield, West Midlands, where she was placed in a coma, but died a week later. Abby Swinfield, 18, a mother-of-one from\u00a0Castle Donington, in Derbyshire, was rushed to hospital in the early hours of March 30 after a house party in Tamworth where it is thought she took M-Cat. Following Miss Swinfield's death, police have arrested one girl for supplying drugs, but Miss Swinfield's cousin has claimed she didn't take the drug voluntarily, saying her drink was spiked. Neighbours say ambulances arrived at this property at 1am. After arriving at hospital Miss Swinfield was put into a coma, but died a week later. Police a -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: An 18-year-old mother-of-one had two heart attacks before dying from multiple organ failure after taking M-Cat at a house party, her family have said. Abby Swinfield, from\u00a0Castle Donington, in Derbyshire, was partying with friends when she was rushed to hospital in the early hours of March 30. Miss Swinfield, who is believed to have a son, three, was taken to Good Hope Hospital in Sutton Coldfield, West Midlands, where she was placed in a coma, but died a week later. Abby Swinfield, 18, a mother-of-one from\u00a0Castle Donington, in Derbyshire, was rushed to hospital in the early hours of March 30 after a house party in Tamworth where it is thought she took M-Cat. Following Miss Swinfield's death, police have arrested one girl for supplying drugs, but Miss Swinfield's cousin has claimed she didn't take the drug voluntarily, saying her drink was spiked. Neighbours say ambulances arrived at this property at 1am. After arriving at hospital Miss Swinfield was put into a coma, but died a week later. Police ae investigating after an 18-year-old mother-of-one died after taking M-Cat at a house party in Tamworth, Staffordshire.\n\nQuestion: What happened to Abby Swinfield after she took M-Cat at a house party?\n\nAnswer: After taking M-Cat at a house party, Abby Swinfield had two heart attacks before dying from multiple organ failure. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.92 ms /   364 runs   (    0.01 ms per token, 189979.12 tokens per second)\nllama_perf_context_print:        load time =    3914.18 ms\nllama_perf_context_print: prompt eval time =    1008.03 ms /   275 tokens (    3.67 ms per token,   272.81 tokens per second)\nllama_perf_context_print:        eval time =   18773.38 ms /    88 runs   (  213.33 ms per token,     4.69 tokens per second)\nllama_perf_context_print:       total time =   19799.67 ms /   363 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 90 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55374.54",
        "tokens": "490",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A playground in Tokyo has been found to have dangerously high levels of radiation, which has raised fears for the health of children in the area. Soil underneath a slide at the park in the Toshima ward in the north-east of the Japanese capital, showed radiation readings of up to 480 microsieverts per hour. This is nearly half the recommended annual limit of exposure for a person. Danger: Japanese authorities detected an unusually high level of radiation around playground equipment in this Tokyo park (pictured) Safety: Authorities cordoned off the park after high levels of radiation were detected underneath a slide. It was not immediately clear why radiation levels were so high in the park, which opened two years after the 2011 Fukushima disaster. Authorities believe that the radiation could be coming from something buried underneath the park. The playground has now been fenced off with an orange barricade. Park representative Kyoko Tsujimura said: 'We are still checking what caused these elevated r -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A playground in Tokyo has been found to have dangerously high levels of radiation, which has raised fears for the health of children in the area. Soil underneath a slide at the park in the Toshima ward in the north-east of the Japanese capital, showed radiation readings of up to 480 microsieverts per hour. This is nearly half the recommended annual limit of exposure for a person. Danger: Japanese authorities detected an unusually high level of radiation around playground equipment in this Tokyo park (pictured) Safety: Authorities cordoned off the park after high levels of radiation were detected underneath a slide. It was not immediately clear why radiation levels were so high in the park, which opened two years after the 2011 Fukushima disaster. Authorities believe that the radiation could be coming from something buried underneath the park. The playground has now been fenced off with an orange barricade. Park representative Kyoko Tsujimura said: 'We are still checking what caused these elevated radaition levels, but we are taking all necessary measures to ensure the safety of children.'\n\nRadiation levels at a playground in Tokyo have been found to be dangerously high, with readings of up to 480 microsieverts per hour detected underneath a slide. This is nearly half the recommended annual limit of exposure for a person, and has raised fears for the health of children in the area.\n\nJapanese authorities detected the high levels of radiation around playground equipment in the Toshima ward in the north-east of the city, and have cordoned off the park as a precaution. It is not yet clear why the radiation levels are so high in the park, which opened two years after the 2011 Fukushima disaster.\n\nAuthorities believe that the radiation could be coming from something buried underneath the park, and are working to determine the source of the contamination. In the meantime, the playground has been fenced off with an orange barricade to prevent children from entering.\n\nA park representative, Kyoko Tsujimura, said: \"We are still checking what caused these elevated radiation levels\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.44 ms /   491 runs   (    0.01 ms per token, 90290.55 tokens per second)\nllama_perf_context_print:        load time =    3891.74 ms\nllama_perf_context_print: prompt eval time =     997.82 ms /   235 tokens (    4.25 ms per token,   235.51 tokens per second)\nllama_perf_context_print:        eval time =   54326.80 ms /   255 runs   (  213.05 ms per token,     4.69 tokens per second)\nllama_perf_context_print:       total time =   55374.54 ms /   490 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55488.27",
        "tokens": "530",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Amir Khan believes Kell Brook has called him out so that he can have the 'one big payday' which has so far eluded him during his 10-and-a-half year career. Brook, who defeated Jo Jo Dan to retain his IBF welterweight title last month, immediately challenged Khan in a post-fight interview but the Bolton-born boxer has instead agreed a bout on May 30, with his opponent yet to be confirmed. Khan has reiterated his intention to take on Brook within the next 12 months, although he insists that the undefeated 28-year-old only wants the fight because of the money it will earn him. Amir Khan (left), pictured during his last fight against Devon Alexander, has blasted Kell Brook's motives. Brook (right) shakes hands with British Superbike champion Shane Byrne at Donington Park on Tuesday. In an interview with iFL TV, Khan said: 'He's only faced one decent opponent in his whole career, so he has no room to talk. It seems to me that he's just calling me out for that one big payday. 'If he keeps going on like t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Amir Khan believes Kell Brook has called him out so that he can have the 'one big payday' which has so far eluded him during his 10-and-a-half year career. Brook, who defeated Jo Jo Dan to retain his IBF welterweight title last month, immediately challenged Khan in a post-fight interview but the Bolton-born boxer has instead agreed a bout on May 30, with his opponent yet to be confirmed. Khan has reiterated his intention to take on Brook within the next 12 months, although he insists that the undefeated 28-year-old only wants the fight because of the money it will earn him. Amir Khan (left), pictured during his last fight against Devon Alexander, has blasted Kell Brook's motives. Brook (right) shakes hands with British Superbike champion Shane Byrne at Donington Park on Tuesday. In an interview with iFL TV, Khan said: 'He's only faced one decent opponent in his whole career, so he has no room to talk. It seems to me that he's just calling me out for that one big payday. 'If he keeps going on like t...\nAmir Khan believes Kell Brook has called him out so that he can have the 'one big payday' which has so far eluded him during his 10-and-a-half year career. Brook, who defeated Jo Jo Dan to retain his IBF welterweight title last month, immediately challenged Khan in a post-fight interview but the Bolton-born boxer has instead agreed a bout on May 30, with his opponent yet to be confirmed.\nKhan has reiterated his intention to take on Brook within the next 12 months, although he insists that the undefeated 28-year-old only wants the fight because of the money it will earn him.\nAmir Khan (left), pictured during his last fight against Devon Alexander, has blasted Kell Brook's motives. Brook (right) shakes hands with British Superbike champion Shane Byrne at Donington Park on Tuesday.\nIn an interview with iFL TV, Khan said: 'He's only faced one decent opponent in his whole career, so he has no room to talk. It seems to me that he'\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.46 ms /   531 runs   (    0.01 ms per token, 97163.77 tokens per second)\nllama_perf_context_print:        load time =    3909.38 ms\nllama_perf_context_print: prompt eval time =    1012.44 ms /   275 tokens (    3.68 ms per token,   271.62 tokens per second)\nllama_perf_context_print:        eval time =   54425.82 ms /   255 runs   (  213.43 ms per token,     4.69 tokens per second)\nllama_perf_context_print:       total time =   55488.27 ms /   530 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56153.49",
        "tokens": "543",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A Dallas architect has taken 14 shipping containers and transformed them into a stunningly modern 3,700-square-foot three-bedroom home. The home, dubbed 'PV14', also boasts a 1,400 sq ft roof deck that can fit up to 150 people, a 40ft long swimming pool on the ground floor and a 360 degree view of White Rock Lake and the sparkling lights of the city's downtown. Matt Mooney wanted to stay true to the materials that became the foundation for his home. Dallas architects Matt Mooney and Michael Gooden have transformed 14 shipping containers into a stunningly modern home. The 3,700 sq ft three-bedroom home, dubbed 'PV14,' boasts a 40ft long swimming pool on the ground floor that reflects the two-story glass-paneled tower, also built out of the containers. The home also features a 1,400 sq ft roof deck that can fit up to 150 people and has a 360 degree view of White Rock Lake and downtown Dallas. Mooney wanted to stay true to the materials that \u00a0built his home and thus all the ceilings were left exposed. -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A Dallas architect has taken 14 shipping containers and transformed them into a stunningly modern 3,700-square-foot three-bedroom home. The home, dubbed 'PV14', also boasts a 1,400 sq ft roof deck that can fit up to 150 people, a 40ft long swimming pool on the ground floor and a 360 degree view of White Rock Lake and the sparkling lights of the city's downtown. Matt Mooney wanted to stay true to the materials that became the foundation for his home. Dallas architects Matt Mooney and Michael Gooden have transformed 14 shipping containers into a stunningly modern home. The 3,700 sq ft three-bedroom home, dubbed 'PV14,' boasts a 40ft long swimming pool on the ground floor that reflects the two-story glass-paneled tower, also built out of the containers. The home also features a 1,400 sq ft roof deck that can fit up to 150 people and has a 360 degree view of White Rock Lake and downtown Dallas. Mooney wanted to stay true to the materials that \u00a0built his home and thus all the ceilings were left exposed. The containers were stacked on top of each other to create the two-story home, with the living room, kitchen and dining area on the first floor and the bedrooms on the second floor. The home also features a 360 degree view of White Rock Lake and downtown Dallas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.41 ms /   544 runs   (    0.01 ms per token, 100628.93 tokens per second)\nllama_perf_context_print:        load time =    3902.85 ms\nllama_perf_context_print: prompt eval time =    1010.65 ms /   288 tokens (    3.51 ms per token,   284.96 tokens per second)\nllama_perf_context_print:        eval time =   55093.73 ms /   255 runs   (  216.05 ms per token,     4.63 tokens per second)\nllama_perf_context_print:       total time =   56153.49 ms /   543 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57770.00",
        "tokens": "502",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Jordon Ibe is poised to sign a new five-year contract with Liverpool by the end of the week. The 19-year-old has agreed terms on a deal which will keep him at Anfield until 2020. Ibe spent the first half of this season on loan at Derby County but returned to Liverpool in January and has made a notable impact in his eight appearances. Jordon Ibe has impressed since returning to Liverpool this season and will sign a new contract this week. Ibe battles with Emre Can (right) for the ball as Liverpool train at Melwood last week. A knee injury disrupted his run in the side and he was cup-tied for Sunday\u2019s FA Cup semi-final defeat to Aston Villa, but he is expected to return against West Brom this weekend. Liverpool also expect to announce Jordan Henderson's new contract this week and are close to finalising an extension for Martin Skrtel. Talks are ongoing with young defender Jon Flanagan too whose contract expires in June. Liverpool yesterday agreed a new three-year shirt sponsorship deal with Standard  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Jordon Ibe is poised to sign a new five-year contract with Liverpool by the end of the week. The 19-year-old has agreed terms on a deal which will keep him at Anfield until 2020. Ibe spent the first half of this season on loan at Derby County but returned to Liverpool in January and has made a notable impact in his eight appearances. Jordon Ibe has impressed since returning to Liverpool this season and will sign a new contract this week. Ibe battles with Emre Can (right) for the ball as Liverpool train at Melwood last week. A knee injury disrupted his run in the side and he was cup-tied for Sunday\u2019s FA Cup semi-final defeat to Aston Villa, but he is expected to return against West Brom this weekend. Liverpool also expect to announce Jordan Henderson's new contract this week and are close to finalising an extension for Martin Skrtel. Talks are ongoing with young defender Jon Flanagan too whose contract expires in June. Liverpool yesterday agreed a new three-year shirt sponsorship deal with Standard  Chartered worth \u00a340m.\n\nAnalysis:\n\n* Jordon Ibe is set to sign a new five-year contract with Liverpool by the end of the week.\n* The 19-year-old has agreed terms on a deal that will keep him at Anfield until 2020.\n* Ibe has impressed since returning to Liverpool this season, making eight appearances.\n* A knee injury disrupted his run in the side, but he is expected to return against West Brom this weekend.\n* Liverpool are also close to finalizing an extension for Martin Skrtel and are in talks with young defender Jon Flanagan.\n* The club yesterday agreed a new three-year shirt sponsorship deal with Standard Chartered worth \u00a340m.\n\nEvaluation:\n\n* Jordon Ibe's new contract is a positive sign for Liverpool, as it shows the club's faith in his potential.\n* The 19-year-old has shown promising form since returning to the club, and the new contract will give him the opportunity to continue developing his skills.\n* The club's agreement on a new shirt\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.47 ms /   503 runs   (    0.01 ms per token, 91939.32 tokens per second)\nllama_perf_context_print:        load time =    4439.21 ms\nllama_perf_context_print: prompt eval time =    1332.33 ms /   247 tokens (    5.39 ms per token,   185.39 tokens per second)\nllama_perf_context_print:        eval time =   56384.97 ms /   255 runs   (  221.12 ms per token,     4.52 tokens per second)\nllama_perf_context_print:       total time =   57770.00 ms /   502 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55935.18",
        "tokens": "515",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Comedian Rik Mayall failed to leave a valid will to deal with his \u00a31.2 million estate, which experts say could leave his family with a large inheritance tax bill. Mayall, who played scheming Tory MP Alan B\u2019Stard in ITV series The New Statesman, and a host of other comedy characters, died suddenly aged 56 last June. But probate records reveal he had no will \u2013 even though he suffered serious head injuries and nearly died in a quad-bike accident in 1998 \u2013 which could mean complications for his wife of 29 years, Barbara, and their three adult children. Scroll down for video. Legacy: Rik Mayall is pictured in 1999 with his wife Barbara and their children after his quad bike accident. His estate could be liable for tax of tens of thousands of pounds which would not have been payable if he had left everything to his wife. Lawyers have always advised that people should have a valid will to avoid tax pitfalls. Mayall, one of the leading \u2018alternative\u2019 comics of the 1980s, surprised doctors by making a good r -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Comedian Rik Mayall failed to leave a valid will to deal with his \u00a31.2 million estate, which experts say could leave his family with a large inheritance tax bill. Mayall, who played scheming Tory MP Alan B\u2019Stard in ITV series The New Statesman, and a host of other comedy characters, died suddenly aged 56 last June. But probate records reveal he had no will \u2013 even though he suffered serious head injuries and nearly died in a quad-bike accident in 1998 \u2013 which could mean complications for his wife of 29 years, Barbara, and their three adult children. Scroll down for video. Legacy: Rik Mayall is pictured in 1999 with his wife Barbara and their children after his quad bike accident. His estate could be liable for tax of tens of thousands of pounds which would not have been payable if he had left everything to his wife. Lawyers have always advised that people should have a valid will to avoid tax pitfalls. Mayall, one of the leading \u2018alternative\u2019 comics of the 1980s, surprised doctors by making a good rally after his accident, but he never fully recovered. Experts say that without a will, Mayall\u2019s estate will be divided between his wife and children, but the taxman will take a large chunk of the cash. Mayall\u2019s death has sparked a renewed call for people to make sure they have a valid will. Experts say that without a will, the taxman will take a large chunk of the cash. Mayall\u2019s death has sparked a renewed call for people to make sure they have a valid will. Experts say that without a will, the taxman will take a large chunk of the cash. Mayall\u2019s death has sparked a renewed call for people to make sure they have a valid will. Experts say that without a will, the taxman will take a large chunk of the cash. Mayall\u2019s death has sparked a renewed call for people to make sure they have a valid will. Experts say that without a will, the taxman will take a large chunk of the cash. Mayall\u2019s death has sparked a renewed call for people to make sure they have a valid will. Experts say that without a will, the tax\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.55 ms /   516 runs   (    0.01 ms per token, 92906.01 tokens per second)\nllama_perf_context_print:        load time =    4032.56 ms\nllama_perf_context_print: prompt eval time =    1005.17 ms /   260 tokens (    3.87 ms per token,   258.66 tokens per second)\nllama_perf_context_print:        eval time =   54880.49 ms /   255 runs   (  215.22 ms per token,     4.65 tokens per second)\nllama_perf_context_print:       total time =   55935.18 ms /   515 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56139.93",
        "tokens": "523",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Sanaa, Yemen (CNN)As soon as the buses parked, the people scrambled onto the airport tarmac. They carried duffel bags, plastic bags full of clothes and small suitcases. They'd already abandoned larger bags in a mound. Only carry-on luggage was allowed on this Air India plane. The passengers were mostly Indian nationals, plus Yemenis and people from other countries who had been working in Yemen's capital, Sanaa. They sprinted or walked with deliberation to the airplanes. The stress of living in a war zone showed on the passengers' faces. Nobody wanted to be left behind. They climbed the steps to the airliner and were greeted by the Air India crew, all wearing pressed uniforms, the flight attendants fully made up. After takeoff, the flight attendants went up and down the aisle offering drinks to the 188 evacuees as if they were on a vacation flight. But some passengers were already fast asleep, exhausted by the push to get out of Yemen, site of some of the most chaotic fighting in the Mideast. This s -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Sanaa, Yemen (CNN)As soon as the buses parked, the people scrambled onto the airport tarmac. They carried duffel bags, plastic bags full of clothes and small suitcases. They'd already abandoned larger bags in a mound. Only carry-on luggage was allowed on this Air India plane. The passengers were mostly Indian nationals, plus Yemenis and people from other countries who had been working in Yemen's capital, Sanaa. They sprinted or walked with deliberation to the airplanes. The stress of living in a war zone showed on the passengers' faces. Nobody wanted to be left behind. They climbed the steps to the airliner and were greeted by the Air India crew, all wearing pressed uniforms, the flight attendants fully made up. After takeoff, the flight attendants went up and down the aisle offering drinks to the 188 evacuees as if they were on a vacation flight. But some passengers were already fast asleep, exhausted by the push to get out of Yemen, site of some of the most chaotic fighting in the Mideast. This saga began in late March, when the Saudi-led coalition launched airstrikes against Yemen's Houthi rebels. The airstrikes have been relentless, and the humanitarian situation has grown dire. The UN has called it the world's worst humanitarian crisis. The evacuees were mostly Indians who had been working in Yemen, along with their families. They were flown to Muscat, Oman, and then on to New Delhi. The flight attendants chatted with the passengers in Hindi, asking about their families and how they were doing. One passenger, a young Indian man, said he was worried about his wife and two kids, who were still in Sanaa. \"I hope they are safe,\" he said, his voice trembling. \"I don't know if I'll ever see them again.\" The flight attendants listened and patted his hand. They had heard this story many times before. The passengers were grateful to be leaving, but they were also scared. They had seen the horrors of war up close, and they knew that Yemen was not safe. As the plane soared over the Mideast, the\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.49 ms /   524 runs   (    0.01 ms per token, 95481.05 tokens per second)\nllama_perf_context_print:        load time =    3902.18 ms\nllama_perf_context_print: prompt eval time =    1044.76 ms /   268 tokens (    3.90 ms per token,   256.52 tokens per second)\nllama_perf_context_print:        eval time =   55042.68 ms /   255 runs   (  215.85 ms per token,     4.63 tokens per second)\nllama_perf_context_print:       total time =   56139.93 ms /   523 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57731.25",
        "tokens": "512",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: An Italian priest has been sacked amid claims that he took part in gay orgies with a Swiss Guard. The prelate was sacked from a religious order in Taranto after one of his online lovers contacted the church authorities with a dossier containing alleged evidence of sex with prostitutes, role play and explicit gay web chats. The priest allegedly favoured sexual role-play scenarios in which his lover played the disciple Judas Escariot, and had to atone for his betrayal of Jesus by becoming his slave. Controversy: News of\u00a0prelate being sacked after an online lover contacted Taranto church authorities with a dossier containing alleged evidence of sex with prostitutes is just the latest sex scandal to engulf the Vatican. The diocese confirmed that the priest had been dismissed but claimed there were no other prelates involved. Turn-on: The priest allegedly favoured sexual role-play in which his lover played Judas Iscariot, and had to atone for his betrayal of Jesus (pictured) A 32-year-old man, from Rovi -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: An Italian priest has been sacked amid claims that he took part in gay orgies with a Swiss Guard. The prelate was sacked from a religious order in Taranto after one of his online lovers contacted the church authorities with a dossier containing alleged evidence of sex with prostitutes, role play and explicit gay web chats. The priest allegedly favoured sexual role-play scenarios in which his lover played the disciple Judas Escariot, and had to atone for his betrayal of Jesus by becoming his slave. Controversy: News of\u00a0prelate being sacked after an online lover contacted Taranto church authorities with a dossier containing alleged evidence of sex with prostitutes is just the latest sex scandal to engulf the Vatican. The diocese confirmed that the priest had been dismissed but claimed there were no other prelates involved. Turn-on: The priest allegedly favoured sexual role-play in which his lover played Judas Iscariot, and had to atone for his betrayal of Jesus (pictured) A 32-year-old man, from Rovi, in the province of Taranto, contacted the church authorities with a dossier containing alleged evidence of the priest's sexual activities. The priest, who has not been named, was a member of the Congregation of the Oratory of Taranto, a religious order founded by St Philip Neri in the 16th century. The man, who is believed to be the priest's online lover, claimed that the priest had been involved in gay orgies with other members of the Swiss Guard, the Vatican's elite security force. The allegations were made in a letter sent to the Taranto diocese, which confirmed that the priest had been dismissed from his duties. The diocese claimed that there were no other prelates involved in the scandal, but the news has sparked outrage among Catholics and non-Catholics alike. The Vatican has faced numerous allegations of sexual misconduct in recent years, including claims of sexual abuse by priests and cover-ups by church officials. The latest scandal has raised questions about the church's stance on homosexuality and its treatment of LGBTQ+ individuals. The priest's alleg\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.59 ms /   513 runs   (    0.01 ms per token, 91754.61 tokens per second)\nllama_perf_context_print:        load time =    3897.15 ms\nllama_perf_context_print: prompt eval time =    1220.52 ms /   257 tokens (    4.75 ms per token,   210.57 tokens per second)\nllama_perf_context_print:        eval time =   56459.64 ms /   255 runs   (  221.41 ms per token,     4.52 tokens per second)\nllama_perf_context_print:       total time =   57731.25 ms /   512 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56187.57",
        "tokens": "505",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Yet another scientific study has found no link between autism and the vaccine against measles, mumps and rubella (MMR) - even among high risk children. The findings, in the Journal of the American Medical Association (JAMA), are based on a study of about 95,000 young people. All those in the study had older siblings. Some of the elder children had autism and some did not. Numerous studies over the last 15 years have ruled out a link between the MMR vaccine and autism. Since autism can run in families, researchers decided to look at whether vaccines could make autism even more likely in children who had siblings with autism. They found that vaccines had no effect on autism risk, whether or not a sibling in the family was diagnosed. 'Consistent with studies in other populations, we observed no association between MMR vaccination and increased ASD risk among privately insured children,' said the study, led by Anjali Jain, a doctor in Falls Church, Virginia. 'We also found no evidence that receipt of e -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Yet another scientific study has found no link between autism and the vaccine against measles, mumps and rubella (MMR) - even among high risk children. The findings, in the Journal of the American Medical Association (JAMA), are based on a study of about 95,000 young people. All those in the study had older siblings. Some of the elder children had autism and some did not. Numerous studies over the last 15 years have ruled out a link between the MMR vaccine and autism. Since autism can run in families, researchers decided to look at whether vaccines could make autism even more likely in children who had siblings with autism. They found that vaccines had no effect on autism risk, whether or not a sibling in the family was diagnosed. 'Consistent with studies in other populations, we observed no association between MMR vaccination and increased ASD risk among privately insured children,' said the study, led by Anjali Jain, a doctor in Falls Church, Virginia. 'We also found no evidence that receipt of e...\nSource: Health News - Medical News Today - http://www.medicalnewstoday.com/articles/317778#.UzZmKLG1uUk\nSummary: A new study published in the Journal of the American Medical Association (JAMA) has found no link between the measles, mumps, and rubella (MMR) vaccine and the development of autism in children, even among those with older siblings who have the condition. The study, led by Anjali Jain, a doctor in Falls Church, Virginia, analyzed data from about 95,000 children aged 1-10 years old, including those with and without older siblings with autism. The researchers found that vaccination with the MMR vaccine had no effect on the risk of autism in either group. The study adds to the growing body of evidence that has consistently shown no link between the MMR vaccine and autism over the past 15 years. The findings are reassuring for parents and healthcare providers, as they provide further evidence that the MMR vaccine is safe and effective in\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.40 ms /   506 runs   (    0.01 ms per token, 93651.67 tokens per second)\nllama_perf_context_print:        load time =    3930.02 ms\nllama_perf_context_print: prompt eval time =    1019.44 ms /   250 tokens (    4.08 ms per token,   245.23 tokens per second)\nllama_perf_context_print:        eval time =   55117.81 ms /   255 runs   (  216.15 ms per token,     4.63 tokens per second)\nllama_perf_context_print:       total time =   56187.57 ms /   505 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "22766.28",
        "tokens": "345",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A 12-year-old girl was tied up, beaten and raped for by seven ISIS different fighters when she was kidnapped from her home in the Iraq region of Yazidi, a shocking new report has revealed. The suffering of Jalila - whose name has been changed - is just one of a number of harrowing accounts given by women who have escaped captured by the brutal Islamist organisation which is taking over swathes of territory in Syria and Iraq. The study undertaken by Human Rights Watch\u00a0found fighters were kidnapping women and children as young as eight, forcing them to marry and convert to Islam and raping them repeatedly \u2013 sometimes picking their victims\u2019 names out of a hand in a sick lottery. Scroll down for video. Yazidi women and girls have been separated from their families, forced to convert to Islam and repeatedly raped by ISIS fighters, the study from Human Rights Watch says. Their actions amount to war crimes. it adds. Yazidi children as young as eight have been abducted from their homes in northern Iraq, ra -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A 12-year-old girl was tied up, beaten and raped for by seven ISIS different fighters when she was kidnapped from her home in the Iraq region of Yazidi, a shocking new report has revealed. The suffering of Jalila - whose name has been changed - is just one of a number of harrowing accounts given by women who have escaped captured by the brutal Islamist organisation which is taking over swathes of territory in Syria and Iraq. The study undertaken by Human Rights Watch\u00a0found fighters were kidnapping women and children as young as eight, forcing them to marry and convert to Islam and raping them repeatedly \u2013 sometimes picking their victims\u2019 names out of a hand in a sick lottery. Scroll down for video. Yazidi women and girls have been separated from their families, forced to convert to Islam and repeatedly raped by ISIS fighters, the study from Human Rights Watch says. Their actions amount to war crimes. it adds. Yazidi children as young as eight have been abducted from their homes in northern Iraq, raped and forced to marry ISIS fighters, the report says.\n\nQuestion: What is the main idea of the article?\n\nAnswer: The main idea of the article is that ISIS fighters have been kidnapping, raping, and forcing Yazidi women and children to convert to Islam in the Iraq region. The article highlights the harrowing accounts of women who have escaped captivity and provides evidence that these actions amount to war crimes. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       2.17 ms /   346 runs   (    0.01 ms per token, 159447.00 tokens per second)\nllama_perf_context_print:        load time =    3974.24 ms\nllama_perf_context_print: prompt eval time =    1025.98 ms /   245 tokens (    4.19 ms per token,   238.80 tokens per second)\nllama_perf_context_print:        eval time =   21719.05 ms /   100 runs   (  217.19 ms per token,     4.60 tokens per second)\nllama_perf_context_print:       total time =   22766.28 ms /   345 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 102 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56163.79",
        "tokens": "496",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: This is the man who allegedly opened fire in the campus library of a North Carolina college on Monday morning, leaving one person dead. Kenneth Stancil, 20, is a former student who did not graduate from Wayne Community College in Goldsboro, where he is accused of shooting dead a man who working in the school's print shop. According to police, Stancil walked into the building at 8am carrying a long-barreled\u00a0gun, went up to the third floor, and fired one shot, which killed long-time employee Ron Lane. It is not known whether Stancil and Lane are connected or related. Scroll Down for Video. Manhunt: Police believe the shooter was Kenneth Stancil, a former student who did not graduate from Wayne Community College in Goldsboro, North Carolina. He allegedly entered the building at 8am and fired one shot. Killed: The single bullet fired on Monday morning killed Rodney Lane (pictured left and right) who was a long-time employee at the Wayne Community College print shop. It is not known if he was connected  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: This is the man who allegedly opened fire in the campus library of a North Carolina college on Monday morning, leaving one person dead. Kenneth Stancil, 20, is a former student who did not graduate from Wayne Community College in Goldsboro, where he is accused of shooting dead a man who working in the school's print shop. According to police, Stancil walked into the building at 8am carrying a long-barreled\u00a0gun, went up to the third floor, and fired one shot, which killed long-time employee Ron Lane. It is not known whether Stancil and Lane are connected or related. Scroll Down for Video. Manhunt: Police believe the shooter was Kenneth Stancil, a former student who did not graduate from Wayne Community College in Goldsboro, North Carolina. He allegedly entered the building at 8am and fired one shot. Killed: The single bullet fired on Monday morning killed Rodney Lane (pictured left and right) who was a long-time employee at the Wayne Community College print shop. It is not known if he was connected  to Stancil.\n\nKenneth Stancil, 20, is the man accused of opening fire in the campus library of a North Carolina college on Monday morning, leaving one person dead.\n\nStancil, a former student who did not graduate from Wayne Community College in Goldsboro, allegedly entered the building at 8am carrying a long-barreled gun and fired one shot, which killed long-time employee Ron Lane.\n\nLane, who worked in the school's print shop, was reportedly shot once by Stancil, who is believed to have entered the building alone.\n\nIt is not known whether Stancil and Lane were connected or related.\n\nPolice have launched a manhunt for Stancil, who is believed to have fled the scene after the shooting.\n\nStancil is described as a white male, 5 feet 10 inches tall and weighing around 150 pounds. He has short brown hair and blue eyes.\n\nAnyone with information on Stancil's whereabouts is urged to contact the Wayne County Sheriff's Office at (919) 731-\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   497 runs   (    0.01 ms per token, 91175.93 tokens per second)\nllama_perf_context_print:        load time =    3892.00 ms\nllama_perf_context_print: prompt eval time =    1004.10 ms /   241 tokens (    4.17 ms per token,   240.02 tokens per second)\nllama_perf_context_print:        eval time =   55110.21 ms /   255 runs   (  216.12 ms per token,     4.63 tokens per second)\nllama_perf_context_print:       total time =   56163.79 ms /   496 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "33746.24",
        "tokens": "402",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss, because of 'negative publicity' from the grooming scandal. The council will now offer up to \u00a3200,000 to candidates, rather than the \u00a3160,000 enjoyed by former chief boss Martin Kimber. He left in December after a council report said 1,400 children had suffered horrific sexual abuse over a 16-year period. The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss. The rest of the cabinet resigned in February, after a Government report said the council was 'not fit for purpose' and 'in denial' about exploitation, mainly of white girls by men of Pakistani origin. HR chief Phil Howe said: 'Due to recent negative publicity, it may be necessary to consider additional incentives.' Councillor Caven Vines, leader of the Ukip group, said he was uncomfortable with the idea. -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss, because of 'negative publicity' from the grooming scandal. The council will now offer up to \u00a3200,000 to candidates, rather than the \u00a3160,000 enjoyed by former chief boss Martin Kimber. He left in December after a council report said 1,400 children had suffered horrific sexual abuse over a 16-year period. The new chief executive of Rotherham council could be paid up to \u00a340,000 more than the outgoing boss. The rest of the cabinet resigned in February, after a Government report said the council was 'not fit for purpose' and 'in denial' about exploitation, mainly of white girls by men of Pakistani origin. HR chief Phil Howe said: 'Due to recent negative publicity, it may be necessary to consider additional incentives.' Councillor Caven Vines, leader of the Ukip group, said he was uncomfortable with the idea. 'I don't think it's right to pay more money to someone who's coming in to clean up the mess that's been left,' he said.\n\nMy questions:\n\n1. What is the reason for the increase in the salary of the new chief executive of Rotherham council?\n2. Who is the new chief executive of Rotherham council?\n3. What is the amount of money that the council is offering to candidates for the position of chief executive?\n4. Why did the rest of the cabinet resign in February?\n5. What is the issue of exploitation that the council is facing?\n6. Who is responsible for the negative publicity that the council is facing? [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.30 ms /   403 runs   (    0.01 ms per token, 122121.21 tokens per second)\nllama_perf_context_print:        load time =    3955.99 ms\nllama_perf_context_print: prompt eval time =    1015.82 ms /   250 tokens (    4.06 ms per token,   246.11 tokens per second)\nllama_perf_context_print:        eval time =   32699.17 ms /   152 runs   (  215.13 ms per token,     4.65 tokens per second)\nllama_perf_context_print:       total time =   33746.24 ms /   402 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 154 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "35540.96",
        "tokens": "427",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose last weekend - and Sportsmail can exclusively reveal what the Spanish midfielder's shield will look like. Fabregas suffered the break during Chelsea's win over Stoke City on Saturday, taking a hit to the face from Charlie Adam's forearm minutes before the Scot struck a 66-yard wondergoal at Stamford Bridge. The Spaniard has since travelled to Milan to see a specialist, who produced masks for Fernando Torres, Petr Cech and Demba Ba previously, in the hope of returning to action as soon as possible. Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose, and this is it. The mask, branded with Fabregas's initial and squad number 'C4', was made by the Ortholabsport. Fabregas wrote this note to the orthopedic centre and its craftsmen\u00a0Stefano Duchini and Lucilla Pezzoni. The Spaniard travelled to Milan on Tuesday with Chelsea medical director Paco Biosca to have the mask fitted.  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose last weekend - and Sportsmail can exclusively reveal what the Spanish midfielder's shield will look like. Fabregas suffered the break during Chelsea's win over Stoke City on Saturday, taking a hit to the face from Charlie Adam's forearm minutes before the Scot struck a 66-yard wondergoal at Stamford Bridge. The Spaniard has since travelled to Milan to see a specialist, who produced masks for Fernando Torres, Petr Cech and Demba Ba previously, in the hope of returning to action as soon as possible. Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose, and this is it. The mask, branded with Fabregas's initial and squad number 'C4', was made by the Ortholabsport. Fabregas wrote this note to the orthopedic centre and its craftsmen\u00a0Stefano Duchini and Lucilla Pezzoni. The Spaniard travelled to Milan on Tuesday with Chelsea medical director Paco Biosca to have the mask fitted.  Fabregas is expected to be out of action for around three weeks, but the mask will allow him to play while protecting his nose.\n\nIn this article, the author is summarizing the news that Cesc Fabregas will wear a special protective face mask for Chelsea after breaking his nose during a recent match. The author provides details on the design of the mask, which is branded with Fabregas's initial and squad number, and explains that the Spaniard travelled to Milan to see a specialist who produced masks for other Chelsea players in the past. The author also mentions that Fabregas is expected to be out of action for around three weeks, but the mask will allow him to play while protecting his nose. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.40 ms /   428 runs   (    0.01 ms per token, 125734.43 tokens per second)\nllama_perf_context_print:        load time =    3951.58 ms\nllama_perf_context_print: prompt eval time =    1035.21 ms /   268 tokens (    3.86 ms per token,   258.88 tokens per second)\nllama_perf_context_print:        eval time =   34473.30 ms /   159 runs   (  216.81 ms per token,     4.61 tokens per second)\nllama_perf_context_print:       total time =   35540.96 ms /   427 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 161 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "55986.78",
        "tokens": "432",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Andros Townsend enjoyed silencing the critics with his wonder strike for England, saying naysayers like Paul Merson provided the perfect motivation for him in Italy. This has been a topsy-turvy season for the 23-year-old, who has yet to reach the heights he scaled when he first burst onto the international scene. Three Lions manager Roy Hodgson has, however, kept faith with the Tottenham winger - belief he paid back in quite exceptional fashion at the Juventus Stadium. Andros Townsend scores England's equaliser in their 1-1 friendly draw with Italy in Turin on Tuesday night. Townsend celebrates his strike with Tottenham Hotspur team-mates Ryan Mason (left) and Kyle Walker. -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Andros Townsend enjoyed silencing the critics with his wonder strike for England, saying naysayers like Paul Merson provided the perfect motivation for him in Italy. This has been a topsy-turvy season for the 23-year-old, who has yet to reach the heights he scaled when he first burst onto the international scene. Three Lions manager Roy Hodgson has, however, kept faith with the Tottenham winger - belief he paid back in quite exceptional fashion at the Juventus Stadium. Andros Townsend scores England's equaliser in their 1-1 friendly draw with Italy in Turin on Tuesday night. Townsend celebrates his strike with Tottenham Hotspur team-mates Ryan Mason (left) and Kyle Walker. \"I've had a lot of criticism this season, especially from Paul Merson, so it's nice to prove him wrong,\" said Townsend, who has struggled to live up to his early promise. \"It's been a tough season for me, but I've worked hard and I'm glad I could repay the manager's faith in me.\" Townsend's stunning strike, which gave England a 1-1 draw in Turin, was his first international goal since scoring against Montenegro in 2015. \"It's been a while since I scored for England, so it's great to get that monkey off my back,\" he said. \"I'm just glad I could help the team get a point in a difficult game.\" Townsend's strike was a rare bright spot in a match that saw England struggle to create chances against a well-organised Italian side. \"It was a tough game, but we're happy with a point,\" said Hodgson. \"Andros has had a difficult season, but he's worked hard and he's got his reward. He's a talented player and he's shown\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.37 ms /   433 runs   (    0.01 ms per token, 80633.15 tokens per second)\nllama_perf_context_print:        load time =    3950.78 ms\nllama_perf_context_print: prompt eval time =     998.21 ms /   177 tokens (    5.64 ms per token,   177.32 tokens per second)\nllama_perf_context_print:        eval time =   54937.08 ms /   255 runs   (  215.44 ms per token,     4.64 tokens per second)\nllama_perf_context_print:       total time =   55986.78 ms /   432 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "15422.05",
        "tokens": "346",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A husband accused of trying to murder his cheating wife wrote her an extraordinary letter while remanded in prison asking her to send him a suit for his trial, a court heard yesterday. Joseph O'Riordan, 73, stabbed his 47-year-old wife Amanda eight times after he discovered she had been having an affair with a postman. She was left with life-threatening injuries to her torso, chest, arms and back. Joseph O'Riordan stabbed his wife of ten years Amanda (left) with a seven inch kitchen knife eight times. O'Riordan, who was a councillor at the time, rang an ambulance after the attack in October last year and admitted what he had done during the 999 call. Yesterday it emerged that four months later, while on remand, O'Riordan wrote to his wife from jail asking her to 'get my things together' before for his trial. Meanwhile Mrs O'Riordan, during questioning by her husband's defence team in court yesterday, sobbed: 'I did not want to hurt Joe. I never wanted to hurt him\u2026 even now. All of this is my fault  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A husband accused of trying to murder his cheating wife wrote her an extraordinary letter while remanded in prison asking her to send him a suit for his trial, a court heard yesterday. Joseph O'Riordan, 73, stabbed his 47-year-old wife Amanda eight times after he discovered she had been having an affair with a postman. She was left with life-threatening injuries to her torso, chest, arms and back. Joseph O'Riordan stabbed his wife of ten years Amanda (left) with a seven inch kitchen knife eight times. O'Riordan, who was a councillor at the time, rang an ambulance after the attack in October last year and admitted what he had done during the 999 call. Yesterday it emerged that four months later, while on remand, O'Riordan wrote to his wife from jail asking her to 'get my things together' before for his trial. Meanwhile Mrs O'Riordan, during questioning by her husband's defence team in court yesterday, sobbed: 'I did not want to hurt Joe. I never wanted to hurt him\u2026 even now. All of this is my fault 100 per cent.'\n\nQuestion: What did Joseph O'Riordan ask his wife to do in the letter he wrote to her while he was in prison?\nAnswer: In the letter, Joseph O'Riordan asked his wife Amanda to \"get my things together\" before his trial. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.49 ms /   347 runs   (    0.00 ms per token, 233512.79 tokens per second)\nllama_perf_context_print:        load time =    3962.64 ms\nllama_perf_context_print: prompt eval time =    1032.19 ms /   279 tokens (    3.70 ms per token,   270.30 tokens per second)\nllama_perf_context_print:        eval time =   14375.11 ms /    67 runs   (  214.55 ms per token,     4.66 tokens per second)\nllama_perf_context_print:       total time =   15422.05 ms /   346 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 69 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "6655.24",
        "tokens": "274",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The UK is to open its first three proton beam therapy centres after cancer-stricken five-year-old Ashya King made a miracle recovery after receiving the treatment. Cardiff-based Proton Partners International Ltd, is to open the treatment centres, which will be available to NHS cancer patients, in Cardiff, London and Northumberland by 2017, with the first in the Welsh capital being operational by next year. The announcement comes just weeks after the parents of brain cancer sufferer Ashya told of how he was given the 'all-clear' after receiving proton bean therapy at a centre in Prague. Scroll down for video. Five-year-old Ashya King, pictured in October, who suffered from brain cancer but has recently been given the all-clear after receiving proton beam therapy. Three centres in the UK in Cardiff, London and Northumberland, are set to open offering proton beam therapy, which Ashya, pictured, received. Both Brett and Naghmeh King sparked an international manhunt last year after taking their son out  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The UK is to open its first three proton beam therapy centres after cancer-stricken five-year-old Ashya King made a miracle recovery after receiving the treatment. Cardiff-based Proton Partners International Ltd, is to open the treatment centres, which will be available to NHS cancer patients, in Cardiff, London and Northumberland by 2017, with the first in the Welsh capital being operational by next year. The announcement comes just weeks after the parents of brain cancer sufferer Ashya told of how he was given the 'all-clear' after receiving proton bean therapy at a centre in Prague. Scroll down for video. Five-year-old Ashya King, pictured in October, who suffered from brain cancer but has recently been given the all-clear after receiving proton beam therapy. Three centres in the UK in Cardiff, London and Northumberland, are set to open offering proton beam therapy, which Ashya, pictured, received. Both Brett and Naghmeh King sparked an international manhunt last year after taking their son out 200 miles away from his hospital in Southampton to Prague for the treatment.\n\n\n\n\n\n\n [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       0.58 ms /   275 runs   (    0.00 ms per token, 474956.82 tokens per second)\nllama_perf_context_print:        load time =    3971.67 ms\nllama_perf_context_print: prompt eval time =    1017.70 ms /   248 tokens (    4.10 ms per token,   243.69 tokens per second)\nllama_perf_context_print:        eval time =    5630.69 ms /    26 runs   (  216.56 ms per token,     4.62 tokens per second)\nllama_perf_context_print:       total time =    6655.24 ms /   274 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 28 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "32299.53",
        "tokens": "415",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments because it is far cheaper than training them in Britain. Managers from the London Ambulance Service, the largest in the NHS, have just filled 225 vacant posts with applicants from Sydney and Melbourne. They were recruited on the back of a series of practical demonstrations on dummies, a written exam and an interview that lasted in all between four and six hours. Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments. The selection process, held at five-star hotels, was so quick that by 8am the next day applicants had received an email informing them if they had a job. It is the latest example of how the NHS is recruiting overseas because it has failed to train enough staff in Britain to meet the needs of the rising and ageing population. Hospital managers have been flying out to Spain and Portugal t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments because it is far cheaper than training them in Britain. Managers from the London Ambulance Service, the largest in the NHS, have just filled 225 vacant posts with applicants from Sydney and Melbourne. They were recruited on the back of a series of practical demonstrations on dummies, a written exam and an interview that lasted in all between four and six hours. Ambulance bosses are routinely making a 21,500-mile round trip to Australia to hire paramedics on \u00a34,500 \u2018golden hello\u2019 payments. The selection process, held at five-star hotels, was so quick that by 8am the next day applicants had received an email informing them if they had a job. It is the latest example of how the NHS is recruiting overseas because it has failed to train enough staff in Britain to meet the needs of the rising and ageing population. Hospital managers have been flying out to Spain and Portugal t...\nSource: The Telegraph\nSummary: The London Ambulance Service is recruiting paramedics from Australia due to a shortage of trained staff in the UK. The service is offering a \u00a34,500 \"golden hello\" payment to attract applicants, who are required to undergo a practical demonstration, written exam, and interview. The selection process is held at five-star hotels and applicants receive an email informing them of their job status by 8am the next day. This is the latest example of the NHS recruiting overseas due to a lack of trained staff in the UK to meet the needs of the aging population. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.12 ms /   416 runs   (    0.01 ms per token, 133162.61 tokens per second)\nllama_perf_context_print:        load time =    3986.80 ms\nllama_perf_context_print: prompt eval time =    1035.28 ms /   271 tokens (    3.82 ms per token,   261.77 tokens per second)\nllama_perf_context_print:        eval time =   31234.63 ms /   144 runs   (  216.91 ms per token,     4.61 tokens per second)\nllama_perf_context_print:       total time =   32299.53 ms /   415 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 146 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56158.96",
        "tokens": "492",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Chelsea have turned to the latest fitness technology to carry them over the line towards the Premier League title following a relentless campaign. Having led the chasing pack for most of the season, the Blues are currently  seven points clear with a game in hand over their closest rivals and seven games remaining ahead of the weekend's visit of Manchester United. Despite their commanding position, Jose Mourinho's side is leaving nothing to chance as players hit the gym following the announcement of a partnership with leading wellness solutions provider Technogym - who have kitted out their Cobham training ground with state-of-the-art equipment. Kurt Zouma (left), Nemanja Matic (top right) and Eden Hazard (bottom right) work out at Cobham. Players including Eden Hazard and Nemanja Matic have been pictured being put through their paces while the Stamford Bridge club's Twitter account features a video of young defender Kurt Zouma working out at Cobham. Carlos Lalin, Chelsea assistant first team fitnes -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Chelsea have turned to the latest fitness technology to carry them over the line towards the Premier League title following a relentless campaign. Having led the chasing pack for most of the season, the Blues are currently  seven points clear with a game in hand over their closest rivals and seven games remaining ahead of the weekend's visit of Manchester United. Despite their commanding position, Jose Mourinho's side is leaving nothing to chance as players hit the gym following the announcement of a partnership with leading wellness solutions provider Technogym - who have kitted out their Cobham training ground with state-of-the-art equipment. Kurt Zouma (left), Nemanja Matic (top right) and Eden Hazard (bottom right) work out at Cobham. Players including Eden Hazard and Nemanja Matic have been pictured being put through their paces while the Stamford Bridge club's Twitter account features a video of young defender Kurt Zouma working out at Cobham. Carlos Lalin, Chelsea assistant first team fitnes coach, said: \"We are delighted to be working with Technogym to enhance our training and recovery programmes. \"Their cutting-edge technology and expertise will help us to optimise our players' performance and well-being, which is crucial in the intense and demanding environment of the Premier League.\"\n\nAnalysis: Chelsea have been using the latest fitness technology to help them maintain their lead in the Premier League. The club has partnered with Technogym, a leading wellness solutions provider, to kit out their training ground with state-of-the-art equipment. This includes treadmills, stationary bikes, and weight machines, among other things. The players, including Eden Hazard and Nemanja Matic, have been putting in extra effort in the gym to stay in top shape. The club's assistant first-team fitness coach, Carlos Lalin, believes that the partnership with Technogym will help the team optimize their performance and well-being. He said that the cutting-edge technology and expertise of Technogym will be crucial in the intense and demanding environment of the Premier League.\n\nThe use\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.43 ms /   493 runs   (    0.01 ms per token, 90842.09 tokens per second)\nllama_perf_context_print:        load time =    3926.63 ms\nllama_perf_context_print: prompt eval time =     997.77 ms /   237 tokens (    4.21 ms per token,   237.53 tokens per second)\nllama_perf_context_print:        eval time =   55111.49 ms /   255 runs   (  216.12 ms per token,     4.63 tokens per second)\nllama_perf_context_print:       total time =   56158.96 ms /   492 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "14442.61",
        "tokens": "296",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Conversation is limited to \u2018mummy\u2019 and \u2018bye bye\u2019 and simply standing up unaided is a major achievement. But 11-month-old babies have already mastered the fundamentals of physics. Baby boys and girls were surprised when a ball seemed to disappear through a solid wall. Their curiosity did not end there. When they were then given the ball to play with, they repeatedly banged it on a table in an apparent attempt to work out its secret. Scroll down for video. Babies in the study reacted with surprise when objects like this toy car appeared to break the laws of gravity. The babies were similarly perplexed by a toy car that appeared to defy the laws of gravity and float in thin air. And given it to play with later, they repeatedly dropped it on the floor. Importantly, the babies shown the magic tricks were more interested in playing with the ball than a brand new toy that they had never seen before. Babies might be smarter than we had previously thought and can understand complex social situations from th -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Conversation is limited to \u2018mummy\u2019 and \u2018bye bye\u2019 and simply standing up unaided is a major achievement. But 11-month-old babies have already mastered the fundamentals of physics. Baby boys and girls were surprised when a ball seemed to disappear through a solid wall. Their curiosity did not end there. When they were then given the ball to play with, they repeatedly banged it on a table in an apparent attempt to work out its secret. Scroll down for video. Babies in the study reacted with surprise when objects like this toy car appeared to break the laws of gravity. The babies were similarly perplexed by a toy car that appeared to defy the laws of gravity and float in thin air. And given it to play with later, they repeatedly dropped it on the floor. Importantly, the babies shown the magic tricks were more interested in playing with the ball than a brand new toy that they had never seen before. Babies might be smarter than we had previously thought and can understand complex social situations from th early stages of life.\n\nQuestion: What did the babies in the study do when they were given a ball to play with?\n\nAnswer: When given the ball to play with, the babies repeatedly banged it on a table in an apparent attempt to work out its secret. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80806 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.35 ms /   297 runs   (    0.00 ms per token, 220326.41 tokens per second)\nllama_perf_context_print:        load time =    3977.70 ms\nllama_perf_context_print: prompt eval time =    1015.18 ms /   234 tokens (    4.34 ms per token,   230.50 tokens per second)\nllama_perf_context_print:        eval time =   13413.46 ms /    62 runs   (  216.35 ms per token,     4.62 tokens per second)\nllama_perf_context_print:       total time =   14442.61 ms /   296 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 64 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "16784.54",
        "tokens": "328",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Manny Pacquiao decided to train on the streets of Los Angeles on Friday as he ran along the trail at Griffith Park ahead of his highly-anticipated fight against Floyd Mayweather. The Filipino swapped the gym for the great outdoors as he worked on his fitness and stamina in preparation for what could be a lengthy bout against his welterweight rival. Pacquiao was spotted with several fitness fanatics during his run around one of the North America's largest urban parks. Manny Pacquiao is joined by several fitness fanatics as he jogs around Griffith Park. Pacquiao was joined by his dog Pacman as he ran around one of North America's largest urban parks. The Filipino boxer shows off his skills by shadow boxing in front of interested spectators. The 36-year-old was joined by his pet dog Pacman as he continued to gear up for his May 2 encounter against the undefeated Mayweather. As well as working on his aerobic endurance, Pacquiao was also tested on his core strength and showed off his impressive footwork -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Manny Pacquiao decided to train on the streets of Los Angeles on Friday as he ran along the trail at Griffith Park ahead of his highly-anticipated fight against Floyd Mayweather. The Filipino swapped the gym for the great outdoors as he worked on his fitness and stamina in preparation for what could be a lengthy bout against his welterweight rival. Pacquiao was spotted with several fitness fanatics during his run around one of the North America's largest urban parks. Manny Pacquiao is joined by several fitness fanatics as he jogs around Griffith Park. Pacquiao was joined by his dog Pacman as he ran around one of North America's largest urban parks. The Filipino boxer shows off his skills by shadow boxing in front of interested spectators. The 36-year-old was joined by his pet dog Pacman as he continued to gear up for his May 2 encounter against the undefeated Mayweather. As well as working on his aerobic endurance, Pacquiao was also tested on his core strength and showed off his impressive footwork by shadow boxing in front of a small crowd of onlookers.\n\nQuestion: What was Manny Pacquiao doing on Friday?\n\nAnswer: Manny Pacquiao was training on the streets of Los Angeles on Friday by running along the trail at Griffith Park ahead of his highly-anticipated fight against Floyd Mayweather. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.61 ms /   329 runs   (    0.00 ms per token, 204474.83 tokens per second)\nllama_perf_context_print:        load time =    3923.46 ms\nllama_perf_context_print: prompt eval time =    1001.86 ms /   254 tokens (    3.94 ms per token,   253.53 tokens per second)\nllama_perf_context_print:        eval time =   15766.80 ms /    74 runs   (  213.06 ms per token,     4.69 tokens per second)\nllama_perf_context_print:       total time =   16784.54 ms /   328 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 76 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "53733.48",
        "tokens": "479",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A cat who had been stuck in a wall at a Cairo train station for five years has been freed at last. Biso had squeezed into the hole in the wall at the Mohamed Naguib metro station as a kitten in 2010 - but then soon became to big to escape. He survived thanks to an elderly man named Uncle Abdo, who gave the trapped cat water and fed him scraps of food  every day. Scroll down for video. Biso the cat, who was stuck behind a wall in a Cairo train station for five years, has finally been freed. The cat survived thanks to an elderly man called Uncle Abdo who fed him scraps of food every day. Now, the feline in finally free thanks to a social media campaign, according to the Cairo Post. Animal rights activist\u00a0Mounira Shehata told the newspaper the smell when the wall was removed was 'heinous'. She said: 'It was like a tomb. There were black worms along four meters of the edge, where Uncle Abdo was able to put in food for Biso.' Uncle Abdo, who named the cat, said Biso would regularly hide from bigger cats -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A cat who had been stuck in a wall at a Cairo train station for five years has been freed at last. Biso had squeezed into the hole in the wall at the Mohamed Naguib metro station as a kitten in 2010 - but then soon became to big to escape. He survived thanks to an elderly man named Uncle Abdo, who gave the trapped cat water and fed him scraps of food  every day. Scroll down for video. Biso the cat, who was stuck behind a wall in a Cairo train station for five years, has finally been freed. The cat survived thanks to an elderly man called Uncle Abdo who fed him scraps of food every day. Now, the feline in finally free thanks to a social media campaign, according to the Cairo Post. Animal rights activist\u00a0Mounira Shehata told the newspaper the smell when the wall was removed was 'heinous'. She said: 'It was like a tomb. There were black worms along four meters of the edge, where Uncle Abdo was able to put in food for Biso.' Uncle Abdo, who named the cat, said Biso would regularly hide from bigger cats in the area. He added: 'I was surprised he didn't get out earlier. He was very clever and resourceful.' Biso was rescued after a social media campaign, with many Egyptians calling for his release. The cat is now in a shelter, where he is being cared for until a new home can be found.\n\nEssential Words:\n\n1. Stuck\n2. Survived\n3. Freed\n4. Trapped\n5. Station\n6. Cat\n7. Wall\n8. Elderly\n9. Scraps\n10. Campaign\n11. Shelter\n12. Home\n\nSummary: A cat named Biso was stuck behind a wall in a Cairo train station for five years. He survived thanks to an elderly man named Uncle Abdo, who fed him every day. After a social media campaign, Biso was finally freed and is now in a shelter until a new home can be found. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.67 ms /   480 runs   (    0.01 ms per token, 102717.74 tokens per second)\nllama_perf_context_print:        load time =    3943.95 ms\nllama_perf_context_print: prompt eval time =    1007.03 ms /   267 tokens (    3.77 ms per token,   265.14 tokens per second)\nllama_perf_context_print:        eval time =   52683.91 ms /   212 runs   (  248.51 ms per token,     4.02 tokens per second)\nllama_perf_context_print:       total time =   53733.48 ms /   479 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 214 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "25184.45",
        "tokens": "381",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: At least 35 workers have been killed at a dairy factory in Yemen as Saudi Arabian warplanes continue to bomb\u00a0Shiite rebel positions in both the north and the south of the country. Today's airstrikes targeted rebel-controlled army camps in the Red Sea port city of Hodeida, and hit warehouses belonging to a factory that produces dairy products. The jets drew return fire from anti-aircraft guns as the Saudi-led campaign to defeat the Iran-backed Shiite rebels loyal to Yemen's deposed president Ali Abdullah Saleh entered its seventh day. The attack on the dairy factory comes as Saudi Arabia hinted that it was prepared to launch a ground invasion in Yemen in the hope of forcing the Houthi rebels to allow\u00a0current president, Abed Rabbo Mansour Hadi to return to the country, after he fled\u00a0the southern port city of Aden last week. Hit: Smoke rises from the dairy factory hit by an air strike in Yemen's western port city of Houdieda today. Blast:\u00a0Today's airstrikes targeted rebel-controlled army camps in the  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: At least 35 workers have been killed at a dairy factory in Yemen as Saudi Arabian warplanes continue to bomb\u00a0Shiite rebel positions in both the north and the south of the country. Today's airstrikes targeted rebel-controlled army camps in the Red Sea port city of Hodeida, and hit warehouses belonging to a factory that produces dairy products. The jets drew return fire from anti-aircraft guns as the Saudi-led campaign to defeat the Iran-backed Shiite rebels loyal to Yemen's deposed president Ali Abdullah Saleh entered its seventh day. The attack on the dairy factory comes as Saudi Arabia hinted that it was prepared to launch a ground invasion in Yemen in the hope of forcing the Houthi rebels to allow\u00a0current president, Abed Rabbo Mansour Hadi to return to the country, after he fled\u00a0the southern port city of Aden last week. Hit: Smoke rises from the dairy factory hit by an air strike in Yemen's western port city of Houdieda today. Blast:\u00a0Today's airstrikes targeted rebel-controlled army camps in the  Red Sea port city of Hodeida, and hit warehouses belonging to a factory that produces dairy products. Dead: At least 35 workers were killed in the attack on the dairy factory.\n\nQuestion: What is the location of the dairy factory that was hit by the airstrikes in Yemen?\n\nAnswer: According to the article, the dairy factory that was hit by the airstrikes is located in the western port city of Hodeida. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       2.31 ms /   382 runs   (    0.01 ms per token, 165511.27 tokens per second)\nllama_perf_context_print:        load time =    3926.53 ms\nllama_perf_context_print: prompt eval time =    1012.32 ms /   277 tokens (    3.65 ms per token,   273.63 tokens per second)\nllama_perf_context_print:        eval time =   24149.76 ms /   104 runs   (  232.21 ms per token,     4.31 tokens per second)\nllama_perf_context_print:       total time =   25184.45 ms /   381 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 106 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56938.52",
        "tokens": "507",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. As officials didn't want to risk running trains under the over-hanging truck, replacement shuttle buses were run for southbound passengers with long queues forming. Scroll down for video. Aerieal shot: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. Overhanging: The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. Carol Hullum, who lives near the crash site, said the collision caused headaches for locals trying to get to work. 'I was just thinking too that the com -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. As officials didn't want to risk running trains under the over-hanging truck, replacement shuttle buses were run for southbound passengers with long queues forming. Scroll down for video. Aerieal shot: Hundreds of morning commuters in Boston were delayed today after a truck crashed on an elevated highway over subway and rail tracks. Overhanging: The box truck collided with a car on Interstate 93 southbound in the city's Dorchester neighborhood at about 6am, and both drivers were taken to hospital with non-life-threatening injuries. Carol Hullum, who lives near the crash site, said the collision caused headaches for locals trying to get to work. 'I was just thinking too that the comute is going to be a nightmare today, and it is,' she said. 'It's a mess. It's a real mess.' Commuters were forced to take replacement shuttle buses, with long queues forming. 'It's a big delay, it's a big inconvenience,' said one commuter. 'I'm just trying to get to work on time, but it's not happening today.' The crash caused delays for both subway and rail passengers, with some trains delayed by up to 30 minutes. The incident is under investigation by police.\nSummary: A truck crash on an elevated highway in Boston caused delays for hundreds of morning commuters, with both drivers taken to the hospital with non-life-threatening injuries. Replacement shuttle buses were run for southbound passengers, with long queues forming. The crash caused delays for both subway and rail passengers, with some trains delayed by up to 30 minutes. The incident is under investigation by police.\nKeywords: Boston, truck crash, morning commute, delays, replacement shuttle buses, non-life-threatening\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.56 ms /   508 runs   (    0.01 ms per token, 91334.05 tokens per second)\nllama_perf_context_print:        load time =    3970.36 ms\nllama_perf_context_print: prompt eval time =    1035.23 ms /   252 tokens (    4.11 ms per token,   243.42 tokens per second)\nllama_perf_context_print:        eval time =   55852.33 ms /   255 runs   (  219.03 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   56938.52 ms /   507 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56420.74",
        "tokens": "515",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Saracens director of rugby Mark McCall lauded his young guns after their latest European heartache before declaring he has no intention of overspending in a competitive post-World Cup transfer market. McCall watched his side, which contained five English-qualified forwards in the starting pack, battle in vain before losing 13-9 to the Clermont on Saturday. Saracens\u2019 millionaire chairman Nigel Wray spent much of last week repeating his belief the cap should be scrapped in order for Saracens to compete at Europe\u2019s top table, raising expectations they could be set to land a \u2018marquee\u2019 player from outside the league whose wages would sit outside next season\u2019s \u00a35.5m cap. Maro Itoje (second left) was one of five England-qualified forwards in the Saracens pack that faced Clermont. Mako Vunipola tries to fend off Clermont lock Jamie Cudmore during a ferocious contest. Saracens director of rugby Mark McCall saw his side come agonisingly close to reaching the final. But McCall said: \u2018We know where we\u2019d like t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Saracens director of rugby Mark McCall lauded his young guns after their latest European heartache before declaring he has no intention of overspending in a competitive post-World Cup transfer market. McCall watched his side, which contained five English-qualified forwards in the starting pack, battle in vain before losing 13-9 to the Clermont on Saturday. Saracens\u2019 millionaire chairman Nigel Wray spent much of last week repeating his belief the cap should be scrapped in order for Saracens to compete at Europe\u2019s top table, raising expectations they could be set to land a \u2018marquee\u2019 player from outside the league whose wages would sit outside next season\u2019s \u00a35.5m cap. Maro Itoje (second left) was one of five England-qualified forwards in the Saracens pack that faced Clermont. Mako Vunipola tries to fend off Clermont lock Jamie Cudmore during a ferocious contest. Saracens director of rugby Mark McCall saw his side come agonisingly close to reaching the final. But McCall said: \u2018We know where we\u2019d like t...\nSource: Sporting Life\nSummary: Mark McCall, the director of rugby at Saracens, has praised the club's young players after their loss to Clermont in the European Champions Cup. Despite the defeat, McCall is pleased with the progress of the team's young guns, including Maro Itoje and Mako Vunipola. He has also ruled out overspending in the transfer market, suggesting that the club will not be making any big-money signings in the near future.\nKeywords: Mark McCall, Saracens, Clermont, European Champions Cup, young players, Maro Itoje, Mako Vunipola, transfer market, cap.\nSaracens director of rugby Mark McCall has praised the club's young players after their loss to Clermont in the European Champions Cup. Despite the defeat, McCall is pleased with the progress of the team's young guns, including Maro Itoje and Mako Vunipola. He has also ruled out overspending in the transfer market, suggesting that the club will not be making any big-money signings in the near future.\nMcCall\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.49 ms /   516 runs   (    0.01 ms per token, 94006.19 tokens per second)\nllama_perf_context_print:        load time =    4071.72 ms\nllama_perf_context_print: prompt eval time =    1065.07 ms /   260 tokens (    4.10 ms per token,   244.12 tokens per second)\nllama_perf_context_print:        eval time =   55304.63 ms /   255 runs   (  216.88 ms per token,     4.61 tokens per second)\nllama_perf_context_print:       total time =   56420.74 ms /   515 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56572.15",
        "tokens": "512",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The decomposing bodies of the men accused of killing 148 innocent people at a Kenyan university were paraded in front of a large crowd at a primary school today. The authorities drove the naked, bloated corpses of the four alleged terrorists around the town in a pickup truck from the mortuary to Garissa Primary School. It comes as survivor\u00a0Cynthia Cheroitich, 19, who spent two days hiding in a wardrobe and drinking body lotion to survive, was rescued after al-Shabaab gunmen stormed Garissa University College on Thursday. Scroll down for video. Paraded: The naked bodies of the alleged attackers were driven around Garissa before the pickup truck parked at a primary school. In the primary school playground, hundreds gathered to look at the bodies of the men accused of being the attackers. Hundreds of people, some grieving for the loved ones they lost in the siege, gathered in the sweltering heat to catch a glimpse of the men who are believed to be responsible. Pictures of the crowds and the bodies pil -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The decomposing bodies of the men accused of killing 148 innocent people at a Kenyan university were paraded in front of a large crowd at a primary school today. The authorities drove the naked, bloated corpses of the four alleged terrorists around the town in a pickup truck from the mortuary to Garissa Primary School. It comes as survivor\u00a0Cynthia Cheroitich, 19, who spent two days hiding in a wardrobe and drinking body lotion to survive, was rescued after al-Shabaab gunmen stormed Garissa University College on Thursday. Scroll down for video. Paraded: The naked bodies of the alleged attackers were driven around Garissa before the pickup truck parked at a primary school. In the primary school playground, hundreds gathered to look at the bodies of the men accused of being the attackers. Hundreds of people, some grieving for the loved ones they lost in the siege, gathered in the sweltering heat to catch a glimpse of the men who are believed to be responsible. Pictures of the crowds and the bodies pilled up in the playground have been shared on social media, with some users expressing shock and outrage at the sight. The men were accused of killing 148 people, including students and staff, during the attack on Garissa University College on Thursday. The attackers were shot dead by security forces after they stormed the campus and opened fire on students and staff. The bodies were taken to the mortuary after they were killed, but they were brought back out for the public to see today. The sighting of the bodies has sparked a mixed reaction on social media, with some users expressing anger and disbelief at the sight of the men who are believed to have carried out the attack. Others have expressed gratitude to the security forces for their bravery in stopping the attack and bringing the attackers to justice. The attack on Garissa University College on Thursday was the deadliest attack in Kenya since the 1998 bombing of the US embassy in Nairobi, which killed 213 people. The attack has sparked a renewed debate about security in the country, with many calling for more to be done to protect civilians from terrorist attacks.\n\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.43 ms /   513 runs   (    0.01 ms per token, 94405.59 tokens per second)\nllama_perf_context_print:        load time =    3954.48 ms\nllama_perf_context_print: prompt eval time =    1020.05 ms /   257 tokens (    3.97 ms per token,   251.95 tokens per second)\nllama_perf_context_print:        eval time =   55502.29 ms /   255 runs   (  217.66 ms per token,     4.59 tokens per second)\nllama_perf_context_print:       total time =   56572.15 ms /   512 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56814.77",
        "tokens": "501",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea shot-stopper looks certain to leave Stamford Bridge at the end of the season after losing the No 1 spot to Thibaut Courtois. Cech will hold talks with manager Jose Mourinho at the end of the season where the keeper will confirm his decision to leave west London. Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea stopper looks certain to leave Stamford Bridge at the end of the season after losing his No 1 spot. Arsene Wenger must be swift with a move for his top goalkeeping target to beat the competition to him. The Gunners are long-term admirers of Cech and are considering a move for the keeper ahead of next season. However, Arsenal will not be the only club interested in luring Cech when the transfer window reopens. Liverpool, Paris Saint-Germain, Roma and Inter Milan are all showing an interest in the 32-year-old and Cech is under -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea shot-stopper looks certain to leave Stamford Bridge at the end of the season after losing the No 1 spot to Thibaut Courtois. Cech will hold talks with manager Jose Mourinho at the end of the season where the keeper will confirm his decision to leave west London. Arsenal must make a swift decision on Petr Cech this summer or risk losing out on the goalkeeper. The Chelsea stopper looks certain to leave Stamford Bridge at the end of the season after losing his No 1 spot. Arsene Wenger must be swift with a move for his top goalkeeping target to beat the competition to him. The Gunners are long-term admirers of Cech and are considering a move for the keeper ahead of next season. However, Arsenal will not be the only club interested in luring Cech when the transfer window reopens. Liverpool, Paris Saint-Germain, Roma and Inter Milan are all showing an interest in the 32-year-old and Cech is under no pressure to make a decision on his future. The keeper has been linked with a move to the Emirates Stadium for several years and is believed to be open to the idea of joining the Gunners. However, Cech will not make a decision until after he has spoken to Mourinho at the end of the season. The Portuguese boss has been a key factor in Cech's decision to leave Chelsea and the keeper will want to know what role he will play at Arsenal before making a decision. Arsenal must act quickly to secure Cech's signature before the transfer window closes. The Gunners have been linked with several other goalkeepers, including David Ospina and Asmir Begovic, but Cech remains their top target.\n\nEssential words:\n\n* Petr Cech\n* Chelsea\n* Stamford Bridge\n* Jose Mourinho\n* Arsenal\n* Arsene Wenger\n* Transfer window\n* Liverpool\n* Paris Saint-Germain\n* Roma\n* Inter Milan\n* David Ospina\n* Asmir Begovic [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.13 ms /   502 runs   (    0.01 ms per token, 97874.83 tokens per second)\nllama_perf_context_print:        load time =    3949.46 ms\nllama_perf_context_print: prompt eval time =    1020.98 ms /   258 tokens (    3.96 ms per token,   252.70 tokens per second)\nllama_perf_context_print:        eval time =   55746.29 ms /   243 runs   (  229.41 ms per token,     4.36 tokens per second)\nllama_perf_context_print:       total time =   56814.77 ms /   501 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 245 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "44642.61",
        "tokens": "403",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Frankie Dettori will be left to focus on the British Flat season this summer after boss Sheik Joaan Al Thani signed up Gregory Benoist to ride the horses which race under his Al Shaqab banner in France. The only exception will be Sheik Joaan\u2019s dual Arc winner Treve, who will continue to be partnered by veteran Thierry Jarnet. Benoist is the only jockey to have ridden Elie Lellouche-trained Ectot, who was unplaced for Al Shaqab in last year\u2019s Arc having gone into the race one of the leading fancies having won the Prix Niel. Frankie Dettori is to concentrate on the British flat season and not being picked to ride in France. The recruitment of Benoist could be seen as a snub for Dettori, who was replaced by Jarnet on Treve in the build-up to her Arc success. But Herbert insisted it was no longer logistically feasible to have one jockey covering both countries because Sheik Joaan\u2019s operation has expanded so much. Herbert said: \u2018Last season it was feasible to keep travelling back and forth across the Ch -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Frankie Dettori will be left to focus on the British Flat season this summer after boss Sheik Joaan Al Thani signed up Gregory Benoist to ride the horses which race under his Al Shaqab banner in France. The only exception will be Sheik Joaan\u2019s dual Arc winner Treve, who will continue to be partnered by veteran Thierry Jarnet. Benoist is the only jockey to have ridden Elie Lellouche-trained Ectot, who was unplaced for Al Shaqab in last year\u2019s Arc having gone into the race one of the leading fancies having won the Prix Niel. Frankie Dettori is to concentrate on the British flat season and not being picked to ride in France. The recruitment of Benoist could be seen as a snub for Dettori, who was replaced by Jarnet on Treve in the build-up to her Arc success. But Herbert insisted it was no longer logistically feasible to have one jockey covering both countries because Sheik Joaan\u2019s operation has expanded so much. Herbert said: \u2018Last season it was feasible to keep travelling back and forth across the Chunnel, but now we have so many horses in France and the operation has grown so much that it\u2019s not possible any more. \u2018We have to make sure we have the right jockey for each horse and that they are able to focus on them properly. \u2018Frankie has been fantastic for us and we are grateful for all he has done, but we have to move on and look to the future. \u2018We are very excited about Gregory joining us and we think he will be a great asset to the team. He has a lot of experience and has ridden some top class horses, including Ectot, who ran in the Arc last year. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       2.85 ms /   404 runs   (    0.01 ms per token, 141704.67 tokens per second)\nllama_perf_context_print:        load time =    4723.73 ms\nllama_perf_context_print: prompt eval time =    1562.15 ms /   269 tokens (    5.81 ms per token,   172.20 tokens per second)\nllama_perf_context_print:        eval time =   43049.63 ms /   134 runs   (  321.27 ms per token,     3.11 tokens per second)\nllama_perf_context_print:       total time =   44642.61 ms /   403 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 136 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "88109.48",
        "tokens": "563",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: (CNN)Lois Lilienstein, co-star of \"Sharon, Lois & Bram's Elephant Show\" \u2014 the Canadian preschool show that ran on Nickelodeon during the early 1990s \u2014 has died, aged 78. Her son, David Lilienstein, told CBC News that his mother died in Toronto on Wednesday night from a rare form of cancer first diagnosed last October. \"She knew it was happening, she was at peace with it, and she died very peacefully and not in pain,\" he told the Canadian broadcaster. Liz Smith at 92. Lilienstein was born in Chicago in 1936 and moved to Toronto in 1996 with her son and her husband, Ernest. She joined Sharon Hampson and Bram Morrison to launch their singing trio in 1978. The TV show \"Sharon, Lois & Bram's Elephant Show\" ran on CBC in Canada from 1984 to 1989, before continuing in reruns on Nickelodeon stateside until 1995. The 30 most popular film producers in Hollywood. Sally Catto, general manager, programming at CBC Television, paid tribute to Lilienstein for her contribution to Canadian kids TV. \"Sharon, Lois and -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: (CNN)Lois Lilienstein, co-star of \"Sharon, Lois & Bram's Elephant Show\" \u2014 the Canadian preschool show that ran on Nickelodeon during the early 1990s \u2014 has died, aged 78. Her son, David Lilienstein, told CBC News that his mother died in Toronto on Wednesday night from a rare form of cancer first diagnosed last October. \"She knew it was happening, she was at peace with it, and she died very peacefully and not in pain,\" he told the Canadian broadcaster. Liz Smith at 92. Lilienstein was born in Chicago in 1936 and moved to Toronto in 1996 with her son and her husband, Ernest. She joined Sharon Hampson and Bram Morrison to launch their singing trio in 1978. The TV show \"Sharon, Lois & Bram's Elephant Show\" ran on CBC in Canada from 1984 to 1989, before continuing in reruns on Nickelodeon stateside until 1995. The 30 most popular film producers in Hollywood. Sally Catto, general manager, programming at CBC Television, paid tribute to Lilienstein for her contribution to Canadian kids TV. \"Sharon, Lois and Bram were a beloved part of many children's lives in Canada and around the world,\" she said. \"Lois's warmth, wit and infectious energy brought joy to countless families and her legacy will continue to inspire future generations of children's entertainers.\"\n\nAnalysis: Lois Lilienstein, a co-star of the popular Canadian preschool show \"Sharon, Lois & Bram's Elephant Show,\" has passed away at the age of 78. She was born in Chicago and moved to Toronto in 1996 with her son and husband. Lilienstein joined Sharon Hampson and Bram Morrison to launch their singing trio in 1978, and the TV show \"Sharon, Lois & Bram's Elephant Show\" ran on CBC in Canada from 1984 to 1989 before continuing in reruns on Nickelodeon stateside until 1995. Her son, David Lilienstein, confirmed her death on Wednesday night from a rare form of cancer first diagnosed last October. Liz Smith, a well-known Canadian film\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.64 ms /   564 runs   (    0.01 ms per token, 100000.00 tokens per second)\nllama_perf_context_print:        load time =    4919.06 ms\nllama_perf_context_print: prompt eval time =    1516.57 ms /   308 tokens (    4.92 ms per token,   203.09 tokens per second)\nllama_perf_context_print:        eval time =   86539.48 ms /   255 runs   (  339.37 ms per token,     2.95 tokens per second)\nllama_perf_context_print:       total time =   88109.48 ms /   563 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "83205.31",
        "tokens": "514",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: This incredible video shows the moment a world freediving champion jumped into the world\u2019s second deepest underwater sink hole. The dramatic footage shows Guillame N\u00e9ry standing at the edge before jumping in - all without scuba gear and simply holding his breath. And in the video he seemingly reaches the bottom of the 660ft (200 metes) hole before swimming back to the surface. French free diver jumped into Dean's Blue Hole in the Bahamas. Guillame N\u00e9ry is seen at the edge before taking the plunge (shown). The hole is 660ft (200 metres) deep, although he doesn't go to the bottom. Free divers are able to hold their breath for more than 20 minutes. The video was filmed in 2010 by the Frenchman's girlfriend and fellow freediver Julie Gautier. It takes place at Dean\u2019s Blue Hole in the Bahamas, the deepest so-called \u2018blue hole\u2019 in the world with an underwater entrance. In order to stay alive, the brain is constantly telling the body to breathe. If you don\u2019t obey its primitive urges, carbon dioxide soon s -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: This incredible video shows the moment a world freediving champion jumped into the world\u2019s second deepest underwater sink hole. The dramatic footage shows Guillame N\u00e9ry standing at the edge before jumping in - all without scuba gear and simply holding his breath. And in the video he seemingly reaches the bottom of the 660ft (200 metes) hole before swimming back to the surface. French free diver jumped into Dean's Blue Hole in the Bahamas. Guillame N\u00e9ry is seen at the edge before taking the plunge (shown). The hole is 660ft (200 metres) deep, although he doesn't go to the bottom. Free divers are able to hold their breath for more than 20 minutes. The video was filmed in 2010 by the Frenchman's girlfriend and fellow freediver Julie Gautier. It takes place at Dean\u2019s Blue Hole in the Bahamas, the deepest so-called \u2018blue hole\u2019 in the world with an underwater entrance. In order to stay alive, the brain is constantly telling the body to breathe. If you don\u2019t obey its primitive urges, carbon dioxide soon saturates the blood and oxygen levels plummet. But free divers have learned to control their breathing to the point where they can hold their breath for more than 20 minutes.\n\nThe video shows the moment French freediving champion Guillame N\u00e9ry jumped into the world's second deepest underwater sinkhole, Dean's Blue Hole in the Bahamas. The dramatic footage shows N\u00e9ry standing at the edge of the hole before taking the plunge without scuba gear and simply holding his breath. Although he doesn't reach the bottom of the 660ft (200 metes) hole, the video shows him swimming back to the surface. The video was filmed in 2010 by N\u00e9ry's girlfriend and fellow freediver Julie Gautier. Dean's Blue Hole is the deepest so-called 'blue hole' in the world with an underwater entrance, and N\u00e9ry's dive is a testament to the incredible abilities of free divers who can hold their breath for more than 20 minutes. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.28 ms /   515 runs   (    0.01 ms per token, 97537.88 tokens per second)\nllama_perf_context_print:        load time =    4919.68 ms\nllama_perf_context_print: prompt eval time =    1519.74 ms /   277 tokens (    5.49 ms per token,   182.27 tokens per second)\nllama_perf_context_print:        eval time =   81634.80 ms /   237 runs   (  344.45 ms per token,     2.90 tokens per second)\nllama_perf_context_print:       total time =   83205.31 ms /   514 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 239 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56222.22",
        "tokens": "530",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Kabul, Afghanistan (CNN)A suicide bomber on a motorbike blew himself up in front of the Kabul Bank in Jalalabad early Saturday, a local government spokesman said. The ISIS terrorist group claimed responsibility for the attack. The explosion killed at least 33 people and injured more than 100 others, public health spokesman Najibullah Kamawal said. The claim appears to be the first in Afghanistan by ISIS, CNN's Nick Paton Walsh said. It was made by an offshoot called ISIS Wilayat Khorasan. In a statement, the group said the bomber was named Abu Mohammad and he belonged to their ranks. He was targeting government workers collecting their pay at the bank, the terrorists said. He detonated his charge at the peak of rush hour on the first day of the week, when the bank would be expected to be crowded. Taliban spokesman Zabiullah Mujahid, in a text message to journalists, distanced his group from the attack. \"The explosion in Jalalabad doesn't have anything to do with us and we condemn it,\" he said. ISIS -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Kabul, Afghanistan (CNN)A suicide bomber on a motorbike blew himself up in front of the Kabul Bank in Jalalabad early Saturday, a local government spokesman said. The ISIS terrorist group claimed responsibility for the attack. The explosion killed at least 33 people and injured more than 100 others, public health spokesman Najibullah Kamawal said. The claim appears to be the first in Afghanistan by ISIS, CNN's Nick Paton Walsh said. It was made by an offshoot called ISIS Wilayat Khorasan. In a statement, the group said the bomber was named Abu Mohammad and he belonged to their ranks. He was targeting government workers collecting their pay at the bank, the terrorists said. He detonated his charge at the peak of rush hour on the first day of the week, when the bank would be expected to be crowded. Taliban spokesman Zabiullah Mujahid, in a text message to journalists, distanced his group from the attack. \"The explosion in Jalalabad doesn't have anything to do with us and we condemn it,\" he said. ISIS has been active in Afghanistan and neighboring Pakistan, and has been blamed for a series of attacks in both countries. The group has been trying to expand its presence in Afghanistan, and has been fighting against the Taliban and other anti-ISIS forces in the region.\n\nAnalysis:\n\n* The suicide bombing in Jalalabad, Afghanistan killed at least 33 people and injured over 100 others.\n* The ISIS terrorist group claimed responsibility for the attack, with an offshoot called ISIS Wilayat Khorasan making the claim.\n* The group said the bomber, named Abu Mohammad, targeted government workers collecting their pay at the bank.\n* The attack occurred at the peak of rush hour on the first day of the week, when the bank would be expected to be crowded.\n* The Taliban distanced themselves from the attack, with a spokesman saying the group does not support such acts of violence.\n* ISIS has been active in Afghanistan and neighboring Pakistan, and has been blamed for a series of attacks in both countries.\n* The group has been trying to expand its presence\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.46 ms /   531 runs   (    0.01 ms per token, 97341.89 tokens per second)\nllama_perf_context_print:        load time =    4524.87 ms\nllama_perf_context_print: prompt eval time =     992.33 ms /   275 tokens (    3.61 ms per token,   277.12 tokens per second)\nllama_perf_context_print:        eval time =   55174.05 ms /   255 runs   (  216.37 ms per token,     4.62 tokens per second)\nllama_perf_context_print:       total time =   56222.22 ms /   530 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "87453.47",
        "tokens": "524",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: She has already taken the modelling world by storm, being snapped up to front campaigns for The Upside, Myer, Berlei and ASOS, to name a few. And now like Megan Gale, Jennifer Hawkins and Elle Macpherson before her, plus-size queen Laura Wells has turned her hand to designing. The brunette bombshell has launched a new swimwear line with a twist - the eco-friendly collection is made entirely of 100 per cent recycled material. Scroll down for video. Curves to coral: Model and environmental scientist Laura Wells uses figure to flaunt ocean inspired swimwear. Sizzling: The Australian beauty smoulders in the campaign. The environmental scientist turned model designed the line in collaboration with US brand Swimsuits For All's trend range, SwimSexy. The collection is inspired by the ocean, with coral reef scenes featuring throughout. The line includes one and two-piece suits, with zippers, criss-cross straps, plunging V-necklines and high-waisted bottoms. The swimsuits cater for every shape, with sizes r -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: She has already taken the modelling world by storm, being snapped up to front campaigns for The Upside, Myer, Berlei and ASOS, to name a few. And now like Megan Gale, Jennifer Hawkins and Elle Macpherson before her, plus-size queen Laura Wells has turned her hand to designing. The brunette bombshell has launched a new swimwear line with a twist - the eco-friendly collection is made entirely of 100 per cent recycled material. Scroll down for video. Curves to coral: Model and environmental scientist Laura Wells uses figure to flaunt ocean inspired swimwear. Sizzling: The Australian beauty smoulders in the campaign. The environmental scientist turned model designed the line in collaboration with US brand Swimsuits For All's trend range, SwimSexy. The collection is inspired by the ocean, with coral reef scenes featuring throughout. The line includes one and two-piece suits, with zippers, criss-cross straps, plunging V-necklines and high-waisted bottoms. The swimsuits cater for every shape, with sizes ranging from 10 to 28. Laura said she wanted to create a line that was both stylish and sustainable. She said: 'I wanted to create a swimwear line that was not only beautiful and comfortable, but also eco-friendly. 'I believe that fashion and sustainability can go hand-in-hand, and I hope that this collection inspires other designers to think about the impact their clothing has on the environment. 'Laura, who has been modelling for 10 years, said she was passionate about the environment and wanted to use her platform to make a difference. She added: 'As a model, I have the opportunity to travel the world and see first-hand the devastating effects of pollution and climate change. 'I wanted to use my passion for fashion to create something that would make a positive impact on the environment. 'Laura's swimwear line is available now on Swimsuits For All's website, priced between $100 and $150. The collection is made entirely of 100 per cent recycled material, including recycled polyester and nylon. The brand said\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   525 runs   (    0.01 ms per token, 96294.94 tokens per second)\nllama_perf_context_print:        load time =    4903.55 ms\nllama_perf_context_print: prompt eval time =    1617.98 ms /   269 tokens (    6.01 ms per token,   166.26 tokens per second)\nllama_perf_context_print:        eval time =   85779.88 ms /   255 runs   (  336.39 ms per token,     2.97 tokens per second)\nllama_perf_context_print:       total time =   87453.47 ms /   524 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "54833.52",
        "tokens": "397",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Every second of the day you are subjected to radiation from compounds in soil, buildings and even your own body. But for people who fly frequently, cosmic radiation can increase this exposure significantly. Now there is an app that records your personal radiation to reveal how much you are exposed to from from medical examinations, changes in your location, and flights. The TrackYourDose app (pictured) was developed by Germany-based firm esooka and uses a mathematical model developed by scientists and meteorologists. The \u00a31.49 ($1.99) iOS app keeps a record of a user's personal radiation including exposure from changes in a person's location and flights. The TrackYourDose app was developed by Germany-based esooka. It uses a mathematical model developed by scientists and meteorologists\u00a0from the Physikalisch-Technische Bundesanstalt (PTB) in Germany and the University Oulu, Finland. Since 1997, PTB has studied the cosmic radiation at typical flight altitudes and used this to calculate radiation level -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Every second of the day you are subjected to radiation from compounds in soil, buildings and even your own body. But for people who fly frequently, cosmic radiation can increase this exposure significantly. Now there is an app that records your personal radiation to reveal how much you are exposed to from from medical examinations, changes in your location, and flights. The TrackYourDose app (pictured) was developed by Germany-based firm esooka and uses a mathematical model developed by scientists and meteorologists. The \u00a31.49 ($1.99) iOS app keeps a record of a user's personal radiation including exposure from changes in a person's location and flights. The TrackYourDose app was developed by Germany-based esooka. It uses a mathematical model developed by scientists and meteorologists\u00a0from the Physikalisch-Technische Bundesanstalt (PTB) in Germany and the University Oulu, Finland. Since 1997, PTB has studied the cosmic radiation at typical flight altitudes and used this to calculate radiation level exposure. The app can also be used to track radiation levels in different locations, such as hospitals or nuclear power plants.\n\nPersonal thoughts: This app is a great tool for people who are concerned about their radiation exposure, especially those who fly frequently. It's interesting to see how much radiation I'm exposed to on a daily basis, and the app provides a clear and easy-to-understand breakdown of my exposure levels. I can also use it to track my exposure levels in different locations, such as hospitals or nuclear power plants, which can help me make informed decisions about where I spend my time. Overall, I think this app is a valuable resource for anyone who wants to stay informed about their radiation exposure. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.42 ms /   398 runs   (    0.01 ms per token, 116340.25 tokens per second)\nllama_perf_context_print:        load time =    4762.24 ms\nllama_perf_context_print: prompt eval time =    1477.62 ms /   237 tokens (    6.23 ms per token,   160.39 tokens per second)\nllama_perf_context_print:        eval time =   53322.92 ms /   160 runs   (  333.27 ms per token,     3.00 tokens per second)\nllama_perf_context_print:       total time =   54833.52 ms /   397 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 162 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "85411.66",
        "tokens": "477",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Blue Bell Ice Cream announced Friday that it has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. 'We are taking this step out of an abundance of caution to ensure that we are doing everything possible to provide our consumers with safe products and to preserve the trust we have built with them and their families for more than a century,' the company said in a statement. Last month, the company and health officials said a 3-ounce cup of ice cream contaminated with listeriosis was traced to a plant in Broken Arrow, Oklahoma. Scroll down for video. Blue Bell Ice Cream has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. Ten products recalled earlier in March were from a production line at a plant the company's headquarters in Brenham, Texas. The now-recalled ice cream product - cups of ch -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Blue Bell Ice Cream announced Friday that it has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. 'We are taking this step out of an abundance of caution to ensure that we are doing everything possible to provide our consumers with safe products and to preserve the trust we have built with them and their families for more than a century,' the company said in a statement. Last month, the company and health officials said a 3-ounce cup of ice cream contaminated with listeriosis was traced to a plant in Broken Arrow, Oklahoma. Scroll down for video. Blue Bell Ice Cream has suspended operations at an Oklahoma production facility that officials had previously connected to a foodborne illness linked to the deaths of three people. Ten products recalled earlier in March were from a production line at a plant the company's headquarters in Brenham, Texas. The now-recalled ice cream product - cups of chocolate chip cookie dough, chocolate, and vanilla - were distributed in Alabama, Arizona, Arkansas, Florida, Georgia, Louisiana, Mississippi, Oklahoma, South Carolina, Tennessee, and Texas. The products were sold in retail stores and supermarkets. The company said it is working with regulatory agencies to determine the cause of the contamination and to prevent it from happening again. The Centers for Disease Control and Prevention (CDC) and the Food and Drug Administration (FDA) are investigating the outbreak of listeriosis, a serious infection that can cause high fever, headache, stiffness, and nausea. The CDC has reported 10 cases of listeriosis in five states, including Texas, Oklahoma, Kansas, Missouri, and Arkansas, since January 2016. The agency said the cases are linked to eating Blue Bell ice cream. The company said it is taking the step out of an abundance of caution to ensure that it is doing everything possible to provide its consumers with safe products and to preserve the trust it has built with them and their families for more than a century. The company said it\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.38 ms /   478 runs   (    0.01 ms per token, 88864.10 tokens per second)\nllama_perf_context_print:        load time =    4749.42 ms\nllama_perf_context_print: prompt eval time =    1463.79 ms /   222 tokens (    6.59 ms per token,   151.66 tokens per second)\nllama_perf_context_print:        eval time =   83896.53 ms /   255 runs   (  329.01 ms per token,     3.04 tokens per second)\nllama_perf_context_print:       total time =   85411.66 ms /   477 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "86563.08",
        "tokens": "519",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Mad Men star Jon Hamm's secret college bullying shame has been exposed by court documents unearthed from the actor's time at the University of Texas at Austin. According to shocking arrest warrants and summons seen by Star magazine, the 44-year-old Golden Globe winner allegedly set fire to one pledge after viciously beating him and using a claw hammer to drag him by his genitals during a hazing ritual gone wrong. The documents claim that Hamm, 44, was eventually charged with assault in connection with the sadistic November 1990 initiation, but he managed to get the charges dismissed before he began his ascent to superstardom. Scroll down for video. College student: Jon Hamm is pictured here in the University of Texas at Austin yearbook for 1990 (left) - the same year he was allegedly involved in a hazing scandal at the Sigma Nu fraternity. The actor is pictured (right) this week. According to the explosive court documents, Hamm was in his sophomore year at UT-Austin when a prospective 21-year-old p -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Mad Men star Jon Hamm's secret college bullying shame has been exposed by court documents unearthed from the actor's time at the University of Texas at Austin. According to shocking arrest warrants and summons seen by Star magazine, the 44-year-old Golden Globe winner allegedly set fire to one pledge after viciously beating him and using a claw hammer to drag him by his genitals during a hazing ritual gone wrong. The documents claim that Hamm, 44, was eventually charged with assault in connection with the sadistic November 1990 initiation, but he managed to get the charges dismissed before he began his ascent to superstardom. Scroll down for video. College student: Jon Hamm is pictured here in the University of Texas at Austin yearbook for 1990 (left) - the same year he was allegedly involved in a hazing scandal at the Sigma Nu fraternity. The actor is pictured (right) this week. According to the explosive court documents, Hamm was in his sophomore year at UT-Austin when a prospective 21-year-old pledge was subjected to a brutal hazing ritual. The documents claim that Hamm and several other Sigma Nu members took the pledge to a remote location, where they beat him with a claw hammer and set him on fire. The victim suffered second- and third-degree burns on his body and was hospitalized for several weeks. The documents allege that Hamm and his fellow fraternity members were charged with aggravated assault and were ordered to appear in court. However, the charges were dismissed after the alleged victim failed to show up to testify. The documents were unearthed by Star magazine, which claims that Hamm's representatives have refused to comment on the allegations. The actor, who is best known for his role as Don Draper on Mad Men, has won numerous awards for his portrayal of the troubled advertising executive. He has been nominated for 16 Emmy Awards and has won three Golden Globes for his work on the hit AMC series. The news of Hamm's alleged involvement in the hazing scandal comes as the actor is enjoying a resurgence in popularity following the release of the hit film Baby Driver. The \n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.61 ms /   520 runs   (    0.01 ms per token, 92774.31 tokens per second)\nllama_perf_context_print:        load time =    4779.10 ms\nllama_perf_context_print: prompt eval time =    1492.03 ms /   264 tokens (    5.65 ms per token,   176.94 tokens per second)\nllama_perf_context_print:        eval time =   85019.70 ms /   255 runs   (  333.41 ms per token,     3.00 tokens per second)\nllama_perf_context_print:       total time =   86563.08 ms /   519 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57145.87",
        "tokens": "484",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Philip Hammond pledged to protect the Falklands from \u2018Argentinian bullying\u2019 last night as the country began legal action against British companies exploring for oil off the islands. It follows a provocative speech by Argentinian president Cristina Fernandez de Kirchner, prompting the Foreign Office to summon her ambassador for a dressing down. Buenos Aires responded by accusing three British oil firms of working without permission. The Argentine government said it was taking legal action against three British and two US companies for \u2018exploration activities for fossil fuels on the Argentine continental shelf without obtaining the corresponding authorisation\u2019. The legal complaint listed Rockhopper Exploration, Premier Oil, Falkland Oil and Gas Limited and US firms Noble Energy and Edison International \u2013 accusing them of sending a semi-submersible rig to explore a basin 120 miles north of the islands \u2018subjected to the illegitimate British occupation\u2019. Argentina's President Cristina Fernandez de Kirch -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Philip Hammond pledged to protect the Falklands from \u2018Argentinian bullying\u2019 last night as the country began legal action against British companies exploring for oil off the islands. It follows a provocative speech by Argentinian president Cristina Fernandez de Kirchner, prompting the Foreign Office to summon her ambassador for a dressing down. Buenos Aires responded by accusing three British oil firms of working without permission. The Argentine government said it was taking legal action against three British and two US companies for \u2018exploration activities for fossil fuels on the Argentine continental shelf without obtaining the corresponding authorisation\u2019. The legal complaint listed Rockhopper Exploration, Premier Oil, Falkland Oil and Gas Limited and US firms Noble Energy and Edison International \u2013 accusing them of sending a semi-submersible rig to explore a basin 120 miles north of the islands \u2018subjected to the illegitimate British occupation\u2019. Argentina's President Cristina Fernandez de Kirchner has accused British companies of 'stealing' the Falklands' oil reserves.\n\nAnalysis: The Falklands/Malvinas dispute between Argentina and the UK has been ongoing for decades, with both countries claiming sovereignty over the islands. The latest development in this longstanding conflict is Argentina's legal action against British companies exploring for oil off the islands. The move comes after a provocative speech by Argentine President Cristina Fernandez de Kirchner, who accused British companies of \"stealing\" the Falklands' oil reserves. The UK has responded by summoning the Argentine ambassador and pledging to protect the Falklands from \"Argentine bullying.\"\n\nThe legal action taken by Argentina against British companies is significant because it highlights the ongoing tensions between the two countries over the Falklands/Malvinas issue. Argentina has long claimed sovereignty over the islands, which it views as a vital strategic interest. The UK, on the other hand, has maintained that the Falklands are a British overseas territory and has refused to negotiate sovereignty with Argentina.\n\nThe use\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.44 ms /   485 runs   (    0.01 ms per token, 89236.43 tokens per second)\nllama_perf_context_print:        load time =    4545.15 ms\nllama_perf_context_print: prompt eval time =    1005.12 ms /   229 tokens (    4.39 ms per token,   227.83 tokens per second)\nllama_perf_context_print:        eval time =   56086.81 ms /   255 runs   (  219.95 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   57145.87 ms /   484 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57145.08",
        "tokens": "504",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Passengers experienced some anxious moments when their plane landed at a New York City airport and was met by firefighters wearing full protective suits. Emergency workers surrounded the Delta Air Lines plane at LaGuardia Airport after an odor of smoke was reported by the crew moments after Flight 2522 touched down. Passengers were still on board as firefighters walked through the cabin with a thermal imaging camera to locate a potential heat source. After the plane landed a firefighter used a thermal imaging camera to locate a potential heat source. Passenger Pamela Sullins snapped photos showing a firefighter in the aisle of the McDonnell Douglas MD-888 twin-engine plane, and others gathered on the tarmac after the commercial jet flew in from Tampa, Florida, yesterday. She told MailOnline Travel: \u2018I noticed an odor that seemed like electrical wires burning after we landed. I did not see any smoke.' Pamela, who was travelling to New York for business meetings, said the Delta crew provided updates  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Passengers experienced some anxious moments when their plane landed at a New York City airport and was met by firefighters wearing full protective suits. Emergency workers surrounded the Delta Air Lines plane at LaGuardia Airport after an odor of smoke was reported by the crew moments after Flight 2522 touched down. Passengers were still on board as firefighters walked through the cabin with a thermal imaging camera to locate a potential heat source. After the plane landed a firefighter used a thermal imaging camera to locate a potential heat source. Passenger Pamela Sullins snapped photos showing a firefighter in the aisle of the McDonnell Douglas MD-888 twin-engine plane, and others gathered on the tarmac after the commercial jet flew in from Tampa, Florida, yesterday. She told MailOnline Travel: \u2018I noticed an odor that seemed like electrical wires burning after we landed. I did not see any smoke.' Pamela, who was travelling to New York for business meetings, said the Delta crew provided updates  via the intercom throughout the incident. She added: \u2018They were very calm and professional. They told us they were going to check the plane and that we would be on the ground soon.' Passengers were evacuated from the plane via stairs after the incident, and were taken to a nearby terminal for questioning by airport police. No injuries were reported. The cause of the odor is still under investigation.\n\nEssential words:\n\n* passengers\n* plane\n* landed\n* firefighters\n* protective suits\n* thermal imaging camera\n* heat source\n* odor\n* smoke\n* tarmac\n* intercom\n* calm\n* professional\n* evacuated\n* stairs\n* terminal\n* police\n* investigation\n\nQuestion: What happened to the passengers on Flight 2522 after they landed at LaGuardia Airport?\nAnswer: After Flight 2522 landed at LaGuardia Airport, firefighters wearing full protective suits surrounded the plane and used a thermal imaging camera to locate a potential heat source. Passengers were evacuated from the plane via stairs and\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.61 ms /   505 runs   (    0.01 ms per token, 89985.74 tokens per second)\nllama_perf_context_print:        load time =    3984.66 ms\nllama_perf_context_print: prompt eval time =    1032.52 ms /   249 tokens (    4.15 ms per token,   241.16 tokens per second)\nllama_perf_context_print:        eval time =   56061.78 ms /   255 runs   (  219.85 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   57145.08 ms /   504 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "31912.40",
        "tokens": "413",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: It appears that David Luiz has taken the phrase 'twiddling my hair' literally judging by the injured Paris Saint-Germain star's latest Instagram post. The PSG defender is out for at least four weeks after scans revealed that the 27-year-old suffered a torn hamstring in Sunday's 3-2 win at Ligue 1 title rivals Marseille. And it seems that boredom has already kicked into the usually jovial Brazilian as he sported a new hairdo on Wednesday. David Luiz posted an Instagram picture sporting a new hairdo in the style of a man bun on Wednesday. Luiz (centre) has time on his hands at present after tearing a hamstring at Marseille on Sunday night. The Paris Saint-Germain defender immediately signalled to the bench that he would need to be replaced. Accompanied with the caption: 'Parisien!' Luiz has his usually flowing brown locks styled into a man bun. The centre back's new look seems to be a source of amusement for his PSG team-mate Lucas Moura who later shared a picture with Luiz among his Instagram follow -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: It appears that David Luiz has taken the phrase 'twiddling my hair' literally judging by the injured Paris Saint-Germain star's latest Instagram post. The PSG defender is out for at least four weeks after scans revealed that the 27-year-old suffered a torn hamstring in Sunday's 3-2 win at Ligue 1 title rivals Marseille. And it seems that boredom has already kicked into the usually jovial Brazilian as he sported a new hairdo on Wednesday. David Luiz posted an Instagram picture sporting a new hairdo in the style of a man bun on Wednesday. Luiz (centre) has time on his hands at present after tearing a hamstring at Marseille on Sunday night. The Paris Saint-Germain defender immediately signalled to the bench that he would need to be replaced. Accompanied with the caption: 'Parisien!' Luiz has his usually flowing brown locks styled into a man bun. The centre back's new look seems to be a source of amusement for his PSG team-mate Lucas Moura who later shared a picture with Luiz among his Instagram followers with the comment: 'David Luiz's new hairdo! \ud83d\ude02\ud83d\udc4d' Luiz has been ruled out for at least four weeks with the injury he sustained in Sunday's 3-2 win at Marseille. The 27-year-old was replaced by Thomas Tuchel after 25 minutes of the match and scans later revealed the full extent of the injury. Luiz has been a key player for PSG this season, making 27 appearances in all competitions and scoring three goals. The injury is a setback for the French champions as they look to win their fifth consecutive Ligue 1 title. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.00 ms /   414 runs   (    0.01 ms per token, 137862.14 tokens per second)\nllama_perf_context_print:        load time =    3975.27 ms\nllama_perf_context_print: prompt eval time =    1043.67 ms /   272 tokens (    3.84 ms per token,   260.62 tokens per second)\nllama_perf_context_print:        eval time =   30839.99 ms /   141 runs   (  218.72 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   31912.40 ms /   413 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 143 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57134.63",
        "tokens": "498",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: US President Barack Obama said on Saturday that diplomacy was the best option to deal with Iran's contested nuclear program, two days after the conclusion of a framework agreement with Tehran. Iran and six world powers determined the outlines of a landmark agreement which would curb Iran's nuclear program and potentially lift economic sanctions. As Obama gears up to sell US skeptics on the deal, he said he is convinced talks are the best way forward. Scroll down for video. US President Barack Obama says diplomacy is the best way to deal with Iran. U.S. Secretary of State John Kerry and staff watch a tablet in Lausanne as U.S. President Barack Obama makes a state address on the status of the Iran nuclear program. 'As President and Commander in Chief, I firmly believe that the diplomatic option - a comprehensive, long-term deal like this - is by far the best option,' Obama said in his weekly address. Explaining that he expects a 'robust debate' on the deal in the United States, Obama said he will kee -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: US President Barack Obama said on Saturday that diplomacy was the best option to deal with Iran's contested nuclear program, two days after the conclusion of a framework agreement with Tehran. Iran and six world powers determined the outlines of a landmark agreement which would curb Iran's nuclear program and potentially lift economic sanctions. As Obama gears up to sell US skeptics on the deal, he said he is convinced talks are the best way forward. Scroll down for video. US President Barack Obama says diplomacy is the best way to deal with Iran. U.S. Secretary of State John Kerry and staff watch a tablet in Lausanne as U.S. President Barack Obama makes a state address on the status of the Iran nuclear program. 'As President and Commander in Chief, I firmly believe that the diplomatic option - a comprehensive, long-term deal like this - is by far the best option,' Obama said in his weekly address. Explaining that he expects a 'robust debate' on the deal in the United States, Obama said he will keeo working with Congress to build support for the agreement. 'But I'm convinced that diplomacy is the best way forward, and that this deal represents the best chance we have to address the threat of Iran's nuclear program while also addressing the many other challenges we face in the region,' he said. Obama also emphasized that the deal is not a 'done deal' and that there will be 'rigorous' inspections and verification measures to ensure Iran is living up to its commitments. 'This is a framework agreement, not a final deal. There will be tough negotiations ahead, and we will continue to work with our international partners to ensure that Iran fulfills its commitments,' he said. 'But I'm confident that if we stick with diplomacy, we can get this done and achieve a safer, more secure future for all of us.' Obama's comments come after a framework agreement was reached between Iran and the P5+1 countries - the United States, the United Kingdom, France, Russia, China, and Germany - on Thursday. The deal outlines a comprehensive plan to limit Iran's nuclear program in exchange for relief from economic sanctions. The agreement is expected\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   499 runs   (    0.01 ms per token, 91626.88 tokens per second)\nllama_perf_context_print:        load time =    4367.05 ms\nllama_perf_context_print: prompt eval time =    1036.00 ms /   243 tokens (    4.26 ms per token,   234.56 tokens per second)\nllama_perf_context_print:        eval time =   56047.23 ms /   255 runs   (  219.79 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   57134.63 ms /   498 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57228.87",
        "tokens": "504",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A 12-year-old girl battling Leukemia for two years has been kicked out of school for her lack of attendance. 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully. Last week St. Joseph's Middle School, a private Catholic School, sent a letter to Rose McGrath and her dismissing her from the school for low attendance and poor academic performance. Scroll down for video. Heartbroken: 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully of her school kicking her out for poor attendance because of her Leukemia. Dismissed: John Fleckenstein, with Battle Creek Area Catholic Schools, claims that the school made many accommodations for Rose and that none of them seemed to help her enough. Rose's mother Barbara McGrath was just as heartbroken to hear the news and said even though her daughter is no longer getting cancer treatment, that her recovery will take some time. 'Even though she's n -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A 12-year-old girl battling Leukemia for two years has been kicked out of school for her lack of attendance. 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully. Last week St. Joseph's Middle School, a private Catholic School, sent a letter to Rose McGrath and her dismissing her from the school for low attendance and poor academic performance. Scroll down for video. Heartbroken: 'I didn't do anything wrong, but they still got rid of me,' Rose McGrath of battle Creek, Michigan said tearfully of her school kicking her out for poor attendance because of her Leukemia. Dismissed: John Fleckenstein, with Battle Creek Area Catholic Schools, claims that the school made many accommodations for Rose and that none of them seemed to help her enough. Rose's mother Barbara McGrath was just as heartbroken to hear the news and said even though her daughter is no longer getting cancer treatment, that her recovery will take some time. 'Even though she's nolonger getting chemo, she still has to go through radiation and she still has to go through physical therapy,' McGrath said. 'It's a long road to recovery and she's still going to be in treatment for a while.'\nRose McGrath, a 12-year-old girl from Battle Creek, Michigan, has been battling Leukemia for two years. Despite her ongoing treatment, she was recently kicked out of school due to poor attendance and academic performance.\nAccording to Rose's mother, Barbara McGrath, the school made several accommodations for her daughter, but they were not enough to help her attend classes regularly.\nRose was diagnosed with Leukemia in 2019 and has been undergoing treatment ever since. Despite her condition, she has been determined to continue her education. However, her attendance has been inconsistent due to her ongoing treatment and hospital visits.\nThe school's decision to dismiss Rose has come as a shock to her family and friends. \"I didn't do anything wrong, but they still got rid of me,\" Rose said tearfully.\nJohn Fleckenstein, with Battle Creek Area\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.49 ms /   505 runs   (    0.01 ms per token, 91985.43 tokens per second)\nllama_perf_context_print:        load time =    4192.06 ms\nllama_perf_context_print: prompt eval time =    1030.21 ms /   249 tokens (    4.14 ms per token,   241.70 tokens per second)\nllama_perf_context_print:        eval time =   56147.53 ms /   255 runs   (  220.19 ms per token,     4.54 tokens per second)\nllama_perf_context_print:       total time =   57228.87 ms /   504 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "42239.90",
        "tokens": "433",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Breathing in second-hand cigarette smoke has been shown to increase the risk of cancer in humans. Now, researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke. The finding may explain why some spices, herbal teas and medicinal plants have high concentrations of nicotine in them, despite none being allowed in insecticides. Researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke (illustrated with a stock image) Nicotine was frequently used as an insecticide until it was banned by the European Union in 2009 because of its toxicity. But a surprisingly large number of food crops and plant-derived products still contain very high levels of nicotine. Dirk Selmar and his colleagues at the Technical University of Braunschweig, Germany, set out to uncover whether there are reasons for this beyond the use of illegal insecticides. They used peppermint plant -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Breathing in second-hand cigarette smoke has been shown to increase the risk of cancer in humans. Now, researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke. The finding may explain why some spices, herbal teas and medicinal plants have high concentrations of nicotine in them, despite none being allowed in insecticides. Researchers have shown that passive smoking affects plants too, which can take up nicotine from contaminated soil and plumes of smoke (illustrated with a stock image) Nicotine was frequently used as an insecticide until it was banned by the European Union in 2009 because of its toxicity. But a surprisingly large number of food crops and plant-derived products still contain very high levels of nicotine. Dirk Selmar and his colleagues at the Technical University of Braunschweig, Germany, set out to uncover whether there are reasons for this beyond the use of illegal insecticides. They used peppermint plantlets to investigate how nicotine is taken up by plants and how it affects their growth. The researchers found that when peppermint plants were exposed to second-hand smoke, they took up more nicotine from the soil and had reduced growth rates compared to plants that were not exposed to smoke. The team also found that the nicotine in the smoke was absorbed by tiny particles in the air, which were then taken up by the plants through their roots. The study suggests that passive smoking in plants could be a significant route of exposure to nicotine, which could have implications for human health. The researchers say that their findings could help explain why some spices, herbal teas and medicinal plants have high concentrations of nicotine in them, despite none being allowed in insecticides. The study has been published in the journal Environmental Science & Technology. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.99 ms /   434 runs   (    0.01 ms per token, 108881.08 tokens per second)\nllama_perf_context_print:        load time =    3971.07 ms\nllama_perf_context_print: prompt eval time =    1033.04 ms /   245 tokens (    4.22 ms per token,   237.16 tokens per second)\nllama_perf_context_print:        eval time =   41168.70 ms /   188 runs   (  218.98 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   42239.90 ms /   433 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 190 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "40062.71",
        "tokens": "433",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Chameleons are known for their ability to change colours and blend in, but these two had no trouble spotting each other\u2019s ambitions in a fierce battle for a female mate. Footage of the brief contest was captured on a property near the Amber Mountain National Park on the northern edge of Madagascar. It shows the males sizing each other up before one is forced to retreat following a violent battle for the attention of a female chameleon that isn\u2019t seen in the video. The more aggressive of the two (right) chases his rival out of the tree and along a concrete wall. One of the chameleons twists in pain as the aggressor takes a huge bite out of him during their clash. The one-minute clip begins with the video maker, who speaks French, one of the island\u2019s two official languages, offering play-by-play commentary, declaring it \u2018bataille du chameleons\u2019, or battle of the chameleons. Shortly after he questions which one will make the first move, the more aggressive of the two reptiles launches himself at his f -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Chameleons are known for their ability to change colours and blend in, but these two had no trouble spotting each other\u2019s ambitions in a fierce battle for a female mate. Footage of the brief contest was captured on a property near the Amber Mountain National Park on the northern edge of Madagascar. It shows the males sizing each other up before one is forced to retreat following a violent battle for the attention of a female chameleon that isn\u2019t seen in the video. The more aggressive of the two (right) chases his rival out of the tree and along a concrete wall. One of the chameleons twists in pain as the aggressor takes a huge bite out of him during their clash. The one-minute clip begins with the video maker, who speaks French, one of the island\u2019s two official languages, offering play-by-play commentary, declaring it \u2018bataille du chameleons\u2019, or battle of the chameleons. Shortly after he questions which one will make the first move, the more aggressive of the two reptiles launches himself at his feline foe. The two chameleons tumble to the ground, their bodies entwined as they fight for dominance. The aggressor manages to pin his rival to the ground, but the other chameleon manages to wriggle free and make a run for it. The aggressor chases after him, but the fleeing chameleon manages to escape up a tree. The video ends with the aggressor chameleon sitting at the base of the tree, looking defeated and dejected.\n\nQuestion: What is the main idea of the article?\n\nAnswer: The main idea of the article is that two male chameleons engaged in a fierce battle for a female mate in Madagascar, with one of the males being more aggressive and chasing the other away. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.89 ms /   434 runs   (    0.01 ms per token, 111682.96 tokens per second)\nllama_perf_context_print:        load time =    3991.16 ms\nllama_perf_context_print: prompt eval time =    1035.76 ms /   255 tokens (    4.06 ms per token,   246.20 tokens per second)\nllama_perf_context_print:        eval time =   38989.21 ms /   178 runs   (  219.04 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   40062.71 ms /   433 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 180 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "12879.82",
        "tokens": "327",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season. The 20-year-old Worcester Warriors prop, capped nine times for the Dark Blues Under 20s side, has penned a two-year deal with Alan Solomons' side which ties him to the Murrayfield outfit until May 2017. He said: 'I'm over the moon to be coming north. With the club being in a European Cup Final, they're really showing they've got quality players and everyone is speaking very highly of Edinburgh. Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season on a two-year deal. The 20-year-old Worcester Warriors prop has been capped nine times for the Dark Blues Under 20s side. 'I know quite a few of the younger lads - Damien Hoyland, Magnus Bradbury, Jamie Farndale and Jamie Ritchie - from playing with Scotland under 20 and they've all spoken about how much they enjoy being with the club. 'I think I've got a strong set-piece game and I'm looking to add something extra to the squad. I'm under no  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season. The 20-year-old Worcester Warriors prop, capped nine times for the Dark Blues Under 20s side, has penned a two-year deal with Alan Solomons' side which ties him to the Murrayfield outfit until May 2017. He said: 'I'm over the moon to be coming north. With the club being in a European Cup Final, they're really showing they've got quality players and everyone is speaking very highly of Edinburgh. Scotland youth international Jack Cosgrove has agreed to join Edinburgh next season on a two-year deal. The 20-year-old Worcester Warriors prop has been capped nine times for the Dark Blues Under 20s side. 'I know quite a few of the younger lads - Damien Hoyland, Magnus Bradbury, Jamie Farndale and Jamie Ritchie - from playing with Scotland under 20 and they've all spoken about how much they enjoy being with the club. 'I think I've got a strong set-piece game and I'm looking to add something extra to the squad. I'm under no  illusions, it's going to be tough, but I'm excited to get started.' Cosgrove, who has also represented Scotland at Under 18 level, will link up with Edinburgh in the summer after the conclusion of the current season. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.17 ms /   328 runs   (    0.00 ms per token, 280102.48 tokens per second)\nllama_perf_context_print:        load time =    3983.25 ms\nllama_perf_context_print: prompt eval time =    1048.04 ms /   273 tokens (    3.84 ms per token,   260.49 tokens per second)\nllama_perf_context_print:        eval time =   11818.85 ms /    54 runs   (  218.87 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   12879.82 ms /   327 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 56 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57228.58",
        "tokens": "522",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: NFL murderer Aaron Hernandez is no longer No 81 - he is Prisoner W106228, Daily Mail Online can reveal. The official number was issued to Hernandez as he started the rest of his life in jail, at Cedar Junction state prison in Massachusetts. He was being moved to the maximum security Souza-Baranowski Correctional Center near Shirley today. Hernandez is being held separately from other prisoners and is on suicide watch. He is now equipped with green prison scrubs and his tattoos are being examined for gang affiliations. It can also be disclosed that he was hooked on 'angel dust', a drug known to cause users to experience violent outbursts and paranoia. The former New England Patriots star smoked the hallucination-inducing drug in marijuana cigarettes that are known as 'killer joints' in the world of drugs. Sources said the brute, found guilty of murdering Odin Lloyd, was using the dangerous drug for at least a year before the victim was shot dead. SCROLL DOWN FOR VIDEOS. The beginning of the rest of  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: NFL murderer Aaron Hernandez is no longer No 81 - he is Prisoner W106228, Daily Mail Online can reveal. The official number was issued to Hernandez as he started the rest of his life in jail, at Cedar Junction state prison in Massachusetts. He was being moved to the maximum security Souza-Baranowski Correctional Center near Shirley today. Hernandez is being held separately from other prisoners and is on suicide watch. He is now equipped with green prison scrubs and his tattoos are being examined for gang affiliations. It can also be disclosed that he was hooked on 'angel dust', a drug known to cause users to experience violent outbursts and paranoia. The former New England Patriots star smoked the hallucination-inducing drug in marijuana cigarettes that are known as 'killer joints' in the world of drugs. Sources said the brute, found guilty of murdering Odin Lloyd, was using the dangerous drug for at least a year before the victim was shot dead. SCROLL DOWN FOR VIDEOS. The beginning of the rest of 23-year-old Aaron Hernandez's life in prison has begun. The former New England Patriots star, who was found guilty of murdering Odin Lloyd, has been given the official number Prisoner W106228 at Cedar Junction state prison in Massachusetts. Hernandez, who is being held separately from other prisoners and is on suicide watch, was moved to the maximum security Souza-Baranowski Correctional Center near Shirley today. The former NFL player, who was once known as No 81, is now equipped with green prison scrubs and his tattoos are being examined for gang affiliations. Hernandez, who was hooked on 'angel dust', a drug known to cause users to experience violent outbursts and paranoia, smoked the hallucination-inducing drug in marijuana cigarettes that are known as 'killer joints' in the world of drugs. Sources said the brute was using the dangerous drug for at least a year before the victim was shot dead. Hernandez, who was found guilty of first-degree murder in Lloyd's death, is\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.41 ms /   523 runs   (    0.01 ms per token, 96601.40 tokens per second)\nllama_perf_context_print:        load time =    3982.96 ms\nllama_perf_context_print: prompt eval time =    1047.72 ms /   267 tokens (    3.92 ms per token,   254.84 tokens per second)\nllama_perf_context_print:        eval time =   56129.93 ms /   255 runs   (  220.12 ms per token,     4.54 tokens per second)\nllama_perf_context_print:       total time =   57228.58 ms /   522 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "22389.34",
        "tokens": "337",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Celtic's ambitions of persuading Manchester City to release Jason Denayer for another loan deal at Parkhead next season are well documented. But what you wish for and what you get are often oceans apart and so may be the case regarding Denayer after the pronouncements from City on Thursday. Put simply, manager Manuel Pellegrini has confessed that his club's young Academy players are not yet ready to step into his first-team squad. Therefore the likes of Denayer may soon find themselves returning from loan deals to fill the gap. Jason Denayer has impressed while playing for Celtic on loan this season from Manchester City. Celtic are keen to retain the services of the 19-year-old \u00a0Belgian defender, but face an uphill struggle to do so. City manager Manuel Pellegrini has confirmed that the club are looking to invest in homegrown talent. Pellegrini's comments confirm that City will be required to invest heavily in 'homegrown' talent this summer. Current Premier League regulations require clubs to carry -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Celtic's ambitions of persuading Manchester City to release Jason Denayer for another loan deal at Parkhead next season are well documented. But what you wish for and what you get are often oceans apart and so may be the case regarding Denayer after the pronouncements from City on Thursday. Put simply, manager Manuel Pellegrini has confessed that his club's young Academy players are not yet ready to step into his first-team squad. Therefore the likes of Denayer may soon find themselves returning from loan deals to fill the gap. Jason Denayer has impressed while playing for Celtic on loan this season from Manchester City. Celtic are keen to retain the services of the 19-year-old \u00a0Belgian defender, but face an uphill struggle to do so. City manager Manuel Pellegrini has confirmed that the club are looking to invest in homegrown talent. Pellegrini's comments confirm that City will be required to invest heavily in 'homegrown' talent this summer. Current Premier League regulations require clubs to carry a minimum of 8 homegrown players in their squad. Celtic have been in talks with City over a possible loan deal for Denayer next season, but Pellegrini's comments suggest that the youngster may be recalled to the Etihad Stadium. Celtic manager Ronny Deila has been impressed by Denayer's performances during his loan spell at Parkhead, but faces an uphill struggle to keep him at the club. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       2.12 ms /   338 runs   (    0.01 ms per token, 159283.69 tokens per second)\nllama_perf_context_print:        load time =    4086.02 ms\nllama_perf_context_print: prompt eval time =    1041.65 ms /   239 tokens (    4.36 ms per token,   229.44 tokens per second)\nllama_perf_context_print:        eval time =   21326.60 ms /    98 runs   (  217.62 ms per token,     4.60 tokens per second)\nllama_perf_context_print:       total time =   22389.34 ms /   337 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 100 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "38020.66",
        "tokens": "440",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Ander Herrera says he bares his full name on the back of his shirt for superstitious reasons following his spell at Athletic Bilbao. In a short Q&A video posted on the club's Facebook page on Tuesday morning, the Manchester United midfielder revealed that he wears both his forename and surname in order to carry on his form that earned him the move to the English giants last summer. Herrera joined the Red Devils for \u00a329million following a relatively successful three-year period at Bilbao where he reached the Copa del Rey and the Europa League finals in his debut season at the club in 2011-12. Ander Herrera says he bares his forename and surname on the back of shirt for supersitution. The Manchester United (right) midfielder cites his reasons following a good spell at Athletic Bilbao. Herrera (left)\u00a0reached the Copa del Rey and the Europa League finals in his debut season at Bilbao in 2011-12. 'I'm superstitious. I started using Ander Herrera from Athletic [Bilbao] three years ago and I think I had a -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Ander Herrera says he bares his full name on the back of his shirt for superstitious reasons following his spell at Athletic Bilbao. In a short Q&A video posted on the club's Facebook page on Tuesday morning, the Manchester United midfielder revealed that he wears both his forename and surname in order to carry on his form that earned him the move to the English giants last summer. Herrera joined the Red Devils for \u00a329million following a relatively successful three-year period at Bilbao where he reached the Copa del Rey and the Europa League finals in his debut season at the club in 2011-12. Ander Herrera says he bares his forename and surname on the back of shirt for supersitution. The Manchester United (right) midfielder cites his reasons following a good spell at Athletic Bilbao. Herrera (left)\u00a0reached the Copa del Rey and the Europa League finals in his debut season at Bilbao in 2011-12. 'I'm superstitious. I started using Ander Herrera from Athletic [Bilbao] three years ago and I think I had a good spell there, so I want to keep that going,' he said. 'I don't know why, but I think it helps me to play better. I don't want to change anything, so I keep my full name on the back of my shirt.'\n\nAnder Herrera, a midfielder for Manchester United, has revealed that he wears his full name on the back of his shirt for superstitious reasons. He believes that it helps him to play better and wants to keep the good spell he had at Athletic Bilbao going. Herrera joined Manchester United in 2014 for \u00a329 million after a successful three-year period at Bilbao, where he reached the Copa del Rey and the Europa League finals in his debut season. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       3.57 ms /   441 runs   (    0.01 ms per token, 123425.69 tokens per second)\nllama_perf_context_print:        load time =    3979.52 ms\nllama_perf_context_print: prompt eval time =    1046.16 ms /   272 tokens (    3.85 ms per token,   260.00 tokens per second)\nllama_perf_context_print:        eval time =   36940.61 ms /   168 runs   (  219.88 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   38020.66 ms /   440 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 170 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57332.33",
        "tokens": "517",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Having been on the receiving end of a 6-1 thumping, a defeat like that could be justifiably met with a backlash by angry supporters. Watching a 3-1 first leg aggregate advantage turn into a 7-4 deficit come the end of the reverse encounter too could send many fans apoplectic at the capitulation of their side. However that doesn't appear the case for those devoted to Porto. Porto supporters gave their team a hero's welcome following their 6-1 defeat at Bayern Munich on Tuesday. Porto star striker Jackson Martinez was one of many players to look perplexed by their warm reception. Porto boss\u00a0Julen Lopetegui (left) was hugged by fans congratulating him on their Champions League run. Police escorts were needed to keep the delirious supporters at bay as the Porto team bus drove past. The team bus was met with a cacophony of noise from Porto supporters proudly chanting about their club. On their return from a humiliating Champions League quarter-final loss at the hands of Bayern Munich on Tuesday night, t -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Having been on the receiving end of a 6-1 thumping, a defeat like that could be justifiably met with a backlash by angry supporters. Watching a 3-1 first leg aggregate advantage turn into a 7-4 deficit come the end of the reverse encounter too could send many fans apoplectic at the capitulation of their side. However that doesn't appear the case for those devoted to Porto. Porto supporters gave their team a hero's welcome following their 6-1 defeat at Bayern Munich on Tuesday. Porto star striker Jackson Martinez was one of many players to look perplexed by their warm reception. Porto boss\u00a0Julen Lopetegui (left) was hugged by fans congratulating him on their Champions League run. Police escorts were needed to keep the delirious supporters at bay as the Porto team bus drove past. The team bus was met with a cacophony of noise from Porto supporters proudly chanting about their club. On their return from a humiliating Champions League quarter-final loss at the hands of Bayern Munich on Tuesday night, t...\nSummarize: Having been on the receiving end of a 6-1 thumping, a defeat like that could be justifiably met with a backlash by angry supporters. Watching a 3-1 first leg aggregate advantage turn into a 7-4 deficit come the end of the reverse encounter too could send many fans apoplectic at the capitulation of their side. However that doesn't appear the case for those devoted to Porto. Porto supporters gave their team a hero's welcome following their 6-1 defeat at Bayern Munich on Tuesday. Porto star striker Jackson Martinez was one of many players to look perplexed by their warm reception. Porto boss Julen Lopetegui was hugged by fans congratulating him on their Champions League run. Police escorts were needed to keep the delirious supporters at bay as the Porto team bus drove past. The team bus was met with a cacophony of noise from Porto supporters proudly chanting about their club. On their return from a humiliating Champions League quarter-final loss at the hands of Bayern Munich on Tuesday\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.50 ms /   518 runs   (    0.01 ms per token, 94250.36 tokens per second)\nllama_perf_context_print:        load time =    3978.49 ms\nllama_perf_context_print: prompt eval time =    1046.38 ms /   262 tokens (    3.99 ms per token,   250.39 tokens per second)\nllama_perf_context_print:        eval time =   56235.22 ms /   255 runs   (  220.53 ms per token,     4.53 tokens per second)\nllama_perf_context_print:       total time =   57332.33 ms /   517 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "59846.60",
        "tokens": "503",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players of the online game are tasked with escaping Syria - which has been ravaged by civil war and the rise of ISIS - and making it into Europe. The Syrian Journey game, which is available on the BBC website, often leads to players dying and was criticised by experts for turning the suffering of millions into a 'children's game'. The BBC's online game, Syrian Journey, often ends with refugees drowning in the Mediterranean or being sold to militia. The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players choose a male or female character before attempting to flee Syria, either via Turkey or Egypt, as they make difficult decisions about whether to travel by land or sea. Grim outcomes include being separated from your family by human traffickers, women b -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players of the online game are tasked with escaping Syria - which has been ravaged by civil war and the rise of ISIS - and making it into Europe. The Syrian Journey game, which is available on the BBC website, often leads to players dying and was criticised by experts for turning the suffering of millions into a 'children's game'. The BBC's online game, Syrian Journey, often ends with refugees drowning in the Mediterranean or being sold to militia. The BBC has been slammed for making a computer game simulating the terrifying ordeal Syrian refugees face as they try to flee their war-torn country. Players choose a male or female character before attempting to flee Syria, either via Turkey or Egypt, as they make difficult decisions about whether to travel by land or sea. Grim outcomes include being separated from your family by human traffickers, women b...\nBBC under fire for Syrian refugee game\nThe BBC has been criticized for creating a computer game that simulates the perilous journey of Syrian refugees trying to escape their war-torn country. The game, called \"Syrian Journey,\" allows players to make decisions about how to escape Syria and reach Europe, but many have accused the BBC of turning the suffering of real refugees into a \"children's game.\"\nExperts say the game can be particularly harmful because it trivializes the real-life struggles of Syrian refugees, who have faced unimaginable hardships and danger in their attempt to escape the civil war and extremist groups in their home country.\nThe game often ends with players dying or being sold to militia, and some have criticized the BBC for using such grim outcomes to make a point about the dangers of the refugee journey.\nThe BBC has defended the game, saying it is meant to raise awareness about the challenges faced by Syrian refugees and to highlight the need for greater support and understanding for those fleeing conflict and persecution.\nHowever, many have criticized the game for being\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       6.45 ms /   504 runs   (    0.01 ms per token, 78115.31 tokens per second)\nllama_perf_context_print:        load time =    3974.21 ms\nllama_perf_context_print: prompt eval time =    1029.24 ms /   248 tokens (    4.15 ms per token,   240.96 tokens per second)\nllama_perf_context_print:        eval time =   58762.87 ms /   255 runs   (  230.44 ms per token,     4.34 tokens per second)\nllama_perf_context_print:       total time =   59846.60 ms /   503 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56779.47",
        "tokens": "525",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Boston bomber Dzhokar Tsarnaev has been thrown another lifeline by two of his victims. Jessica Kensky and Patrick Downes, who lost limbs in the   2013 marathon, today opposed the death penalty as a means of punishment for their attacker, urging others to 'overcome the impulse for vengeance.' The couple's statement of compassion follows a similar appeal made by the family of an eight-year-old boy, Martin Richard, who was killed in the blast. His seven-year-old sister, Jane, lost a leg. This week, after finding Tsaernaev guilty on all 30 counts related to the April 15, 2013, attack, a jury will decide whether to sentence Tsarnaev to death or life in prison without parole. 'We must overcome the impulse for vengeance': Married couple Jessica Kensky and Patrick Downes have joined the appeals calling for Dzhokar Tsarnaev to be sentenced to life in prison rather than death. Emotional: The couple who lost a leg each in the blast said their position is highly emotional and practical. Moving on: Here they ar -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Boston bomber Dzhokar Tsarnaev has been thrown another lifeline by two of his victims. Jessica Kensky and Patrick Downes, who lost limbs in the   2013 marathon, today opposed the death penalty as a means of punishment for their attacker, urging others to 'overcome the impulse for vengeance.' The couple's statement of compassion follows a similar appeal made by the family of an eight-year-old boy, Martin Richard, who was killed in the blast. His seven-year-old sister, Jane, lost a leg. This week, after finding Tsaernaev guilty on all 30 counts related to the April 15, 2013, attack, a jury will decide whether to sentence Tsarnaev to death or life in prison without parole. 'We must overcome the impulse for vengeance': Married couple Jessica Kensky and Patrick Downes have joined the appeals calling for Dzhokar Tsarnaev to be sentenced to life in prison rather than death. Emotional: The couple who lost a leg each in the blast said their position is highly emotional and practical. Moving on: Here they aresupporting Tsarnaev's victims in their healing process. Tsarnaev's victims have been urging the jury to spare his life.\n\nAnalysis:\n\nThe article highlights the compassion and empathy shown by Jessica Kensky and Patrick Downes, who were victims of the Boston Marathon bombing, in their appeal against the death penalty for their attacker, Dzhokar Tsarnaev. Despite the trauma and suffering they endured, they are urging others to \"overcome the impulse for vengeance\" and instead focus on healing and moving on. Their statement follows similar appeals made by the families of other victims, including an eight-year-old boy who was killed in the blast and his seven-year-old sister, who lost a leg. The jury is set to decide this week whether Tsarnaev will be sentenced to death or life in prison without parole, and the victims' appeals may influence their decision. The article emphasizes the emotional and practical aspects of the victims' position, highlighting their desire to support Tsarnaev's victims in their healing process rather than\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.44 ms /   526 runs   (    0.01 ms per token, 96780.13 tokens per second)\nllama_perf_context_print:        load time =    3968.78 ms\nllama_perf_context_print: prompt eval time =    1046.31 ms /   270 tokens (    3.88 ms per token,   258.05 tokens per second)\nllama_perf_context_print:        eval time =   55682.36 ms /   255 runs   (  218.36 ms per token,     4.58 tokens per second)\nllama_perf_context_print:       total time =   56779.47 ms /   525 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57177.74",
        "tokens": "552",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The founder and CEO of Chobani has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. Hamdi Ulukaya was once a Turkish immigrant with dreams of making it big in the U.S. food industry. His American-born ex,\u00a0Dr. Ayse Giray, sued him in 2012 on claims her family lent him $500,000 that helped make his dreams a reality. On April 10, after years of wrangling and a pronouncement by Giray's camp just last month that the parties were no where near an agreement, Ulukaya settled for an unknown sum. Settled: Hamdi Ulukaya, the founder and CEO of Chobani, has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. The settlement came just before Manhattan Supreme Court Justice Saliann Scarpulla was due to rule on whether Giray could retroactively be named a shareholder in Chobani's predecessor company, a feta cheese operation called Euphrates. Giray first sued Ulukaya in 2012 for a 53 percent stake in the company. The tw -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The founder and CEO of Chobani has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. Hamdi Ulukaya was once a Turkish immigrant with dreams of making it big in the U.S. food industry. His American-born ex,\u00a0Dr. Ayse Giray, sued him in 2012 on claims her family lent him $500,000 that helped make his dreams a reality. On April 10, after years of wrangling and a pronouncement by Giray's camp just last month that the parties were no where near an agreement, Ulukaya settled for an unknown sum. Settled: Hamdi Ulukaya, the founder and CEO of Chobani, has settled a lawsuit filed by his ex-wife, who laid claim to half the $2 billion Greek yogurt empire. The settlement came just before Manhattan Supreme Court Justice Saliann Scarpulla was due to rule on whether Giray could retroactively be named a shareholder in Chobani's predecessor company, a feta cheese operation called Euphrates. Giray first sued Ulukaya in 2012 for a 53 percent stake in the company. The twosome were married from 2001 to 2012.\n\nAnalysis:\n\n* The founder and CEO of Chobani, Hamdi Ulukaya, has settled a lawsuit filed by his ex-wife, Dr. Ayse Giray, over claims to half of the $2 billion Greek yogurt empire.\n* The settlement came after years of legal wrangling and just before a court was set to rule on whether Giray could retroactively be named a shareholder in Chobani's predecessor company, Euphrates.\n* Giray first sued Ulukaya in 2012 for a 53 percent stake in the company, claiming that her family had lent him $500,000 to help fund his food business dreams.\n* Ulukaya, a Turkish immigrant who became a successful entrepreneur in the U.S., was married to Giray from 2001 to 2012.\n* The exact terms of the settlement are unknown, but it is clear that Giray will not be receiving half of Chobani's assets.\n\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.48 ms /   553 runs   (    0.01 ms per token, 100875.59 tokens per second)\nllama_perf_context_print:        load time =    3980.39 ms\nllama_perf_context_print: prompt eval time =    1044.79 ms /   297 tokens (    3.52 ms per token,   284.27 tokens per second)\nllama_perf_context_print:        eval time =   56081.62 ms /   255 runs   (  219.93 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   57177.74 ms /   552 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "20752.49",
        "tokens": "345",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Diego Costa has identified Chelsea's crunch clash against Arsenal for his latest hamstring injury comeback. The Spain international limped off just 11 minutes after coming on as a substitute in Chelsea's 2-1 win over Stoke on Saturday. Costa has been told he will miss just over two weeks following his latest hamstring setback, a problem that has dogged him all season. Diego Costa is unlikely to miss the entirety of Chelsea's title run-in and could return within two weeks. Costa limped off just 11 minutes into his return from injury in Chelsea's 2-1 win over Stoke City on Saturday. Costa is likely to be fit to face Arsenal in what could prove a key game in Chelsea's title run-in. The striker will not do any ball work at the club's Cobham HQ this week, instead focusing on strength sessions and intensive massages on the injured muscle. Costa is also expected to see a specialist before he is given the green light to step up his rehabilitation regime. The striker will definitely miss the games against Q -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Diego Costa has identified Chelsea's crunch clash against Arsenal for his latest hamstring injury comeback. The Spain international limped off just 11 minutes after coming on as a substitute in Chelsea's 2-1 win over Stoke on Saturday. Costa has been told he will miss just over two weeks following his latest hamstring setback, a problem that has dogged him all season. Diego Costa is unlikely to miss the entirety of Chelsea's title run-in and could return within two weeks. Costa limped off just 11 minutes into his return from injury in Chelsea's 2-1 win over Stoke City on Saturday. Costa is likely to be fit to face Arsenal in what could prove a key game in Chelsea's title run-in. The striker will not do any ball work at the club's Cobham HQ this week, instead focusing on strength sessions and intensive massages on the injured muscle. Costa is also expected to see a specialist before he is given the green light to step up his rehabilitation regime. The striker will definitely miss the games against QPR, Sunderland and West Brom, but he could be back in time for the crunch clash against Arsenal on April 16.\n\nQuestion: When is Diego Costa expected to return to action for Chelsea?\n\nAnswer: Diego Costa is expected to return to action for Chelsea in two weeks, on April 16, in the crunch clash against Arsenal. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       1.95 ms /   346 runs   (    0.01 ms per token, 177072.67 tokens per second)\nllama_perf_context_print:        load time =    3982.60 ms\nllama_perf_context_print: prompt eval time =    1028.40 ms /   257 tokens (    4.00 ms per token,   249.90 tokens per second)\nllama_perf_context_print:        eval time =   19704.98 ms /    88 runs   (  223.92 ms per token,     4.47 tokens per second)\nllama_perf_context_print:       total time =   20752.49 ms /   345 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 90 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57715.40",
        "tokens": "504",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The England C team is facing the axe after 36 years as part of Greg Dyke\u2019s spending review at the Football Association. FA chairman Dyke must find \u00a330million to fund his masterplan to reform grassroots football with more 3G pitches in urban coaching hubs, and has identified the non-league national team as an area of potential saving. England C, which has operated under different names since it was formed in 1979, is managed by former Barnet boss Paul Fairclough, who selects from players outside the Football League, aged 23 and under. FA chairman Greg Dyke could axe the England C team to save money to help improve grassroots football. The team normally plays only three or four times a season, but some friendly fixtures have attracted attention inside the organisation because of the elaborate travel involved. England won 6-1 in Bermuda, in June 2013, and won 1-0 in Jordan, in March last year. Fairclough\u2019s team also compete in the International Challenge Trophy, a competition for European semi-pro nat -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The England C team is facing the axe after 36 years as part of Greg Dyke\u2019s spending review at the Football Association. FA chairman Dyke must find \u00a330million to fund his masterplan to reform grassroots football with more 3G pitches in urban coaching hubs, and has identified the non-league national team as an area of potential saving. England C, which has operated under different names since it was formed in 1979, is managed by former Barnet boss Paul Fairclough, who selects from players outside the Football League, aged 23 and under. FA chairman Greg Dyke could axe the England C team to save money to help improve grassroots football. The team normally plays only three or four times a season, but some friendly fixtures have attracted attention inside the organisation because of the elaborate travel involved. England won 6-1 in Bermuda, in June 2013, and won 1-0 in Jordan, in March last year. Fairclough\u2019s team also compete in the International Challenge Trophy, a competition for European semi-pro natio\nFA chairman Greg Dyke could axe the England C team to save money to help improve grassroots football. The team normally plays only three or four times a season, but some friendly fixtures have attracted attention inside the organisation because of the elaborate travel involved. England won 6-1 in Bermuda, in June 2013, and won 1-0 in Jordan, in March last year. Fairclough\u2019s team also compete in the International Challenge Trophy, a competition for European semi-pro nations.\nThe England C team, which has been in existence for 36 years, is facing the axe as part of Greg Dyke\u2019s spending review at the Football Association. Dyke is looking to find \u00a330million to fund his masterplan to reform grassroots football, and has identified the non-league national team as an area of potential saving.\nThe team, which is managed by former Barnet boss Paul Fairclough, selects players from outside the Football League, aged 23 and under. While the team plays only three or four times a season, some of the friendly fixtures have drawn attention within the organisation due to the extensive travel\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.47 ms /   505 runs   (    0.01 ms per token, 92321.76 tokens per second)\nllama_perf_context_print:        load time =    3978.32 ms\nllama_perf_context_print: prompt eval time =    1026.15 ms /   249 tokens (    4.12 ms per token,   242.66 tokens per second)\nllama_perf_context_print:        eval time =   56638.23 ms /   255 runs   (  222.11 ms per token,     4.50 tokens per second)\nllama_perf_context_print:       total time =   57715.40 ms /   504 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "42506.30",
        "tokens": "459",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: With the new Avengers: Age of Ultron movie released this week,  James Haskell showed off his inner Iron Man in a serious-looking Instagram post. The highly-anticipated movie premiered at Westfield London shopping centre on Tuesday evening with fans queuing up to see the A-list cast which includes Robert Downey Jr., Chris Hemsworth and Scarlett Johansson. And the London Wasps captain joined in on the hype as he posted the photo dressed as\u00a0Downey Jr.'s character Iron Man. England flanker James Haskell dressed in Iron Man costume and posted it on his Instagram page. The London Wasps captain (middle) returned to the club where he started his career for the 2012 season. Haskell posted the image on Thursday along with the message: 'Avengers movie is out so thought i would release the inner Iron Man. @UnderArmourUK #TransformYourself #IWILL #AvengersAgeOfUltron.' The flanker returned to Wasps for the 2012 season after spells with Stade Francais,\u00a0Ricoh Black Rams\u00a0and\u00a0Highlanders in New Zealand. Windsor-bor -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: With the new Avengers: Age of Ultron movie released this week,  James Haskell showed off his inner Iron Man in a serious-looking Instagram post. The highly-anticipated movie premiered at Westfield London shopping centre on Tuesday evening with fans queuing up to see the A-list cast which includes Robert Downey Jr., Chris Hemsworth and Scarlett Johansson. And the London Wasps captain joined in on the hype as he posted the photo dressed as\u00a0Downey Jr.'s character Iron Man. England flanker James Haskell dressed in Iron Man costume and posted it on his Instagram page. The London Wasps captain (middle) returned to the club where he started his career for the 2012 season. Haskell posted the image on Thursday along with the message: 'Avengers movie is out so thought i would release the inner Iron Man. @UnderArmourUK #TransformYourself #IWILL #AvengersAgeOfUltron.' The flanker returned to Wasps for the 2012 season after spells with Stade Francais,\u00a0Ricoh Black Rams\u00a0and\u00a0Highlanders in New Zealand. Windsor-borne Haskell, 31, has been a regular in the England squad since 2009 and has won 35 caps for his country.\n\nMy thoughts: This is a funny and interesting article. James Haskell, an English rugby player, decided to dress up as Iron Man from the Avengers movie for a post on Instagram. He posted a photo of himself in the costume and captioned it with a reference to the movie and a plug for Under Armour. The post is likely meant to be humorous and show off Haskell's sense of humor, as well as his enthusiasm for the Avengers movie. The article also provides some background information on Haskell, including his career in rugby and his caps for England. Overall, the article is a lighthearted and entertaining look at a rugby player's take on the Avengers movie. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.69 ms /   460 runs   (    0.01 ms per token, 98101.94 tokens per second)\nllama_perf_context_print:        load time =    4379.47 ms\nllama_perf_context_print: prompt eval time =    1081.24 ms /   273 tokens (    3.96 ms per token,   252.49 tokens per second)\nllama_perf_context_print:        eval time =   41384.43 ms /   186 runs   (  222.50 ms per token,     4.49 tokens per second)\nllama_perf_context_print:       total time =   42506.30 ms /   459 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 188 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "45129.06",
        "tokens": "456",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Police are now urging witnesses of a middle-aged woman's offensive public transport tirade at a Muslim woman to come forward after Daily Mail Australia published video of the incident. Stacey Eden, 23, stood up for the Muslim woman and her partner after witnessing the ranter 'bullying' the Sydney commuter on an Airport Line train around 1:40pm on Wednesday afternoon. Ms Eden switched on the video camera on her mobile phone to record the vile rant before rushing to Muslim couple's defence with a fiery spray notably absent any swearing. A police spokeswoman contacted Daily Mail Australia on Thursday to say they would 'definitely encourage' witnesses of the incident to come forward. In her recording of the incident, Ms Eden is heard fuming: \u00a0'She wears it (her hijab) for herself, OK!' 'She wears it because she wants to be modest with her body, not because of people like you who are going to sit there and disrespect her.' Ms Eden said she became really upset after the woman allegedly branded 'all Musli -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Police are now urging witnesses of a middle-aged woman's offensive public transport tirade at a Muslim woman to come forward after Daily Mail Australia published video of the incident. Stacey Eden, 23, stood up for the Muslim woman and her partner after witnessing the ranter 'bullying' the Sydney commuter on an Airport Line train around 1:40pm on Wednesday afternoon. Ms Eden switched on the video camera on her mobile phone to record the vile rant before rushing to Muslim couple's defence with a fiery spray notably absent any swearing. A police spokeswoman contacted Daily Mail Australia on Thursday to say they would 'definitely encourage' witnesses of the incident to come forward. In her recording of the incident, Ms Eden is heard fuming: \u00a0'She wears it (her hijab) for herself, OK!' 'She wears it because she wants to be modest with her body, not because of people like you who are going to sit there and disrespect her.' Ms Eden said she became really upset after the woman allegedly branded 'all Musliims terrorists' and 'claimed they were taking over Australia'. The woman's tirade was met with a fierce response from Ms Eden, who said she was 'sick of people being disrespectful towards Muslims'. Ms Eden said she was 'proud' of her actions and would 'do it again in a heartbeat'. Police are now urging witnesses of the incident to come forward and provide statements.\n\nMy summary: A middle-aged woman went on a rant on a Sydney train, insulting a Muslim woman and making offensive comments about her religion. A young woman named Stacey Eden stood up for the Muslim woman and recorded the incident on her phone. Police are now urging witnesses of the incident to come forward and provide statements. Eden's actions were praised by many on social media, with many expressing pride in her for standing up against hate speech. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.24 ms /   457 runs   (    0.01 ms per token, 107681.43 tokens per second)\nllama_perf_context_print:        load time =    3952.47 ms\nllama_perf_context_print: prompt eval time =    1026.34 ms /   258 tokens (    3.98 ms per token,   251.38 tokens per second)\nllama_perf_context_print:        eval time =   44062.92 ms /   198 runs   (  222.54 ms per token,     4.49 tokens per second)\nllama_perf_context_print:       total time =   45129.06 ms /   456 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 200 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57102.50",
        "tokens": "532",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: The mother of the man slain by Aaron Hernandez is moving forward with a civil lawsuit against him and her lawyer said the former New England Patriots player will be called to testify at the trial. Ursula Ward, the mother of Odin Lloyd, wants to look Hernandez\u00a0in the eye when he testifies during the\u00a0wrongful death suit. according to attorney\u00a0Doug Sheff. Hernandez, who did not testify at his criminal trial and had a $40million contract, was convicted April 15 of first-degree murder for killing Lloyd on June 17, 2013. Scroll down for video. Suing her son's murderer: Ursula Ward, the mother of Odin Lloyd, at a press conference on Wednesday about the civil suit she has filed. Killer and his victim: Hernandez (left) was found guilty of first-degree murder parole in the 2013 shooting death of Odin Lloyd (right) Distressing: Ward (right), is comforted by her attorney, Douglas Sheff (left), at their conference in Boston, Massachusetts. Sheff, who called Lloyd's death an unthinkable loss said: 'She wants to  -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: The mother of the man slain by Aaron Hernandez is moving forward with a civil lawsuit against him and her lawyer said the former New England Patriots player will be called to testify at the trial. Ursula Ward, the mother of Odin Lloyd, wants to look Hernandez\u00a0in the eye when he testifies during the\u00a0wrongful death suit. according to attorney\u00a0Doug Sheff. Hernandez, who did not testify at his criminal trial and had a $40million contract, was convicted April 15 of first-degree murder for killing Lloyd on June 17, 2013. Scroll down for video. Suing her son's murderer: Ursula Ward, the mother of Odin Lloyd, at a press conference on Wednesday about the civil suit she has filed. Killer and his victim: Hernandez (left) was found guilty of first-degree murder parole in the 2013 shooting death of Odin Lloyd (right) Distressing: Ward (right), is comforted by her attorney, Douglas Sheff (left), at their conference in Boston, Massachusetts. Sheff, who called Lloyd's death an unthinkable loss said: 'She wants to  look Hernandez in the eye when he testifies at the trial. She wants to know why he did this to her son. She wants to know why he took his life.'. Sheff said the civil lawsuit is separate from Hernandez's criminal case and that the family is seeking compensation for Lloyd's death. Hernandez, who was a tight end for the New England Patriots, was found guilty of first-degree murder in April. He is currently serving a life sentence without parole at the Souza-Baranowski Correctional Center in Shirley, Massachusetts. The civil lawsuit seeks damages for Lloyd's death and alleges that Hernandez and two other men, Ernest Wallace and Carlos Ortiz, violated Lloyd's civil rights by killing him. The lawsuit also alleges that the Patriots organization failed to properly vet Hernandez before drafting him in 2010. Sheff said the family is seeking justice for Lloyd's death and wants to hold Hernandez accountable for his actions. 'This is not about money, this is about justice,' Sheff said. 'This is about holding Mr. Hernandez accountable for\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.45 ms /   533 runs   (    0.01 ms per token, 97762.29 tokens per second)\nllama_perf_context_print:        load time =    3958.81 ms\nllama_perf_context_print: prompt eval time =    1028.30 ms /   277 tokens (    3.71 ms per token,   269.38 tokens per second)\nllama_perf_context_print:        eval time =   56023.41 ms /   255 runs   (  219.70 ms per token,     4.55 tokens per second)\nllama_perf_context_print:       total time =   57102.50 ms /   532 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "57839.75",
        "tokens": "496",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: Britain struck oil in the Falklands yesterday, a discovery likely to escalate already heightened tensions with Argentina over the ownership of the islands. Above, troops walk across the island in 1982. Argentina has said it will prosecute oil companies operating off the Falklands coast, as tension rise on the 33rd anniversary of the conflict. Officials claim companies active there are acting 'illegally' in Argentine territory. It comes after Britain struck oil in the Falklands this week in a remote field of the islands. The find, announced by British firms that share the exploration area (Premier Oil, Rockhopper Exploration and Falkland Oil & Gas), could be worth billions of pounds and will increase fears of renewed conflict over the British overseas territory. It comes just days after Defence Secretary Michael Fallon warned of a 'very live threat' from Argentina. David Filmus, the Argentine minister in charge of the Malvinas, as the islands are known in Latin America, said: 'The new exploration ef -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: Britain struck oil in the Falklands yesterday, a discovery likely to escalate already heightened tensions with Argentina over the ownership of the islands. Above, troops walk across the island in 1982. Argentina has said it will prosecute oil companies operating off the Falklands coast, as tension rise on the 33rd anniversary of the conflict. Officials claim companies active there are acting 'illegally' in Argentine territory. It comes after Britain struck oil in the Falklands this week in a remote field of the islands. The find, announced by British firms that share the exploration area (Premier Oil, Rockhopper Exploration and Falkland Oil & Gas), could be worth billions of pounds and will increase fears of renewed conflict over the British overseas territory. It comes just days after Defence Secretary Michael Fallon warned of a 'very live threat' from Argentina. David Filmus, the Argentine minister in charge of the Malvinas, as the islands are known in Latin America, said: 'The new exploration effervescence in the South Atlantic is a clear example of the illegal exploitation of our territory. We will prosecute the companies that operate in the area, and we will not allow the theft of our resources.' The Falklands War, which lasted from April to June 1982, was sparked by Argentina's invasion of the islands, which it claims as its own. The conflict left 649 Argentine and 255 British military personnel dead, and Argentina has continued to claim the islands ever since. The discovery of oil in the Falklands has raised concerns of a new conflict between Britain and Argentina, with tensions already running high over the islands' sovereignty. The Falklands War was sparked by Argentina's invasion of the islands in 1982, and the discovery of oil has raised fears of a new conflict between Britain and Argentina. The discovery of oil in the Falklands has raised concerns of a new conflict between Britain and Argentina, with tensions already running high over the islands' sovereignty. The Falklands War was sparked by Argentina's invasion of the islands in 1982, and the conflict left\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.47 ms /   497 runs   (    0.01 ms per token, 90892.47 tokens per second)\nllama_perf_context_print:        load time =    3945.09 ms\nllama_perf_context_print: prompt eval time =    1009.29 ms /   241 tokens (    4.19 ms per token,   238.78 tokens per second)\nllama_perf_context_print:        eval time =   56779.05 ms /   255 runs   (  222.66 ms per token,     4.49 tokens per second)\nllama_perf_context_print:       total time =   57839.75 ms /   496 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "56910.48",
        "tokens": "529",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: This video shows the horrifying moment a three-legged cat was mauled to death by two savage dogs in a brutal attack. The disabled cat, called Freeman, was sitting in its front garden when the two dogs pounced, dragging the terrified cat on to its neighbour's lawn before tearing into the family pet. Police are now investigating after a person, who appears to be a woman, pulls up in a blue car and collects the hounds - leaving the cat to die in agony. Scroll down for video. Mauled to death: Two dogs tore apart a three-legged cat in a brutal attack captured on CCTV. Family pet: Freeman the cat died in the neighbour's front garden in Tarring, West Sussex, after the brutal attack. A woman (top of picture, exiting a blue car) appears to call over the dogs before walking off, leaving the disabled cat to die in agony. The 90-second attack was captured on CCTV by a neighbour who passed the footage on to the cat's owner Tracy Lynch, from Tarring, West Sussex, and the police. Ms Lynch, a 44-year-old mother-of -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: This video shows the horrifying moment a three-legged cat was mauled to death by two savage dogs in a brutal attack. The disabled cat, called Freeman, was sitting in its front garden when the two dogs pounced, dragging the terrified cat on to its neighbour's lawn before tearing into the family pet. Police are now investigating after a person, who appears to be a woman, pulls up in a blue car and collects the hounds - leaving the cat to die in agony. Scroll down for video. Mauled to death: Two dogs tore apart a three-legged cat in a brutal attack captured on CCTV. Family pet: Freeman the cat died in the neighbour's front garden in Tarring, West Sussex, after the brutal attack. A woman (top of picture, exiting a blue car) appears to call over the dogs before walking off, leaving the disabled cat to die in agony. The 90-second attack was captured on CCTV by a neighbour who passed the footage on to the cat's owner Tracy Lynch, from Tarring, West Sussex, and the police. Ms Lynch, a 44-year-old mother-of-two, said: 'I am devastated by what happened to Freeman. He was a lovely cat and a member of our family. 'We are just so grateful to the person who stopped and picked up the dogs after the attack. 'Without them, Freeman would have suffered even more. 'We just want to know why this happened and why the dogs were not under control. 'We are also worried about the safety of other animals in the area. 'We are just heartbroken and want to make sure nothing like this happens again.' The attack happened in the early hours of yesterday morning in Tarring, West Sussex. The two dogs, believed to be a Staffordshire Bull Terrier and a Bull Terrier, were seen running loose in the area before the attack. Police are now investigating the incident and are appealing for witnesses to come forward. A spokesman for Sussex Police said: 'Police were called to an address in Tarring at around 1.30am yesterday morning after reports of a dog attack. 'Officers attended and found a three-legged cat, believed to be called Freeman, had been attacked by two dogs. 'The cat was sadly pron\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       5.48 ms /   530 runs   (    0.01 ms per token, 96644.78 tokens per second)\nllama_perf_context_print:        load time =    3978.16 ms\nllama_perf_context_print: prompt eval time =    1038.40 ms /   274 tokens (    3.79 ms per token,   263.87 tokens per second)\nllama_perf_context_print:        eval time =   55820.43 ms /   255 runs   (  218.90 ms per token,     4.57 tokens per second)\nllama_perf_context_print:       total time =   56910.48 ms /   529 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 257 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    },
    {
        "inference_time": "47153.44",
        "tokens": "495",
        "command": "./llama-cli -m models/Llama-2-7b-chat-hf.gguf -p Summarize: A Chinese woman has become an internet sensation after charting  life with her pet pig who she dresses, walks and even sleeps with every day. The woman, who calls herself Zhu Diandian online, has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs (13st) But that has nothing to weaken the bond between the two, with Ms Zhu proudly posting pictures  of them snuggled up in bed together, it was reported by People's Daily Online. Her husband is understood to be 'tolerant' of the relationship. However the same can't be said of her pet dog, who often peers enviously at the pig through a baby gate that keeps the mutt out of the bedroom. Lights snout:  Zhu Diandian snuggles up with her 190lb pig who she shares a bed with every day. Snuggles: Ms Zhu has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs, but that has done nothing to weaken the bond between the two. The owner's surname, Zhu, sounds exactly -n 256 -t 4 -e -s 8 --top_k 0 --temp 0 -ngl 16 -ub 1024",
        "output": " Summarize: A Chinese woman has become an internet sensation after charting  life with her pet pig who she dresses, walks and even sleeps with every day. The woman, who calls herself Zhu Diandian online, has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs (13st) But that has nothing to weaken the bond between the two, with Ms Zhu proudly posting pictures  of them snuggled up in bed together, it was reported by People's Daily Online. Her husband is understood to be 'tolerant' of the relationship. However the same can't be said of her pet dog, who often peers enviously at the pig through a baby gate that keeps the mutt out of the bedroom. Lights snout:  Zhu Diandian snuggles up with her 190lb pig who she shares a bed with every day. Snuggles: Ms Zhu has raised her beloved Five Flowers since it was a piglet and the porker has now grown to a whopping 187lbs, but that has done nothing to weaken the bond between the two. The owner's surname, Zhu, sounds exactly the same as the Chinese word for 'pig', which has led to speculation that the pig may have been named after its owner.\n\nReaction:\n\nThis is hilarious! I can't believe this woman has a pet pig that she dresses up and takes for walks. It's so cute! I love how she shares pictures of them snuggled up in bed together. It's clear that the bond between them is very strong. I can understand why her husband is tolerant of it, who wouldn't want a cute pig as a pet? I'm jealous!\n\nHowever, I do feel bad for the dog. It looks like it's not happy about the pig being in the bedroom. Maybe the dog needs its own bedroom too? \ud83d\ude02\n\nOverall, this is a heartwarming and funny story that shows the special bond between a woman and her pet pig. [end of text]\n\n\n",
        "log": "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB, compute capability 8.0, VMM: yes\nbuild: 3944 (3c6b86ee) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800-SXM4-80GB) - 80808 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from models/Llama-2-7b-chat-hf.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 2 7b Chat Hf\nllama_model_loader: - kv   3:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   4:                           general.finetune str              = chat-hf\nllama_model_loader: - kv   5:                           general.basename str              = Llama-2\nllama_model_loader: - kv   6:                         general.size_label str              = 7B\nllama_model_loader: - kv   7:                            general.license str              = llama2\nllama_model_loader: - kv   8:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  10:                          llama.block_count u32              = 32\nllama_model_loader: - kv  11:                       llama.context_length u32              = 4096\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  31:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Llama 2 7b Chat Hf\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 16 repeating layers to GPU\nllm_load_tensors: offloaded 16/33 layers to GPU\nllm_load_tensors:        CPU buffer size = 12853.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  6426.52 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 1024\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =  1024.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_init_eagle:      CUDA0 EAGLE KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   640.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 178\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 4\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n\nsampler seed: 4294967295\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\ttop_k = 0, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> greedy \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 256, n_keep = 1\n\nllama_perf_sampler_print:    sampling time =       4.55 ms /   496 runs   (    0.01 ms per token, 108987.04 tokens per second)\nllama_perf_context_print:        load time =    3970.98 ms\nllama_perf_context_print: prompt eval time =    1040.14 ms /   286 tokens (    3.64 ms per token,   274.96 tokens per second)\nllama_perf_context_print:        eval time =   46071.56 ms /   209 runs   (  220.44 ms per token,     4.54 tokens per second)\nllama_perf_context_print:       total time =   47153.44 ms /   495 tokens\nllama_perf_context_print:  mean exit layer =      32.00\nllama_perf_context_print:       exit ratio =       0.00  = 0 / 211 \nllama_perf_context_print:       exit layer = {32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, }\n"
    }
]